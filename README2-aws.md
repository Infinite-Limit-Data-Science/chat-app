### AWS EC2

EC2 Instances are organized in Instance Types. It comprises of an Instance Class, Generation and Instance Size. For example, for m5.2xlarge, the Instance Class is m, the generation is 5, and the instance size is 2xlarge. General Purpose instances are great for a diversity of workloads, a balance between compute, memory and networking. Compute Optimized instances are great for compute-intensive tasks that require high performance, such as media transcoding, High Performance Computing (HPC), machine learning and dedicated gaming servers. The Instance Class is "C". Memory Optimized instances are fast performance for workloads that process large data sets in memory, such as databases, distributed web scale cache stores, and applications performing real-time processing of big unstructured data. The Instance Class is "R". Storage Optimized Instances are great for storage-intensive tasks that require high, sequential read and write access to large data sets on local storage. An example is High frequency online transaction processing (OLTP) systems and data warehousing applications.

However, the above Instance Classes aren't the best suited for complex NLP tasks. The Amazon EC2 Service introduced the G4 instances, which are dedicated GPU instances for deploying machine learning models and graphics-intensive applications. The G4 instances are available with a choice of NVIDIA GPUs (G4dn) or AMD GPUs (G4ad). G4dn instances feature NVIDIA T4 GPUs and custom Intel Cascade Lake CPUs. Earlier, I compared the NVidia enterprise Graphics Cards, A100 and H100, and the consumer Graphics Card, GeForce RTX 4090. The NVIDIA Tesla T4 is ideal for cloud computing due to its size and features. It offers a high level of performance, boasting 2560 Cuda Cores, 320 Tensor Cores, 16 GB of V-RAM (GDDR6), 320 GB/s of Memory Bandwidth, 1.59 GHz of Boost Clock, 1.35 GHz of Base Clock, using the Turing architecture, at a lower cost than the A100 or H100 GPUs. The T4 GPU has a lower power consumption than the A100 or H100 GPUs, which makes it more suitable for datacenter deployments where power consumption is a concern. The T4 GPU has been widely available and has a well-established supply chain, which makes it easier for AWS to source and deploy in large quantities. The T4 GPU has a wide range of software support, including NVIDIA's CUDA and cuDNN libraries, which makes it easy for developers to deploy and run their applications on AWS G4 instances.  The T4 GPU is compatible with a wide range of operating systems and frameworks, including Linux, Windows, and popular deep learning frameworks like TensorFlow and PyTorch.

Note in March of 2023, AWS announced a collaboration with NVIDIA for the EC2 P5 Instance, which is powered by the NVidia H100 Tensor Core GPUs for accelerated generative AI and HPC applications. Also, note that the EC2 P4d instances are backed by the A100s. This is important as the Turing architecture has been somewhat deprecated in a fast evolving AI industry. For example, the HuggingFace TGI Inference uses Flash Attention, a technique that reduces the computational cost of attention mechanisms in transformers by using a more efficient algorithm. It does not support the older Turing architecture. As a runtime error I received succintly states it: "RuntimeError: FlashAttention only supports Ampere GPUs or newer." 

The NVIDIA A10G Tensor Core GPU is powered by the NVIDIA Ampere Architecture. Beyond that, it boasts more capacity than the Tesla T4s of the Turing architecture. The A10G has 6928 CUDA Cores, 288 Tensor Cores, 24GB of GDDR6 V-RAM (Video RAM or GPU memory), 384-bit memory bus, 600.2 GB/s memory bandwidth, 1695 MHz Boost Clock, 885 MHz Base Clock, and 28,300 million transistors. 

### AWS SageMaker

To be added soon