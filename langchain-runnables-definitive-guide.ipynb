{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a step‑by‑step explanation of what’s going on when RunnableWithMessageHistory sets up this snippet:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "history_chain: Runnable = RunnableLambda(\n",
    "    self._enter_history, self._aenter_history\n",
    ").with_config(run_name=\"load_history\")\n",
    "\n",
    "messages_key = history_messages_key or input_messages_key\n",
    "if messages_key:\n",
    "    history_chain = (\n",
    "        RunnablePassthrough\n",
    "        .assign(**{messages_key: history_chain})\n",
    "        .with_config(run_name=\"insert_history\")\n",
    "    )\n",
    "Overall Goal\n",
    "RunnableWithMessageHistory has two major jobs:\n",
    "\n",
    "Load chat history from the user’s storage (like a database) and place it into the incoming dictionary.\n",
    "After the main chain finishes, it appends the new AI message and user message back to the chat history storage.\n",
    "The snippet you asked about handles (1) loading the chat history into the dictionary. Specifically, it:\n",
    "\n",
    "Creates a history_chain (RunnableLambda) that fetches a list of messages from your stored conversation.\n",
    "Then merges those messages into the pipeline’s dictionary under a key like \"chat_history\" (or whichever messages_key is given).\n",
    "That way, the next node in the chain has the entire conversation so far.\n",
    "\n",
    "Step by Step\n",
    "history_chain = RunnableLambda(self._enter_history, self._aenter_history):\n",
    "\n",
    "RunnableLambda is a Runnable that calls either the sync function (_enter_history) or the async function (_aenter_history) depending on whether you do .invoke() or .ainvoke().\n",
    "\n",
    "Inside _enter_history/_aenter_history, RunnableWithMessageHistory does something like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "hist: BaseChatMessageHistory = config[\"configurable\"][\"message_history\"]\n",
    "messages = hist.messages.copy()\n",
    "...\n",
    "# Possibly also merges user messages into the list\n",
    "return messages\n",
    "In other words, it returns a list of all messages up to now from your conversation storage. This “list of messages” is the RunnableLambda output.\n",
    "\n",
    "if messages_key: (like \"chat_history\" or \"history\") – we do:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "history_chain = RunnablePassthrough.assign(**{messages_key: history_chain})\n",
    "RunnablePassthrough.assign(...) means:\n",
    "\n",
    "“Take the input dictionary, pass it through untouched, except add or overwrite a new key–value pair: messages_key = <the output of the sub-runnable>.”\n",
    "So effectively, we have:\n",
    "\n",
    "The pipeline input dictionary (say, {\"input\": \"User question\"}).\n",
    "We run history_chain in parallel to produce the list of stored messages, e.g. [SystemMessage(\"...\"), HumanMessage(\"...\")].\n",
    "Then we put that list as dict[messages_key] (like dict[\"chat_history\"] = [...]).\n",
    "We pass the final dictionary on to the next step in the chain.\n",
    "Hence, the final dictionary going forward might look like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "{\n",
    "    \"input\": \"User question\",\n",
    "    \"chat_history\": [\n",
    "       SystemMessage(...),\n",
    "       HumanMessage(...),\n",
    "       ...\n",
    "    ]\n",
    "}\n",
    "Why a two-step structure?\n",
    "history_chain alone returns list_of_messages. But the chain system expects to keep the original dictionary plus a new field.\n",
    "RunnablePassthrough.assign(...) merges those two results together.\n",
    "So the pipeline says:\n",
    "“Get the conversation from history_chain.”\n",
    "“Store it under a dictionary key like chat_history.”\n",
    "“Pass that dictionary forward.”\n",
    "Final Flow in RunnableWithMessageHistory\n",
    "Putting it all together in RunnableWithMessageHistory:\n",
    "\n",
    "Before the main chain runs, we do history_chain so that the dictionary has the full conversation.\n",
    "The chain can see chat_history or history and incorporate it into the prompt.\n",
    "After the chain completes, _exit_history merges the brand-new AI message (and user’s message if needed) back into permanent storage.\n",
    "Essentially:\n",
    "\n",
    "“Load” messages → chat_history.\n",
    "Call user’s chain with that chat_history.\n",
    "On end, “save” new messages.\n",
    "That’s how the snippet works in-depth.\n",
    "\n",
    "You said:\n",
    "history_chain = RunnablePassthrough.assign(**{messages_key: history_chain}) And how is it possible these runnables are not executed right away but rather only when the chain is invoked?\n",
    "Reasoned for a few seconds\n",
    "In LangChain’s Runnable architecture, creating or chaining runnables is a declarative (lazy) process, whereas actually running them only happens when you eventually call something like .invoke(...), .ainvoke(...), or .stream(...). In other words:\n",
    "\n",
    "When you write code like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "history_chain = RunnablePassthrough.assign(**{messages_key: history_chain})\n",
    "you’re building (or extending) a “pipeline” object. Each Runnable references the next node or transformation. No work is done at this time; you’re simply composing runnables.\n",
    "\n",
    "At some later point, you call:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "final_result = history_chain.invoke(input_dict)\n",
    "or the chain you built is used as part of a larger chain that is .invoke(...)d. Only then does the system:\n",
    "\n",
    "Take input_dict.\n",
    "Pass it through the RunnablePassthrough.assign(...) logic.\n",
    "Actually execute history_chain (the sub-runnable) to produce its output.\n",
    "Assign that output to the specified dictionary key.\n",
    "Return or feed that final dictionary to the next node.\n",
    "Hence, the runnables aren’t executed right away during the chain construction — they’re merely configured. Execution only happens once the final chain is called with .invoke(...) (or .ainvoke(...), etc.). This is how LangChain’s “DAG” of runnables keeps everything lazy until you explicitly run it.\n",
    "\n",
    "You said:\n",
    "hist: BaseChatMessageHistory = config[\"configurable\"][\"message_history\"]\n",
    "messages = hist.messages.copy()   How is this pulling messages from let's say a mongo database?\n",
    "Reasoned for a couple of seconds\n",
    "In that snippet:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "hist: BaseChatMessageHistory = config[\"configurable\"][\"message_history\"]\n",
    "messages = hist.messages.copy()\n",
    "hist is already an instance of some concrete BaseChatMessageHistory subclass (e.g. MyMongoDBChatMessageHistory). That subclass’s messages property does the actual retrieval from Mongo (or whichever database). For example, it might look like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "class MyMongoDBChatMessageHistory(BaseChatMessageHistory):\n",
    "    @property\n",
    "    def messages(self) -> list[BaseMessage]:\n",
    "        # code that queries Mongo and returns a list of messages\n",
    "So calling hist.messages triggers your custom logic (like a Mongo query) under the hood. That’s why from the outside you just see hist.messages.copy(), but in practice, the property’s getter does the real DB read. Essentially:\n",
    "\n",
    "hist is an object you set up in the config, referencing your “Mongo-based chat history” class.\n",
    "When you do hist.messages, the property’s getter runs your code that fetches documents from Mongo, constructs them as BaseMessage objects, and returns them.\n",
    "Then .copy() just clones that list in memory.\n",
    "Hence, the retrieval from Mongo is abstracted away in your custom BaseChatMessageHistory subclass implementation.\n",
    "\n",
    "You said:\n",
    "What is the purpose of these two additional runnables:         runnable_sync: Runnable = runnable.with_listeners(on_end=self._exit_history)\n",
    "        runnable_async: Runnable = runnable.with_alisteners(on_end=self._aexit_history)\n",
    "Reasoned for a couple of seconds\n",
    "In LangChain’s Runnable API, a single Runnable can have two ways of running:\n",
    "\n",
    "Synchronous (.invoke(...))\n",
    "Asynchronous (.ainvoke(...))\n",
    "To support both modes, LangChain allows you to attach different listener callbacks that fire at different lifecycle events (like on_end) for each mode:\n",
    "\n",
    "runnable.with_listeners(on_end=...) attaches callbacks for the sync execution path.\n",
    "runnable.with_alisteners(on_end=...) attaches callbacks for the async execution path.\n",
    "Hence:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "runnable_sync: Runnable = runnable.with_listeners(on_end=self._exit_history)\n",
    "runnable_async: Runnable = runnable.with_alisteners(on_end=self._aexit_history)\n",
    "runnable_sync is the same underlying Runnable as runnable, but it has a sync “end” listener (on_end=self._exit_history) which is called once .invoke() finishes.\n",
    "runnable_async is the same underlying Runnable but with an async “end” listener (on_end=self._aexit_history) which is called once .ainvoke() finishes.\n",
    "Why do this?\n",
    "If someone calls .invoke() (sync path), the code will eventually trigger on_end=self._exit_history – your method that handles final logic (like saving new messages to Mongo). But if someone calls .ainvoke() (async path), the chain eventually calls on_end=self._aexit_history – an async version that performs the same logic in a fully non-blocking way.\n",
    "\n",
    "So effectively, the code is preparing two versions of the same chain:\n",
    "\n",
    "One for synchronous usage, with a synchronous callback.\n",
    "One for asynchronous usage, with an asynchronous callback.\n",
    "You said:\n",
    "In _exit_history, it actually adds the human and assistant message to mongo database:     def _exit_history(self, run: Run, config: RunnableConfig) -> None:\n",
    "        hist: BaseChatMessageHistory = config[\"configurable\"][\"message_history\"]\n",
    "\n",
    "        # Get the input messages\n",
    "        inputs = load(run.inputs)\n",
    "        input_messages = self._get_input_messages(inputs)\n",
    "        # If historic messages were prepended to the input messages, remove them to\n",
    "        # avoid adding duplicate messages to history.\n",
    "        if not self.history_messages_key:\n",
    "            historic_messages = config[\"configurable\"][\"message_history\"].messages\n",
    "            input_messages = input_messages[len(historic_messages) :]\n",
    "\n",
    "        # Get the output messages\n",
    "        output_val = load(run.outputs)\n",
    "        output_messages = self._get_output_messages(output_val)\n",
    "        hist.add_messages(input_messages + output_messages)\n",
    "Reasoned for a second\n",
    "Yes, exactly. The _exit_history method runs after the entire chain finishes (i.e., once the final output is available). It retrieves:\n",
    "\n",
    "The user’s input messages (which typically includes the new user/HumanMessage for this request).\n",
    "The newly produced AI or assistant messages (from the chain’s final output).\n",
    "Then it calls hist.add_messages(...), which in turn (in your custom Mongo-based class) persists those messages to the MongoDB collection.\n",
    "\n",
    "So effectively, _exit_history merges:\n",
    "\n",
    "The user’s new question\n",
    "The newly generated LLM response\n",
    "…into your conversation history in the database. This is why it’s an “on_end” listener – it only runs once the chain has produced all the needed messages.\n",
    "\n",
    "You said:\n",
    "This is my chatbot that uses a mix of runnables and graphs: from typing import (\n",
    "    Annotated, \n",
    "    TypedDict, \n",
    "    Dict, \n",
    "    Literal, \n",
    "    Optional,\n",
    "    TypeVar,\n",
    "    Union,\n",
    "    Iterator,\n",
    "    AsyncIterator,\n",
    "    AsyncGenerator,\n",
    "    Sequence,\n",
    "    TypeAlias,\n",
    "    override,\n",
    "    Self,\n",
    "    Any,\n",
    "    List,\n",
    "    Tuple,\n",
    ")\n",
    "import os\n",
    "import json\n",
    "from bson import ObjectId\n",
    "from collections import defaultdict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_core.runnables import (\n",
    "    Runnable, \n",
    "    RunnableSerializable, \n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    RunnableBranch,\n",
    "    RunnableBinding,\n",
    ")\n",
    "from langchain_core.tracers.schemas import Run\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.prompt_values import PromptValue, ChatPromptValue\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain.chains.combine_documents.base import (\n",
    "    DEFAULT_DOCUMENT_SEPARATOR, \n",
    "    DEFAULT_DOCUMENT_PROMPT,\n",
    ")\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage, \n",
    "    HumanMessage, \n",
    "    AIMessage, \n",
    "    BaseMessage,    \n",
    "    AIMessageChunk,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, MessageLikeRepresentation\n",
    "from langchain_core.messages.utils import AnyMessage\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.outputs import (\n",
    "    ChatGeneration, \n",
    "    ChatGenerationChunk\n",
    ")\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from pydantic import (\n",
    "    BaseModel,\n",
    "    Field, \n",
    "    model_validator,\n",
    "    ConfigDict\n",
    ")\n",
    "from langchain_core.retrievers import RetrieverLike\n",
    "from pymongo import DESCENDING\n",
    "\n",
    "from .graph_state import State\n",
    "from .language_models.huggingface import HuggingFaceInference\n",
    "from .chat_bot_config import ChatBotConfig\n",
    "# from .local_tools.route_query_tool import RouteQueryTool\n",
    "\n",
    "from langchain_redis import RedisConfig\n",
    "from redisvl.query.filter import Tag, FilterExpression\n",
    "from ..gwblue_redis_vectoretriever.config import VectorStoreSchema\n",
    "from ..gwblue_redis_vectoretriever.vectorstore import RedisVectorStoreTTL\n",
    "\n",
    "from .prompts import registry\n",
    "from .message_history import (\n",
    "    MongoMessageHistorySchema, \n",
    "    MongoMessageHistory, \n",
    ")\n",
    "\n",
    "ChatGenerationLike: TypeAlias = ChatGeneration | Iterator[ChatGeneration] | AsyncIterator[ChatGenerationChunk]\n",
    "\n",
    "I = TypeVar('I', bound=Union[PromptValue, str, Sequence[MessageLikeRepresentation]])\n",
    "O = TypeVar('O', bound=ChatGenerationLike)\n",
    "C = TypeVar('C', bound=BaseChatModel)\n",
    "S = TypeVar('S', bound=BaseChatModel)\n",
    "\n",
    "def _clamp_temperature(temp: float) -> float:\n",
    "    if temp < 1.0:\n",
    "        return 1.0\n",
    "    elif temp >= 5.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return max(0.0, min(1.0, temp))    \n",
    "\n",
    "def _textualize_model_input(content: Any) -> str:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "\n",
    "    elif isinstance(content, list):\n",
    "        text_chunks = []\n",
    "        for item in content:\n",
    "            if isinstance(item, dict) and item.get('type') == 'text':\n",
    "                text_value = item.get('text', '')\n",
    "                text_chunks.append(text_value)\n",
    "        return \" \".join(text_chunks)\n",
    "\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def _chunk_pairs(pairs: List[Dict[str, Any]]):\n",
    "    if len(pairs) % 2 != 0:\n",
    "        raise ValueError('pairs must contain an even number of elements.')\n",
    "    for i in range(0, len(pairs), 2):\n",
    "        yield pairs[i : i + 2]\n",
    "        \n",
    "class StreamingResponse(BaseModel):\n",
    "    type: str\n",
    "    content: str\n",
    "    token_usage: Dict[str, Any]\n",
    "    tool_calls: List[Dict[str, Any]]\n",
    "    vector_metadata: List[Dict[str, Any]]\n",
    "    session_id: str\n",
    "    message_id: str\n",
    "\n",
    "class ChatBot(RunnableSerializable[I, O]):\n",
    "    config: ChatBotConfig\n",
    "    graph: Optional[CompiledStateGraph] = None\n",
    "    chat_model: BaseChatModel = Field(default=None, exclude=True)\n",
    "    retry_model: BaseChatModel = Field(default=None, exclude=True)\n",
    "    safety_model: BaseChatModel = Field(default=None, exclude=True)\n",
    "    embeddings: Embeddings = Field(default=None, exclude=True)\n",
    "    vector_store: RedisVectorStoreTTL = Field(default=None, exclude=True)\n",
    "    message_history: MongoMessageHistory = Field(default=None, exclude=True)\n",
    "\n",
    "    alt: Optional[bool] = False\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True,\n",
    "    )\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def load_environment(self) -> Self:\n",
    "        graph = StateGraph(State)\n",
    "\n",
    "        hf = HuggingFaceInference(config=self.config,model_types={})\n",
    "        inference_engine = {\n",
    "            'tgi': hf,\n",
    "            'tei': hf,\n",
    "            'vllm': None,\n",
    "        }\n",
    "        \n",
    "        self.chat_model = inference_engine[self.config.llm.server]('chat_model')\n",
    "        self.safety_model = inference_engine[self.config.guardrails.server]('guardrails')\n",
    "        self.embeddings = inference_engine[self.config.embeddings.server]('embeddings')\n",
    "\n",
    "        if self.alt:\n",
    "            self.chat_model.llm = self.chat_model.llm.bind(\n",
    "                temperature=_clamp_temperature(self.config.llm.parameters['temperature']),\n",
    "                seed=42,\n",
    "            )\n",
    "\n",
    "        config = RedisConfig(**{\n",
    "            'redis_client': self.config.vectorstore.client,\n",
    "            'metadata_schema': self.config.vectorstore.metadata_schema,\n",
    "            'embedding_dimensions': self.config.embeddings.dimensions,\n",
    "            **VectorStoreSchema().model_dump()\n",
    "        })\n",
    "            \n",
    "        self.vector_store = RedisVectorStoreTTL(\n",
    "            self.embeddings, \n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        message_schema = MongoMessageHistorySchema(\n",
    "            session_id=self.config.message_history.session_id,\n",
    "            connection_string=self.config.message_history.url,\n",
    "            database_name=self.config.message_history.name,\n",
    "            collection_name=self.config.message_history.collection_name,\n",
    "            session_id_key=self.config.message_history.session_id_key,\n",
    "        )\n",
    "        self.message_history = MongoMessageHistory(message_schema)\n",
    "\n",
    "        self.graph = self._compile(graph)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    @override\n",
    "    def InputType(self) -> TypeAlias:\n",
    "        from langchain_core.prompt_values import (\n",
    "            ChatPromptValueConcrete,\n",
    "            StringPromptValue,\n",
    "        )\n",
    "\n",
    "        return Union[\n",
    "            str,\n",
    "            Union[StringPromptValue, ChatPromptValueConcrete],\n",
    "            list[AnyMessage],\n",
    "        ]\n",
    "\n",
    "    async def invoke(\n",
    "        self,\n",
    "        input: I,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "        *,\n",
    "        stop: Optional[list[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> O:\n",
    "        \"\"\"Implementation soon\"\"\"\n",
    "        ...\n",
    "\n",
    "    async def ainvoke(\n",
    "        self,\n",
    "        input: I,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "        *,\n",
    "        stop: Optional[list[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> O:\n",
    "        \"\"\"Implementation soon\"\"\"\n",
    "        ...\n",
    "\n",
    "    async def stream(\n",
    "        self,\n",
    "        input: I,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "        *,\n",
    "        stop: Optional[list[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> O:\n",
    "        \"\"\"Implementation soon\"\"\"\n",
    "        ...\n",
    "\n",
    "    async def astream(\n",
    "        self,\n",
    "        input: I,\n",
    "        config: Optional[Dict[str, Any]] = None,\n",
    "        *,\n",
    "        stop: Optional[list[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncGenerator[StreamingResponse, None]:\n",
    "        if isinstance(input, ChatPromptValue):\n",
    "            input = input.to_messages()\n",
    "    \n",
    "        state = {\n",
    "            'messages': input,\n",
    "            'metadata': config['metadata'].get('vector_metadata', []),\n",
    "            'retrieval_mode': config['configurable'].get('retrieval_mode', 'similarity'),\n",
    "        }        \n",
    "\n",
    "        async for event in self.graph.astream(state, config, stream_mode='messages'):\n",
    "            ai_message_chunk, state_args = event\n",
    "            if not isinstance(ai_message_chunk, AIMessageChunk):\n",
    "                continue\n",
    "\n",
    "            yield StreamingResponse(**{\n",
    "                'type': ai_message_chunk.__class__.__name__,\n",
    "                'content': ai_message_chunk.content,\n",
    "                'token_usage': ai_message_chunk.additional_kwargs.get('token_usage', {}),\n",
    "                'tool_calls': ai_message_chunk.additional_kwargs.get('tool_calls', []),\n",
    "                'vector_metadata': state_args.get('vector_metadata', []),\n",
    "                'session_id': str(self.config.message_history.session_id),\n",
    "                'message_id': ai_message_chunk.additional_kwargs.get('uuid', ''),\n",
    "            })\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprompt_filter(state: State, metadata: Dict[str, Any]) -> RunnableLambda:\n",
    "        def create_preprompt_filter(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "            return {\n",
    "                **input_data,\n",
    "                'chat_history': [\n",
    "                    message for message in input_data.get('chat_history', [])\n",
    "                    if not isinstance(message, SystemMessage) or not message.additional_kwargs.get('preprompt', False)\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        return RunnableLambda(create_preprompt_filter).with_config(\n",
    "            run_name=f'filter_preprompt_chain_{state['route']}',\n",
    "            metadata=metadata\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_filter_expression(metadata: Dict[str, Any]) -> FilterExpression:\n",
    "        from functools import reduce\n",
    "        import operator\n",
    "\n",
    "        tag_expressions = [\n",
    "            Tag(key) == str(value)\n",
    "            for key, value in metadata.items() \n",
    "        ]\n",
    "        filter_expression = reduce(operator.and_, tag_expressions)\n",
    "        return filter_expression\n",
    "\n",
    "    def create_generic_chain(self, state: State, system_prompt: str) -> Runnable:\n",
    "        \"\"\"\n",
    "        Conform to answer key produced by create_retrieval_chain\n",
    "        using a custom output parser\n",
    "\n",
    "        Important note: passing only content in output parser instead of entire\n",
    "        AIMessage means that any metadata associated with that AIMessage is lost,\n",
    "        scuh as token_usage or logprobs\n",
    "        \"\"\"\n",
    "        answer_parser = RunnableLambda(lambda ai_message: {'answer': ai_message.content } )\n",
    "        chain = registry['chat_prompt_with_history'](system_prompt) | self.chat_model | answer_parser\n",
    "        \n",
    "        return chain.with_config(run_name=f'generic_chat_model_chain_{state['route']}')    \n",
    "\n",
    "    def create_history_aware_retriever(\n",
    "        self,\n",
    "        retriever: RetrieverLike,\n",
    "        prompt: BasePromptTemplate,\n",
    "        preprompt_filter: Optional[Runnable] = None,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Custom implementation to handle preprompt messages\"\"\"\n",
    "        def validate_history(input_data: Dict[str, Any]) -> bool:\n",
    "            return not input_data.get('chat_history')\n",
    "            \n",
    "        retrieve_documents = (preprompt_filter or RunnablePassthrough()) | RunnableBranch(\n",
    "            (\n",
    "                validate_history,\n",
    "                (lambda input_data: input_data['input']) | retriever,\n",
    "            ),\n",
    "            prompt\n",
    "            | self.chat_model\n",
    "            | StrOutputParser()\n",
    "            | retriever,\n",
    "        ).with_config(run_name='history_aware_retriever_chain')\n",
    "        \n",
    "        return retrieve_documents\n",
    "\n",
    "    def create_stuff_documents_chain(\n",
    "        self,\n",
    "        prompt: BasePromptTemplate,\n",
    "        preprompt_filter: Optional[Runnable] = None,\n",
    "    ) -> Runnable[Dict[str, Any], Any]:\n",
    "        \"\"\"Custom implementation to handle preprompt messages\"\"\"        \n",
    "        def format_docs(inputs: dict) -> str:\n",
    "            return DEFAULT_DOCUMENT_SEPARATOR.join(\n",
    "                format_document(doc, DEFAULT_DOCUMENT_PROMPT)\n",
    "                for doc in inputs['context']\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            (preprompt_filter or RunnablePassthrough())\n",
    "            | RunnablePassthrough.assign(context=format_docs).with_config(run_name='format_inputs')\n",
    "            | prompt\n",
    "            | self.chat_model\n",
    "            | StrOutputParser()\n",
    "        ).with_config(run_name='stuff_documents_chain')    \n",
    "\n",
    "    def create_context_aware_chain(self, state: State) -> Runnable:\n",
    "        metadata = state['metadata'][0]\n",
    "        system_prompt = state['messages'][0].content\n",
    "\n",
    "        filter_expression = self.create_filter_expression(metadata)\n",
    "        search_kwargs = {\n",
    "            'k': 6,\n",
    "            'filter': filter_expression,\n",
    "        }\n",
    "        if state['retrieval_mode'] == 'similarity_score_threshold':\n",
    "            search_kwargs['score_threshold'] = 0.8\n",
    "\n",
    "        retriever = self.vector_store.as_retriever(\n",
    "            search_type=state['retrieval_mode'],\n",
    "            search_kwargs=search_kwargs\n",
    "        ).with_config(\n",
    "            tags=[f'create_context_aware_chain_{state['route']}'],\n",
    "            metadata=metadata,           \n",
    "        )\n",
    "        \n",
    "        history_aware_retriever = self.create_history_aware_retriever(\n",
    "            retriever,\n",
    "            registry['contextualized_template'](),\n",
    "            preprompt_filter=self.preprompt_filter(state, metadata),\n",
    "        )\n",
    "\n",
    "        question_answer_chain = self.create_stuff_documents_chain(\n",
    "            registry['qa_template'](system_prompt),\n",
    "            preprompt_filter=self.preprompt_filter(state, metadata)\n",
    "        )\n",
    "        \n",
    "        return create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "    def create_multi_retriever_chain(\n",
    "        self, \n",
    "        retrievers: Tuple[List[Runnable], Dict[str, Any]],\n",
    "        state: State\n",
    "    ) -> Runnable:\n",
    "        context_prompt = registry['contextualized_template']()\n",
    "        retriever_map = {\n",
    "            f'Source {metadata['source']}': retriever\n",
    "            for retriever, metadata in retrievers\n",
    "        }\n",
    "        parallel_retrieval = RunnableParallel(retriever_map)\n",
    "\n",
    "        def combine_contexts(retrieved_results: dict, separator=DEFAULT_DOCUMENT_SEPARATOR) -> list:\n",
    "            combined_results = []\n",
    "            for key, docs in retrieved_results.items():\n",
    "                combined_docs = separator.join(doc.page_content for doc in docs)\n",
    "                combined_results.append(\n",
    "                    Document(page_content=f'Context from {key}:\\n{combined_docs}')\n",
    "                )\n",
    "\n",
    "            return combined_results\n",
    "        \n",
    "        combine_contexts_runnable = RunnableLambda(combine_contexts) \\\n",
    "            .with_config(run_name='combine_context_chain')\n",
    "\n",
    "        retrieve_documents = self.create_history_aware_retriever(\n",
    "            parallel_retrieval,\n",
    "            context_prompt,\n",
    "            preprompt_filter=self.preprompt_filter(state, {})\n",
    "        )\n",
    "        \n",
    "        return retrieve_documents | combine_contexts_runnable\n",
    "    \n",
    "    def create_multi_stuff_chain(self, state: State, system_prompt: str) -> Runnable:\n",
    "        qa_template = registry['qa_template'](system_prompt)\n",
    "\n",
    "        return self.create_stuff_documents_chain(\n",
    "            qa_template,\n",
    "            preprompt_filter=self.preprompt_filter(state, {})\n",
    "        )\n",
    "\n",
    "    def create_multicontext_aware_chain(self, state: State) -> Runnable:\n",
    "        system_prompt = state['messages'][0].content\n",
    "        retrievers = []\n",
    "        \n",
    "        for index, metadata in enumerate(state['metadata']):\n",
    "            filter_expression = self.create_filter_expression(metadata)\n",
    "            search_kwargs = {\n",
    "                'k': 6,\n",
    "                'filter': filter_expression,\n",
    "            }\n",
    "            if state['retrieval_mode'] == 'similarity_score_threshold':\n",
    "                search_kwargs['score_threshold'] = 0.8\n",
    "\n",
    "            retriever = self.vector_store.as_retriever(\n",
    "                search_type=state['retrieval_mode'],\n",
    "                search_kwargs=search_kwargs\n",
    "            ).with_config(\n",
    "                tags=[f'create_context_aware_chain_{index}_{state['route']}'],\n",
    "                metadata=metadata,           \n",
    "            )\n",
    "            retrievers.append((retriever, metadata))\n",
    "\n",
    "        multi_retriever_chain = self.create_multi_retriever_chain(retrievers, state)\n",
    "        stuffing_chain = self.create_multi_stuff_chain(state, system_prompt)\n",
    "        \n",
    "        multicontext_aware_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                context=multi_retriever_chain.with_config(run_name='retrieval_chain'),\n",
    "            ).assign(answer=stuffing_chain)\n",
    "        ).with_config(run_name='multicontext_aware_chain')\n",
    "\n",
    "        return multicontext_aware_chain\n",
    "\n",
    "    async def _aenter_chat_chain(self, run: Run, config: RunnableConfig, system_prompt: str) -> Optional[SystemMessage]:\n",
    "        \"\"\"On start runnable listener\"\"\"\n",
    "        collection = self.message_history.chat_message_history.collection\n",
    "        \n",
    "        document = collection.find_one({\n",
    "            'type': 'system', \n",
    "            'content': system_prompt, \n",
    "            self.message_history._schema.session_id_key: self.message_history._schema.session_id,\n",
    "        })\n",
    "        \n",
    "        if document is None:\n",
    "            await self.message_history.asystem(system_prompt, additional_kwargs={'preprompt': True})\n",
    "        else:\n",
    "            history_data = json.loads(document['History'])\n",
    "            \n",
    "            additional_kwargs = history_data.get('data', {}).get('additional_kwargs', {})\n",
    "            if not additional_kwargs.get('preprompt', False):\n",
    "                await self.message_history.asystem(system_prompt, additional_kwargs={'preprompt': True})\n",
    "\n",
    "    async def _aexit_chat_chain(self, run: Run, config: RunnableConfig) -> None:\n",
    "        \"\"\"On end runnable listener\"\"\"\n",
    "        collection = self.message_history.chat_message_history.collection\n",
    "        if(\n",
    "            ai_message := collection.find_one(\n",
    "                {\n",
    "                    'type': { '$in': ['ai', 'AIMessageChunk'] }, \n",
    "                    self.config.message_history.session_id_key: self.config.message_history.session_id,\n",
    "                }, \n",
    "                sort=[(\"createdAt\", DESCENDING)])\n",
    "        ) is not None:\n",
    "            chain = registry['summarization_template']() | self.chat_model.bind(stream=False)\n",
    "            summary = await chain.ainvoke({'input': ai_message['content']})\n",
    "            self.message_history.chat_message_history.add_summary(summary.content)    \n",
    "\n",
    "    async def generate_with_history(\n",
    "        self, \n",
    "        state: State, \n",
    "        chain: Runnable,\n",
    "        *,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "    ) -> dict:\n",
    "        system_prompt = state['messages'][0].content\n",
    "        human_message = state['messages'][-2]\n",
    "\n",
    "        if isinstance(human_message.content, str):\n",
    "            input_dict = {'input': human_message.content}\n",
    "        elif isinstance(human_message.content, list):\n",
    "            input_dict = { 'input': human_message.content[1]['text'] }\n",
    "\n",
    "        async def on_start(run: Run, config: RunnableConfig):\n",
    "            await self._aenter_chat_chain(run, config, system_prompt)\n",
    "\n",
    "        async def on_end(run: Run, config: RunnableConfig):\n",
    "            await self._aexit_chat_chain(run, config)\n",
    "\n",
    "        chain_with_history = self.message_history.get(chain).with_alisteners(\n",
    "            on_start=on_start,\n",
    "            on_end=on_end\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            not config\n",
    "            or 'configurable' not in config.configurable\n",
    "            or 'session_id' not in config.configurable['configurable']\n",
    "        ):\n",
    "            config = RunnableConfig(\n",
    "                configurable={\n",
    "                    'configurable': { 'session_id': self.config.message_history.session_id } # TODO: generalize to session_id\n",
    "                }\n",
    "            )\n",
    "        chain_values = await chain_with_history.ainvoke(input_dict, config=config['configurable'])\n",
    "        return { 'messages': [AIMessage(content=chain_values['answer'])] }\n",
    "    \n",
    "    def _compile(self, graph: StateGraph):\n",
    "        async def guardrails(state: State) -> State:\n",
    "            user_content = state['messages'][1].content\n",
    "            sanitized_text = _textualize_model_input(user_content)\n",
    "            ai_message = await self.safety_model.bind(stream=False).ainvoke([sanitized_text])\n",
    "\n",
    "            guardrails_message = AIMessage(\n",
    "                content=ai_message.content,\n",
    "                additional_kwargs={'guardrails': True},\n",
    "            )\n",
    "            return {**state, 'messages': [guardrails_message]}\n",
    "\n",
    "        def guardrails_condition(state: State) -> str:\n",
    "            last_msg: AIMessage = state[\"messages\"][-1]\n",
    "            text = last_msg.content.lower().strip('\\n')\n",
    "            if 'unsafe' in text:\n",
    "                return 'not_safe'\n",
    "            elif 'safe' in text:\n",
    "                return 'prefill_system_prompt'\n",
    "            else:\n",
    "                return 'not_safe'\n",
    "\n",
    "        def not_safe(_: State):\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessageChunk(\n",
    "                        content=\"Your request cannot be processed. (Content flagged as not safe.)\"\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        async def prefill_system_prompt(state: State) -> State:\n",
    "            import copy\n",
    "\n",
    "            system_message = state['messages'][0]\n",
    "            human_message = state['messages'][-2]\n",
    "            \n",
    "            if not isinstance(human_message.content, list):\n",
    "                return state\n",
    "\n",
    "            pairs = list(_chunk_pairs(human_message.content))\n",
    "            batches = []\n",
    "            for pair in pairs:\n",
    "                cloned_pair = [copy.deepcopy(item) for item in pair]\n",
    "                cloned_pair[1]['text'] = 'Describe the image in-depth.'\n",
    "                messages = [\n",
    "                    SystemMessage(content=system_message.content),\n",
    "                    HumanMessage(content=cloned_pair),\n",
    "                ]\n",
    "                batches.append(messages)\n",
    "            \n",
    "            non_streaming_model = self.chat_model.bind(stream=False)\n",
    "            ai_messages = await non_streaming_model.abatch(batches)\n",
    "\n",
    "            labeled_descs = []\n",
    "            for i, ai_message in enumerate(ai_messages, start=1):\n",
    "                desc = ai_message.content.strip()\n",
    "                labeled_descs.append(f\"**Image #{i}**:\\n{desc}\")\n",
    "            combined_desc = \"\\n\\n\".join(labeled_descs)\n",
    "\n",
    "            old_prompt = system_message.content\n",
    "            updated_prompt = (\n",
    "                old_prompt\n",
    "                + \"\\n\\n\"\n",
    "                + \"If the user asks about images, pictures, or photos, then use the following descriptions as if they were the actual images:\\n\"\n",
    "                + combined_desc\n",
    "            )\n",
    "\n",
    "            new_system_message = system_message.copy(update={'content': updated_prompt})\n",
    "            state['messages'][0] = new_system_message\n",
    "\n",
    "            return state\n",
    "\n",
    "        async def route_query(state: State):\n",
    "            \"\"\"\n",
    "            Account for scenarios:\n",
    "            - 'Explain this document' (where this is not specified and refers to upload)\n",
    "            - 'Compare these two documents' (where these is not specified and refers to multiple uploads)\n",
    "            - 'Explain why it has impact' (where it refers to previously vectorized data)\n",
    "            - 'Compare this and that' (where this is current upload and that is previously vectorized data)\n",
    "            - 'Explain something' (where no vector data but can tap into pretrained corpus of LLM)\n",
    "            \"\"\"\n",
    "            metadata = state['metadata']\n",
    "\n",
    "            if len(metadata) > 1:\n",
    "                return {'route': 'multi_doc_prompt', **state }\n",
    "            \n",
    "            if len(metadata) == 1 and 'source' in metadata[0]:\n",
    "                return {\"route\": 'single_doc_prompt', **state}\n",
    "        \n",
    "            human_prompt = _textualize_model_input(state['messages'][-2].content)\n",
    "            vector_filter = metadata[0]\n",
    "\n",
    "            filter_expression = (\n",
    "                (Tag(self.config.message_history.session_id_key) == str(self.config.message_history.session_id)) \n",
    "                & (Tag('uuid') == vector_filter['uuid'])\n",
    "            )\n",
    "            relevant_docs_with_score = await self.vector_store.asimilarity_search_with_score(\n",
    "                query=human_prompt,\n",
    "                k=20,\n",
    "                filter=filter_expression,\n",
    "            )\n",
    "            file_to_best_chunk = defaultdict(lambda: (None, float('inf')))\n",
    "            for doc, dist in relevant_docs_with_score:\n",
    "                fname = doc.metadata.get('source', '')                \n",
    "                if dist < file_to_best_chunk[fname][1]:\n",
    "                    file_to_best_chunk[fname] = (doc, dist)\n",
    "\n",
    "            best_metadata = [doc.metadata for (doc, _) in file_to_best_chunk.values() if doc is not None]\n",
    "\n",
    "            num_files = len(best_metadata)\n",
    "            if num_files > 1:\n",
    "                route = 'multi_doc_prompt'\n",
    "            elif num_files == 1:\n",
    "                route = 'single_doc_prompt'\n",
    "            else:\n",
    "                route = 'pretrained_corpus_prompt'\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                'route': route,\n",
    "                'metadata': best_metadata,\n",
    "            }  \n",
    "\n",
    "        def route_query_condition(state: State) -> str:\n",
    "            return state['route']\n",
    "        \n",
    "        async def single_doc_prompt(state: State) -> dict:\n",
    "            \"\"\"\n",
    "            Generate prompt for single document\n",
    "            \"\"\"\n",
    "            chain = self.create_context_aware_chain(state)\n",
    "            messages = await self.generate_with_history(state, chain)\n",
    "            return messages\n",
    "\n",
    "        async def multi_doc_prompt(state: State) -> Dict[str, Any]:\n",
    "            \"\"\"\n",
    "            Generate prompt for multiple documents\n",
    "            \"\"\"\n",
    "            chain = self.create_multicontext_aware_chain(state)\n",
    "            messages = await self.generate_with_history(state, chain)\n",
    "            return messages\n",
    "\n",
    "        async def pretrained_corpus_prompt(state: State) -> Dict[str, Any]:\n",
    "            \"\"\"\n",
    "            Generate prompt for pretrained corpus\n",
    "            \"\"\"\n",
    "            system_prompt = state['messages'][0].content\n",
    "            chain = self.create_generic_chain(state, system_prompt)\n",
    "            messages = await self.generate_with_history(state, chain)\n",
    "            return messages\n",
    "\n",
    "        graph.add_node('guardrails', guardrails)\n",
    "        graph.add_node('not_safe', not_safe)\n",
    "        graph.add_node('prefill_system_prompt', prefill_system_prompt)\n",
    "        graph.add_node('route_query', route_query)\n",
    "        graph.add_node('single_doc_prompt', single_doc_prompt)\n",
    "        graph.add_node('multi_doc_prompt', multi_doc_prompt)\n",
    "        graph.add_node('pretrained_corpus_prompt', pretrained_corpus_prompt)\n",
    "\n",
    "        graph.add_edge(START, 'guardrails')\n",
    "        graph.add_conditional_edges(\n",
    "            'guardrails',\n",
    "            guardrails_condition,\n",
    "            {\n",
    "                'prefill_system_prompt': 'prefill_system_prompt',\n",
    "                'not_safe': 'not_safe'\n",
    "            }\n",
    "        )\n",
    "        graph.add_edge('prefill_system_prompt', 'route_query')\n",
    "        graph.add_edge('not_safe', END)\n",
    "        graph.add_conditional_edges(\n",
    "            'route_query',\n",
    "            route_query_condition,\n",
    "            {\n",
    "                #'vectorstore': 'vectorstore',\n",
    "                #'dataframe_tool': 'dataframe_tool',\n",
    "                'single_doc_prompt': 'single_doc_prompt',\n",
    "                'multi_doc_prompt': 'multi_doc_prompt',\n",
    "                'pretrained_corpus_prompt': 'pretrained_corpus_prompt',\n",
    "            }\n",
    "        )\n",
    "        #graph.add_edge('vectorstore', END)\n",
    "        #graph.add_edge('dataframe_tool', END)\n",
    "        graph.add_edge('single_doc_prompt', END)\n",
    "        graph.add_edge('multi_doc_prompt', END)\n",
    "        graph.add_edge('pretrained_corpus_prompt', END)\n",
    "        return graph.compile()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Determine if multiple candidate completions (\n",
    "        use both semantic similarity and max marginal\n",
    "        relevance\n",
    "        )\n",
    "        Go to vector store find vectors based on the single\n",
    "        doc\n",
    "        AND ALSO find vectors based on the query for the given\n",
    "        conversation_id (just in case they ask to compare with\n",
    "        a previous doc)\n",
    "        \"\"\"    The graph implementation is newer concept, slowly migrating towards it. As you can see, I build up a runnable chain that I later pass to RunnableWithMessageHistory. As an example let's look at the path of single_doc_prompt:         async def single_doc_prompt(state: State) -> dict:\n",
    "            \"\"\"\n",
    "            Generate prompt for single document\n",
    "            \"\"\"\n",
    "            chain = self.create_context_aware_chain(state)\n",
    "            messages = await self.generate_with_history(state, chain)\n",
    "            return messages     In the method create_context_aware_chain that is the chain that will be passed to the RunnableWithMessageHistory. How is RunnableWithMessageHistory able to detect which aimessage in the chain to store in database and not. For example: in the create_history_aware_retriever, we invoke an llm:     def create_history_aware_retriever(\n",
    "        self,\n",
    "        retriever: RetrieverLike,\n",
    "        prompt: BasePromptTemplate,\n",
    "        preprompt_filter: Optional[Runnable] = None,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Custom implementation to handle preprompt messages\"\"\"\n",
    "        def validate_history(input_data: Dict[str, Any]) -> bool:\n",
    "            return not input_data.get('chat_history')\n",
    "            \n",
    "        retrieve_documents = (preprompt_filter or RunnablePassthrough()) | RunnableBranch(\n",
    "            (\n",
    "                validate_history,\n",
    "                (lambda input_data: input_data['input']) | retriever,\n",
    "            ),\n",
    "            prompt\n",
    "            | self.chat_model\n",
    "            | StrOutputParser()\n",
    "            | retriever,\n",
    "        ).with_config(run_name='history_aware_retriever_chain')\n",
    "        \n",
    "        return retrieve_documents   All the llm does is it takes the prompt, it pulls some history and adds context to prompt such as replacing \"it\" with specific context. Yet this model invocation is never stored as the new created aimessage. How does RunnableWithHistory know now to create this AIMessage but then later creates the other in here:     def create_stuff_documents_chain(\n",
    "        self,\n",
    "        prompt: BasePromptTemplate,\n",
    "        preprompt_filter: Optional[Runnable] = None,\n",
    "    ) -> Runnable[Dict[str, Any], Any]:\n",
    "        \"\"\"Custom implementation to handle preprompt messages\"\"\"        \n",
    "        def format_docs(inputs: dict) -> str:\n",
    "            return DEFAULT_DOCUMENT_SEPARATOR.join(\n",
    "                format_document(doc, DEFAULT_DOCUMENT_PROMPT)\n",
    "                for doc in inputs['context']\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            (preprompt_filter or RunnablePassthrough())\n",
    "            | RunnablePassthrough.assign(context=format_docs).with_config(run_name='format_inputs')\n",
    "            | prompt\n",
    "            | self.chat_model\n",
    "            | StrOutputParser()\n",
    "        ).with_config(run_name='stuff_documents_chain')  in both cases, we use self.chat_model which is of type BaseChatModel yet one gets created as aimessage, the other not.\n",
    "\n",
    "Reasoned for 8 seconds\n",
    "In RunnableWithMessageHistory, only the chain’s final output gets added to the chat history – not every intermediate LLM call. In other words:\n",
    "\n",
    "Every LLM invocation in your chain (e.g. inside create_history_aware_retriever or create_stuff_documents_chain) might produce an AIMessage internally as part of the pipeline.\n",
    "But RunnableWithMessageHistory looks only at the final return of the chain once it finishes. It then identifies the user’s input message(s) plus the final AI message(s) in that final output. Those are what it stores in your database.\n",
    "How RunnableWithMessageHistory does this\n",
    "Attaching a “listener”\n",
    "RunnableWithMessageHistory wraps your chain with an “on_end” (or “on_end_async”) listener that runs _exit_history (or _aexit_history).\n",
    "In _exit_history:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# 1) Get the final run outputs\n",
    "output_val = load(run.outputs)\n",
    "output_messages = self._get_output_messages(output_val)\n",
    "\n",
    "# 2) Store them with hist.add_messages(...)\n",
    "_get_output_messages() looks for an AIMessage (or list of them) in the final chain output. By default, if your chain returns a dictionary with 'answer': AIMessage(...), it sees that as the final AI message.\n",
    "Intermediate LLM calls never appear in that final dictionary. They’re ephemeral steps in the pipeline. So _exit_history doesn’t see them, and thus never writes them to Mongo.\n",
    "Concretely in your example\n",
    "create_history_aware_retriever(...) calls self.chat_model to produce some text. That partial text is used to refine a prompt or retrieve documents – but the chain eventually moves on.\n",
    "The output from that partial LLM call is not the final chain return. Instead, your chain might produce something like {\"context\": <list of docs>}.\n",
    "Eventually you reach create_stuff_documents_chain(...): ... | self.chat_model | StrOutputParser(). That portion typically yields the final text answer. The chain’s final return might look like {\"answer\": \"Here’s your final answer\"} or a single AIMessage(...).\n",
    "Because RunnableWithMessageHistory sees only that final object as the “chain output,” it’s the only one stored in the DB.\n",
    "Why only the final answer is stored\n",
    "By default, RunnableWithMessageHistory is designed to store one “user input” plus one “assistant output” for each chain invocation. It does not track every intermediate node.\n",
    "If you want to store multiple LLM calls, you’d have to either:\n",
    "Return them in the chain’s final dictionary so _exit_history sees them, or\n",
    "Create custom logic in your chain to do partial updates to the DB.\n",
    "But out of the box, RunnableWithMessageHistory is meant to store “the user’s question” and “the final AI answer” from each chain run. All other AI calls along the way remain internal.\n",
    "\n",
    "You said:\n",
    "When this chain is invoked, exactly what happens:         def format_docs(inputs: dict) -> str:\n",
    "            return DEFAULT_DOCUMENT_SEPARATOR.join(\n",
    "                format_document(doc, DEFAULT_DOCUMENT_PROMPT)\n",
    "                for doc in inputs['context']\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            (preprompt_filter or RunnablePassthrough())\n",
    "            | RunnablePassthrough.assign(context=format_docs).with_config(run_name='format_inputs')\n",
    "            | prompt\n",
    "            | self.chat_model\n",
    "            | StrOutputParser()\n",
    "        ).with_config(run_name='stuff_documents_chain')    \n",
    "\n",
    "Reasoned about `RunnableWithMessageHistory` behavior for 7 seconds\n",
    "Here’s the high-level flow when you call, for example, my_chain.invoke({\"context\": [...docs...]}) on that snippet:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def format_docs(inputs: dict) -> str:\n",
    "    return DEFAULT_DOCUMENT_SEPARATOR.join(\n",
    "        format_document(doc, DEFAULT_DOCUMENT_PROMPT)\n",
    "        for doc in inputs[\"context\"]\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    (preprompt_filter or RunnablePassthrough())\n",
    "    | RunnablePassthrough.assign(context=format_docs)\n",
    "    | prompt\n",
    "    | self.chat_model\n",
    "    | StrOutputParser()\n",
    ").with_config(run_name=\"stuff_documents_chain\")\n",
    "Input → (preprompt_filter or RunnablePassthrough())\n",
    "\n",
    "You pass in some dictionary (for example: {\"context\": [Document(...), ...], \"input\": \"...\"}).\n",
    "If preprompt_filter exists (another Runnable), it processes that input. Otherwise, RunnablePassthrough() simply passes the dictionary unchanged.\n",
    "The result is still a dictionary at this point.\n",
    "→ RunnablePassthrough.assign(context=format_docs)\n",
    "\n",
    "Now the chain calls format_docs(inputs) on the dictionary from step 1.\n",
    "format_docs iterates over inputs[\"context\"] (which should be a list of Documents), merges them into a string, and returns it.\n",
    "Then .assign(context=...) sets that result as the new \"context\" field in the dictionary.\n",
    "So if you originally had {\"context\": [Document1, ...], \"input\": \"...\"}, by the end of this step you have something like:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "{\n",
    "  \"context\": \"<formatted string of docs>\",\n",
    "  \"input\": \"...\",\n",
    "  ... # plus other keys from the original\n",
    "}\n",
    "→ prompt\n",
    "\n",
    "Next, the dictionary (with \"context\" now a string) is fed into your BasePromptTemplate (named prompt).\n",
    "That template typically expects certain keys (for example, it might have placeholders like {context}, {input} in the template).\n",
    "It substitutes them to build a PromptValue or ChatPromptValue, i.e. the prompt text/messages for the LLM.\n",
    "→ self.chat_model\n",
    "\n",
    "The ChatPromptValue from step 3 is passed to your model (self.chat_model).\n",
    "The model then generates a response (for example, an AIMessage), presumably applying the newly combined context as part of the prompt.\n",
    "→ StrOutputParser()\n",
    "\n",
    "Finally, the LLM’s output (an AIMessage or text) goes through StrOutputParser(), which usually extracts just the .content as a string.\n",
    "That is the final output of the chain. If you call chain.invoke(...), you end up with a plain string from this parser.\n",
    "So in summary:\n",
    "\n",
    "Possibly filter/transform the input dictionary with preprompt_filter.\n",
    "Convert the list of documents in \"context\" into one long text string (format_docs), store that back in \"context\".\n",
    "Use that dictionary to fill the prompt template.\n",
    "Pass the prompt to self.chat_model to generate a textual response.\n",
    "Parse that response to a string with StrOutputParser.\n",
    "Return that final string to the caller.\n",
    "You said:\n",
    "Where are the format_docs coming from? Where are they being loaded?\n",
    "\n",
    "Reasoned for 10 seconds\n",
    "In your snippet:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "return (\n",
    "    (preprompt_filter or RunnablePassthrough())\n",
    "    | RunnablePassthrough.assign(context=format_docs).with_config(run_name='format_inputs')\n",
    "    | prompt\n",
    "    | self.chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "the call format_docs(inputs) relies on inputs[\"context\"] – meaning your pipeline expects to see a dictionary (the “inputs”) containing a key \"context\" which is a list of Document objects. Something upstream in the chain is responsible for putting that list of documents in \"context\". Typically, it’s one of two scenarios:\n",
    "\n",
    "The chain’s caller provides the documents when calling .invoke() or .ainvoke().\n",
    "For example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "docs = [Document(page_content=\"Doc 1...\"), Document(page_content=\"Doc 2...\")]\n",
    "output = my_chain.invoke({\"context\": docs, \"input\": \"Please summarize\"})\n",
    "Then format_docs sees docs in inputs[\"context\"].\n",
    "\n",
    "A previous step in the chain stored documents under \"context\".\n",
    "For instance, if you have a retrieval step like retriever.invoke(...) returning [Document(...), ...], you might do:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "chain_part = RunnablePassthrough.assign(context=retriever)  # store retriever's doc list\n",
    "# ...\n",
    "chain_part | (prompt | self.chat_model)\n",
    "In that case, the retrieval step’s output becomes the \"context\" field for the next step.\n",
    "\n",
    "Therefore, the “format_docs” function sees inputs[\"context\"] because something earlier in the pipeline attached a list of Documents to that key. That “something” might be a retrieval chain, user input, or any other node in the pipeline that sets \"context\" in the dictionary.\n",
    "\n",
    "You said:\n",
    "And create_retriever_chain fills in that resulting text under answer: def create_retrieval_chain(\n",
    "    retriever: Union[BaseRetriever, Runnable[dict, RetrieverOutput]],\n",
    "    combine_docs_chain: Runnable[Dict[str, Any], str],\n",
    ") -> Runnable:\n",
    "    \"\"\"Create retrieval chain that retrieves documents and then passes them on.\n",
    "\n",
    "    Args:\n",
    "        retriever: Retriever-like object that returns list of documents. Should\n",
    "            either be a subclass of BaseRetriever or a Runnable that returns\n",
    "            a list of documents. If a subclass of BaseRetriever, then it\n",
    "            is expected that an input key be passed in - this is what\n",
    "            is will be used to pass into the retriever. If this is NOT a\n",
    "            subclass of BaseRetriever, then all the inputs will be passed\n",
    "            into this runnable, meaning that runnable should take a dictionary\n",
    "            as input.\n",
    "        combine_docs_chain: Runnable that takes inputs and produces a string output.\n",
    "            The inputs to this will be any original inputs to this chain, a new\n",
    "            context key with the retrieved documents, and chat_history (if not present\n",
    "            in the inputs) with a value of [] (to easily enable conversational\n",
    "            retrieval.\n",
    "\n",
    "    Returns:\n",
    "        An LCEL Runnable. The Runnable return is a dictionary containing at the very\n",
    "        least a context and answer key.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            # pip install -U langchain langchain-community\n",
    "\n",
    "            from langchain_community.chat_models import ChatOpenAI\n",
    "            from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "            from langchain.chains import create_retrieval_chain\n",
    "            from langchain import hub\n",
    "\n",
    "            retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "            llm = ChatOpenAI()\n",
    "            retriever = ...\n",
    "            combine_docs_chain = create_stuff_documents_chain(\n",
    "                llm, retrieval_qa_chat_prompt\n",
    "            )\n",
    "            retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "            retrieval_chain.invoke({\"input\": \"...\"})\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(retriever, BaseRetriever):\n",
    "        retrieval_docs: Runnable[dict, RetrieverOutput] = retriever\n",
    "    else:\n",
    "        retrieval_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "    retrieval_chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=retrieval_docs.with_config(run_name=\"retrieve_documents\"),\n",
    "        ).assign(answer=combine_docs_chain)\n",
    "    ).with_config(run_name=\"retrieval_chain\")\n",
    "\n",
    "    return retrieval_chain    But then if the result is text, again how does RunnableWithMessageHistory know the AIMessage if all that chain outputs is text?\n",
    "Reasoned for a second\n",
    "Because RunnableWithMessageHistory has a small piece of logic that, at the very end, converts the chain’s final output (the text under \"answer\") into an AIMessage. Specifically, its _get_output_messages(...) method:\n",
    "\n",
    "Sees a dictionary like {\"context\": [...], \"answer\": \"some text from the chain\"}.\n",
    "Because output_messages_key=\"answer\", it takes that string \"some text from the chain\" and converts it to an AIMessage(content=\"some text from the chain\").\n",
    "Then it stores that AIMessage in the database.\n",
    "So from your chain’s perspective, all it returns is a dictionary with a string under \"answer\". But RunnableWithMessageHistory knows, “The final message is at output_messages_key='answer'; if that is a string, I wrap it as an AIMessage(content=that_string).” That’s how the text becomes an AIMessage for your chat history.\n",
    "\n",
    "You said:\n",
    "What is the eventual output of this chain:     def create_history_aware_retriever(\n",
    "        self,\n",
    "        retriever: RetrieverLike,\n",
    "        prompt: BasePromptTemplate,\n",
    "        preprompt_filter: Optional[Runnable] = None,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Custom implementation to handle preprompt messages\"\"\"\n",
    "        def validate_history(input_data: Dict[str, Any]) -> bool:\n",
    "            return not input_data.get('chat_history')\n",
    "            \n",
    "        retrieve_documents = (preprompt_filter or RunnablePassthrough()) | RunnableBranch(\n",
    "            (\n",
    "                validate_history,\n",
    "                (lambda input_data: input_data['input']) | retriever,\n",
    "            ),\n",
    "            prompt\n",
    "            | self.chat_model\n",
    "            | StrOutputParser()\n",
    "            | retriever,\n",
    "        ).with_config(run_name='history_aware_retriever_chain')\n",
    "        \n",
    "        return retrieve_documents\n",
    "\n",
    "Reasoned about create_history_aware_retriever chain output\n",
    "Looking at this snippet:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def create_history_aware_retriever(\n",
    "    self,\n",
    "    retriever: RetrieverLike,\n",
    "    prompt: BasePromptTemplate,\n",
    "    preprompt_filter: Optional[Runnable] = None,\n",
    ") -> Runnable:\n",
    "    def validate_history(input_data: Dict[str, Any]) -> bool:\n",
    "        return not input_data.get('chat_history')\n",
    "            \n",
    "    retrieve_documents = (preprompt_filter or RunnablePassthrough()) | RunnableBranch(\n",
    "        (\n",
    "            validate_history,\n",
    "            (lambda input_data: input_data['input']) | retriever,\n",
    "        ),\n",
    "        prompt\n",
    "        | self.chat_model\n",
    "        | StrOutputParser()\n",
    "        | retriever,\n",
    "    ).with_config(run_name='history_aware_retriever_chain')\n",
    "    \n",
    "    return retrieve_documents\n",
    "the eventual final output of this chain is a list of Documents produced by the retriever. Here’s how it works step by step:\n",
    "\n",
    "(preprompt_filter or RunnablePassthrough()):\n",
    "\n",
    "If you have a preprompt_filter runnable, it modifies the incoming dictionary; otherwise, the RunnablePassthrough leaves it as is.\n",
    "| RunnableBranch(...):\n",
    "\n",
    "RunnableBranch takes two branches and a condition function (validate_history):\n",
    "\n",
    "Condition: validate_history(input_data) → bool\n",
    "If validate_history returns True → use the first branch. Otherwise → use the second branch.\n",
    "First branch:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "(lambda input_data: input_data[\"input\"]) | retriever\n",
    "That effectively says:\n",
    "\n",
    "Grab input_data[\"input\"] (usually the user query).\n",
    "Pass that string to retriever to get a list of Documents.\n",
    "Second branch:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "prompt\n",
    "| self.chat_model\n",
    "| StrOutputParser()\n",
    "| retriever\n",
    "That means we:\n",
    "\n",
    "Format the dictionary with prompt (which might incorporate chat history or other template fields).\n",
    "Call self.chat_model to get some generated text.\n",
    "Parse that LLM text into a string (StrOutputParser()).\n",
    "Finally, pass that string to retriever, which also returns a list of Documents.\n",
    "At the end:\n",
    "No matter which branch is taken, the final piece is always retriever – so the chain’s final output is the list of Documents returned by that retriever call.\n",
    "\n",
    "Hence, whether we skip chat history or not, the chain returns a list of Documents that downstream steps can use.\n",
    "\n",
    "You said:\n",
    "Ok so one chain populates the documents for context. The other chain generates the chat completion. But of course that second chain will need access to the context of documents. I don't see how that is happening here: def create_retrieval_chain(\n",
    "    retriever: Union[BaseRetriever, Runnable[dict, RetrieverOutput]],\n",
    "    combine_docs_chain: Runnable[Dict[str, Any], str],\n",
    ") -> Runnable:\n",
    "    \"\"\"Create retrieval chain that retrieves documents and then passes them on.\n",
    "\n",
    "    Args:\n",
    "        retriever: Retriever-like object that returns list of documents. Should\n",
    "            either be a subclass of BaseRetriever or a Runnable that returns\n",
    "            a list of documents. If a subclass of BaseRetriever, then it\n",
    "            is expected that an input key be passed in - this is what\n",
    "            is will be used to pass into the retriever. If this is NOT a\n",
    "            subclass of BaseRetriever, then all the inputs will be passed\n",
    "            into this runnable, meaning that runnable should take a dictionary\n",
    "            as input.\n",
    "        combine_docs_chain: Runnable that takes inputs and produces a string output.\n",
    "            The inputs to this will be any original inputs to this chain, a new\n",
    "            context key with the retrieved documents, and chat_history (if not present\n",
    "            in the inputs) with a value of [] (to easily enable conversational\n",
    "            retrieval.\n",
    "\n",
    "    Returns:\n",
    "        An LCEL Runnable. The Runnable return is a dictionary containing at the very\n",
    "        least a context and answer key.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            # pip install -U langchain langchain-community\n",
    "\n",
    "            from langchain_community.chat_models import ChatOpenAI\n",
    "            from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "            from langchain.chains import create_retrieval_chain\n",
    "            from langchain import hub\n",
    "\n",
    "            retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "            llm = ChatOpenAI()\n",
    "            retriever = ...\n",
    "            combine_docs_chain = create_stuff_documents_chain(\n",
    "                llm, retrieval_qa_chat_prompt\n",
    "            )\n",
    "            retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "            retrieval_chain.invoke({\"input\": \"...\"})\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(retriever, BaseRetriever):\n",
    "        retrieval_docs: Runnable[dict, RetrieverOutput] = retriever\n",
    "    else:\n",
    "        retrieval_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "    retrieval_chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=retrieval_docs.with_config(run_name=\"retrieve_documents\"),\n",
    "        ).assign(answer=combine_docs_chain)\n",
    "    ).with_config(run_name=\"retrieval_chain\")\n",
    "\n",
    "    return retrieval_chain\n",
    "Reasoned for a couple of seconds\n",
    "In create_retrieval_chain, the “combine docs” step does receive access to the retrieved documents, thanks to the line:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "RunnablePassthrough.assign(\n",
    "    context=retrieval_docs.with_config(run_name=\"retrieve_documents\")\n",
    ").assign(answer=combine_docs_chain)\n",
    "Here’s how that works step by step:\n",
    "\n",
    "retrieval_docs (the first half) returns a list of documents from the retriever.\n",
    "RunnablePassthrough.assign(context=...) means:\n",
    "“Take the original dictionary that came in (which probably had keys like {\"input\": \"...\", ...}),\n",
    "Then add/override a new key \"context\" that gets its value from retrieval_docs.”\n",
    "So now your dictionary looks like\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "{\n",
    "  \"input\": \"...\",\n",
    "  ...,\n",
    "  \"context\": [Document(...), Document(...), ...]\n",
    "}\n",
    "Next step: .assign(answer=combine_docs_chain) means we call combine_docs_chain with that entire dictionary (which now has \"context\").\n",
    "Because combine_docs_chain expects to see inputs[\"context\"], it can read the docs from the newly filled context field.\n",
    "combine_docs_chain uses or summarizes those docs, returning something like a string final answer.\n",
    "Hence, the “combine docs” chain sees \"context\" in the same dictionary because the “retrieval docs” are assigned to the \"context\" key before calling combine_docs_chain. That is exactly how the second step has access to the retrieved documents.\n",
    "\n",
    "You said:\n",
    "So this retrieves the documents to use as context:         history_aware_retriever = self.create_history_aware_retriever(\n",
    "            retriever,\n",
    "            registry['contextualized_template'](),\n",
    "            preprompt_filter=self.preprompt_filter(state, metadata),\n",
    "        )     ANd this populates the context in the prompt and sends it off to llm for chat completion:         question_answer_chain = self.create_stuff_documents_chain(\n",
    "            registry['qa_template'](system_prompt),\n",
    "            preprompt_filter=self.preprompt_filter(state, metadata)\n",
    "        )    This is what I would like to do. I want to return multiple responses to the user but for one response instead of using the retriever that uses mmr, I want to use one that just uses semantic similarity, and i want to pass it off to the chat model that is has runnable binding with a different temperature that is different from what user set. I want the two branches to run in parallel.\n",
    "\n",
    "Reasoned for 6 seconds\n",
    "Here’s one way to do it using LangChain’s RunnableParallel, so you can produce two distinct responses in parallel – each with:\n",
    "\n",
    "A different retrieval mode (e.g., MMR vs. similarity).\n",
    "A different LLM parameter (e.g., a different temperature).\n",
    "Then you return both outputs as a dictionary with two keys.\n",
    "\n",
    "1) Build two sub-chains\n",
    "Let’s say your normal chain is something like “retrieve docs, then run a ‘stuff documents’ chain with a certain prompt.” You can do two separate sub-chains, each with a different retrieval approach and/or different LLM parameters.\n",
    "\n",
    "Sub-chain A: mmr retrieval, plus a specialized LLM\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def create_mmr_chain(self, system_prompt: str, mmr_temp: float = 0.0) -> Runnable:\n",
    "    # 1) an mmr retriever\n",
    "    mmr_retriever = self.vector_store.as_retriever(\n",
    "        search_type=\"mmr\",  # or pass 'mmr' in search_kwargs if your VectorStore expects that\n",
    "        search_kwargs={\"k\": 6, \"lambda_mult\": 0.2}  # or whatever you want\n",
    "    )\n",
    "\n",
    "    # 2) a custom LLM binding with a different temperature\n",
    "    mmr_chat_model = self.chat_model.bind(temperature=mmr_temp)\n",
    "\n",
    "    # 3) chain that assigns the \"context\" with docs, then calls a \"stuff chain\"\n",
    "    # e.g. we can reuse your create_stuff_documents_chain with a new LLM\n",
    "    # but we need a separate step to do the retrieval\n",
    "    chain = RunnablePassthrough.assign(\n",
    "        context=mmr_retriever\n",
    "    ).assign(\n",
    "        answer=self.create_stuff_documents_chain(\n",
    "            registry[\"qa_template\"](system_prompt),\n",
    "            preprompt_filter=None,   # or whatever\n",
    "        )\n",
    "        # but patch the chain to use the mmr_chat_model\n",
    "        # easiest is to do something like .replace_llm(mmr_chat_model) or build a new chain\n",
    "    )\n",
    "\n",
    "    return chain.with_config(run_name=\"mmr_chain\")\n",
    "Sub-chain B: similarity retrieval, plus LLM with some user-set temperature\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def create_similarity_chain(self, system_prompt: str, user_temp: float = 0.8) -> Runnable:\n",
    "    # 1) similarity retriever\n",
    "    sim_retriever = self.vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}\n",
    "    )\n",
    "\n",
    "    # 2) user-chosen temperature\n",
    "    sim_chat_model = self.chat_model.bind(temperature=user_temp)\n",
    "\n",
    "    # 3) chain\n",
    "    chain = RunnablePassthrough.assign(\n",
    "        context=sim_retriever\n",
    "    ).assign(\n",
    "        answer=self.create_stuff_documents_chain(\n",
    "            registry[\"qa_template\"](system_prompt),\n",
    "            preprompt_filter=None,\n",
    "        )\n",
    "        # again, ensure that \"stuff_documents_chain\"\n",
    "        # uses sim_chat_model instead of your default self.chat_model\n",
    "    )\n",
    "\n",
    "    return chain.with_config(run_name=\"similarity_chain\")\n",
    "In each of these sub-chains, you do the steps:\n",
    "\n",
    "RunnablePassthrough.assign(context=retriever) so that the retriever’s list of docs goes under \"context\".\n",
    "Then you call your “stuff documents” chain with a specialized LLM.\n",
    "The chain produces {\"answer\": \"...some text...\"} at the end.\n",
    "2) Combine them in parallel\n",
    "Once you have these sub-chains, do something like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "mmr_chain = self.create_mmr_chain(system_prompt, mmr_temp=0.0)\n",
    "sim_chain = self.create_similarity_chain(system_prompt, user_temp=0.8)\n",
    "\n",
    "parallel = RunnableParallel({\n",
    "    \"mmr_result\": mmr_chain,\n",
    "    \"similarity_result\": sim_chain,\n",
    "})\n",
    "When you call:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "final_outputs = parallel.invoke({\"input\": user_input})\n",
    "you’ll get a dictionary:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "{\n",
    "  \"mmr_result\": {\n",
    "    \"context\": [...],   # if sub-chain retained that\n",
    "    \"answer\": \"Here’s the MMR-based answer\"\n",
    "  },\n",
    "  \"similarity_result\": {\n",
    "    \"context\": [...],\n",
    "    \"answer\": \"Here’s the similarity-based answer\"\n",
    "  }\n",
    "}\n",
    "Each sub-chain runs in parallel. If you do .astream(...), you can get partial outputs from each chain interleaved.\n",
    "\n",
    "3) Handling user input / system prompts\n",
    "You might want each chain to see the same user input. That’s easy – you pass the same dictionary to parallel.invoke(...). If each chain needs a system_prompt, you can either:\n",
    "\n",
    "Hardcode the system prompt in each sub-chain, or\n",
    "Store it in the input dictionary under some key, then read it in each sub-chain or prompt.\n",
    "4) If you want them both to store an AIMessage in DB\n",
    "By default, RunnableWithMessageHistory only logs a single AI response to your history. If you want to store both sub-chains’ final AI messages, you’ll need to customize _exit_history or produce a final list of messages in your chain’s final dictionary. Something like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "final_outputs = parallel.invoke({...})\n",
    "# final_outputs = {\n",
    "#   \"mmr_result\": {\"answer\": \"...\"},\n",
    "#   \"similarity_result\": {\"answer\": \"...\"},\n",
    "# }\n",
    "\n",
    "return {\n",
    "  \"multi_answers\": [\n",
    "    AIMessage(content=final_outputs[\"mmr_result\"][\"answer\"]),\n",
    "    AIMessage(content=final_outputs[\"similarity_result\"][\"answer\"]),\n",
    "  ]\n",
    "}\n",
    "so _exit_history sees multiple AI messages. Or you might keep it simpler, storing only one. That’s up to you.\n",
    "\n",
    "Putting it all together (pseudo-code)\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "async def multi_response_astream(self, user_input: str) -> AsyncIterator[dict]:\n",
    "    # Suppose we have a system prompt\n",
    "    system_prompt = \"You are an advanced question-answering system.\"\n",
    "\n",
    "    # Build each sub-chain\n",
    "    mmr_chain = self.create_mmr_chain(system_prompt, mmr_temp=0.0)\n",
    "    sim_chain = self.create_similarity_chain(system_prompt, user_temp=0.8)\n",
    "\n",
    "    # Wrap them in parallel\n",
    "    parallel = RunnableParallel({\n",
    "        \"mmr_result\": mmr_chain,\n",
    "        \"similarity_result\": sim_chain,\n",
    "    })\n",
    "\n",
    "    # Possibly also wrap it with RunnableWithMessageHistory if you want partial storage\n",
    "    # or store them manually\n",
    "    async for partial_output in parallel.astream({\"input\": user_input}):\n",
    "        yield partial_output\n",
    "At the end, you get two parallel sub-chains, each retrieving docs differently and using different LLM temperatures. They run concurrently, returning two final answers in a dictionary.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
