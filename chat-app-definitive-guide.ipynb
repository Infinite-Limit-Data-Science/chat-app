{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Router\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "User input does not need to be passed directly into the LLM. The prompt template provides additional context on the specific task at hand.\n",
    "\n",
    "Prompt Templates are designed for single-turn interactions. It typically accepts a single template string with placeholders that can be dynamically filled with inputs.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Translate the following English text to French: {text}\"\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "filled_prompt = prompt.format(text=\"Hello, how are you?\")\n",
    "print(filled_prompt)\n",
    "# Output: Translate the following English text to French: Hello, how are you?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Template\n",
    "\n",
    "ChatPromptTemplate is used for multi-turn interactions or prompts designed for chat-based language models. It is composed of messages like SystemMessage, HumanMessage, and AIMessage. These represent the context, user input, and AI responses, respectively.\n",
    "\n",
    "ChatPromptTemplate can take messages in various formats, including predefined message classes (e.g., SystemMessage, HumanMessage) as well as tuple formats. LangChain provides message classes such as SystemMessage, HumanMessage, AIMessage, and ChatMessage. These are structured representations for different roles in a chat.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessage, HumanMessage\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Can you help me with my math homework?\")\n",
    "])\n",
    "formatted_messages = chat_prompt.format_messages()\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "```\n",
    "\n",
    "Instead of using the predefined message classes, you can represent messages as tuples, where the first element is the role (e.g., \"system\", \"human\", \"assistant\"), and the second element is the content of the message.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Can you help me solve a quadratic equation?\"),\n",
    "    (\"assistant\", \"Of course! What's the equation?\")\n",
    "])\n",
    "formatted_messages = chat_prompt.format_messages()\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Template and Message Placeholder\n",
    "\n",
    "The MessagesPlaceholder in LangChain is a utility used within ChatPromptTemplate to insert dynamic chat histories or message sequences into a prompt. This is particularly useful when you want to incorporate conversation history dynamically without hardcoding all previous messages into the prompt.\n",
    "\n",
    "MessagesPlaceholder is commonly used when:\n",
    "- Maintaining Context: You want to include past messages in a chat.\n",
    "- Dynamic History: The history may change or be trimmed based on application logic.\n",
    "- Trimming: You can preprocess the chat_history (e.g., trim irrelevant messages) before passing it to the prompt.\n",
    "\n",
    "Here’s how to use MessagesPlaceholder in a ChatPromptTemplate:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"What do you think about {topic}?\")\n",
    "])\n",
    "```\n",
    "\n",
    "At runtime, you pass the actual chat history to replace the MessagesPlaceholder:\n",
    "\n",
    "```python\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Can you summarize quantum physics?\"),\n",
    "    AIMessage(content=\"Sure! Quantum physics deals with subatomic particles and wave-particle duality.\"),\n",
    "]\n",
    "\n",
    "formatted_messages = chat_prompt.format_messages(\n",
    "    chat_history=chat_history,  # Replace placeholder\n",
    "    topic=\"black holes\"\n",
    ")\n",
    "\n",
    "for message in formatted_messages:\n",
    "    print(f\"{message.type}: {message.content}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Template and LCEL\n",
    "\n",
    "PromptTemplate and ChatPromptTemplate implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n",
    "\n",
    "PromptTemplate accepts a dictionary (of the prompt variables) and returns a StringPromptValue. A ChatPromptTemplate accepts a dictionary and returns a ChatPromptValue.\n",
    "\n",
    "```python\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "prompt_val # StringPromptValue(text='Tell me a funny joke about chickens.')\n",
    "prompt_val.to_string() # 'Tell me a funny joke about chickens.'\n",
    "prompt_val.to_messages() # [HumanMessage(content='Tell me a funny joke about chickens.')]\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "chat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})\n",
    "chat_val.to_messages()\n",
    "# [SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompt Templates and Example Selectors\n",
    "\n",
    "The purpose of Example Selectors in LangChain is to dynamically choose the most relevant examples to include in a prompt. This is especially important when you have a large dataset of examples and cannot include all of them due to constraints like token limits or relevance to the current input. Example selectors ensure that the chosen examples are tailored to the specific input, improving the quality of responses from the language model.\n",
    "\n",
    "**Large language models like GPT often perform better with few-shot learning, where the prompt includes input-output examples to demonstrate the task. Including irrelevant or too many examples can lead to suboptimal responses or exceed token limits.**\n",
    "\n",
    "**Example Selectors dynamically choose the most relevant subset of examples based on the current input, ensuring optimal context is provided to the model.**\n",
    "\n",
    "LangChain provides several prebuilt example selectors, and you can also implement custom ones. Common types include:\n",
    "- Similarity Selector\n",
    "    - Uses semantic similarity between inputs and examples to decide which examples to choose.\n",
    "- Max Marginal Relevance (MMR) Selector\n",
    "    - Balances relevance and diversity by selecting examples that are similar to the input but not too similar to each other.\n",
    "- Length-Based Selector\n",
    "    - Chooses examples that fit within a token limit or are of a similar length to the input.\n",
    "- N-gram Overlap Selector\n",
    "    - Selects examples based on shared n-grams between the input and examples.\n",
    "\n",
    "Select by length example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\n",
    "\n",
    "```python\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    # The examples it has available to choose from.\n",
    "    examples=examples,\n",
    "    # The PromptTemplate being used to format the examples.\n",
    "    example_prompt=example_prompt,\n",
    "    # The maximum length that the formatted examples should be.\n",
    "    # Length is measured by the get_text_length function below.\n",
    "    max_length=25,\n",
    "    # The function used to get the length of a string, which is used\n",
    "    # to determine which examples to include. It is commented out because\n",
    "    # it is provided as a default value if none is specified.\n",
    "    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "```\n",
    "\n",
    "The MaxMarginalRelevanceExampleSelector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import (\n",
    "    MaxMarginalRelevanceExampleSelector,\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# Input is a feeling, so should select the happy/sad example as the first one\n",
    "print(mmr_prompt.format(adjective=\"worried\"))\n",
    "Give the antonym of every input\n",
    "\n",
    "Input: happy\n",
    "Output: sad\n",
    "\n",
    "Input: windy\n",
    "Output: calm\n",
    "\n",
    "Input: worried\n",
    "Output:\n",
    "\n",
    "\n",
    "# Let's compare this to what we would just get if we went solely off of similarity,\n",
    "# by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=2,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "print(similar_prompt.format(adjective=\"worried\"))\n",
    "Give the antonym of every input\n",
    "\n",
    "Input: happy\n",
    "Output: sad\n",
    "\n",
    "Input: sunny\n",
    "Output: gloomy\n",
    "\n",
    "Input: worried\n",
    "Output:\n",
    "```\n",
    "\n",
    "The NGramOverlapExampleSelector selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\n",
    "\n",
    "The selector allows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input.\n",
    "\n",
    "```python\n",
    "from langchain_community.example_selector.ngram_overlap import (\n",
    "    NGramOverlapExampleSelector,\n",
    ")\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a fictional translation task.\n",
    "examples = [\n",
    "    {\"input\": \"See Spot run.\", \"output\": \"Ver correr a Spot.\"},\n",
    "    {\"input\": \"My dog barks.\", \"output\": \"Mi perro ladra.\"},\n",
    "    {\"input\": \"Spot can run.\", \"output\": \"Spot puede correr.\"},\n",
    "]\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector(\n",
    "    # The examples it has available to choose from.\n",
    "    examples=examples,\n",
    "    # The PromptTemplate being used to format the examples.\n",
    "    example_prompt=example_prompt,\n",
    "    # The threshold, at which selector stops.\n",
    "    # It is set to -1.0 by default.\n",
    "    threshold=-1.0,\n",
    "    # For negative threshold:\n",
    "    # Selector sorts examples by ngram overlap score, and excludes none.\n",
    "    # For threshold greater than 1.0:\n",
    "    # Selector excludes all examples, and returns an empty list.\n",
    "    # For threshold equal to 0.0:\n",
    "    # Selector sorts examples by ngram overlap score,\n",
    "    # and excludes those with no ngram overlap with input.\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the Spanish translation of every input\",\n",
    "    suffix=\"Input: {sentence}\\nOutput:\",\n",
    "    input_variables=[\"sentence\"],\n",
    ")\n",
    "\n",
    "# An example input with large ngram overlap with \"Spot can run.\"\n",
    "# and no overlap with \"My dog barks.\"\n",
    "print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\n",
    "Give the Spanish translation of every input\n",
    "\n",
    "Input: Spot can run.\n",
    "Output: Spot puede correr.\n",
    "\n",
    "Input: See Spot run.\n",
    "Output: Ver correr a Spot.\n",
    "\n",
    "Input: My dog barks.\n",
    "Output: Mi perro ladra.\n",
    "\n",
    "Input: Spot can run fast.\n",
    "Output:\n",
    "\n",
    "Select by similarity selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # The number of examples to produce.\n",
    "    k=1,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# Input is a feeling, so should select the happy/sad example\n",
    "print(similar_prompt.format(adjective=\"worried\"))\n",
    "Give the antonym of every input\n",
    "\n",
    "Input: happy\n",
    "Output: sad\n",
    "\n",
    "Input: worried\n",
    "Output:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Chat Message Prompt Template\n",
    "\n",
    "FewShotPromptTemplate is a general-purpose template for few-shot prompting. Examples are stored as plain text templates or dictionaries. Examples are formatted using a simple PromptTemplate. Includes a prefix (instructions or context) and a suffix (the current input or question). Dynamically selects the most relevant examples using an ExampleSelector (e.g., SemanticSimilarityExampleSelector).\n",
    "\n",
    "FewShotChatMessagePromptTemplate is specifically designed for chat-based models (e.g., ChatGPT, Anthropic, or any model that uses chat messages). Formats examples into chat-style messages, distinguishing between system, human, and ai roles. Each example is converted into a sequence of chat messages (e.g., Human: Input\\nAI: Output). Uses ChatPromptTemplate to define message roles. Can dynamically insert examples in a conversational format using ExampleSelector. Suitable for conversational tasks where examples need to be formatted as chat exchanges.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"3+3\", \"output\": \"6\"},\n",
    "]\n",
    "\n",
    "few_shot_chat_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_chat_prompt.format())\n",
    "Human: 2+2\n",
    "AI: 4\n",
    "Human: 3+3\n",
    "AI: 6\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_chat_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "\n",
    "chain = final_prompt | ChatAnthropic(temperature=0.0)\n",
    "\n",
    "chain.invoke({\"input\": \"What's the square of a triangle?\"})\n",
    "AIMessage(content=' Triangles do not have a \"square\". A square refers to a shape with 4 equal sides and 4 right angles. Triangles have 3 sides and 3 angles.\\n\\nThe area of a triangle can be calculated using the formula:\\n\\nA = 1/2 * b * h\\n\\nWhere:\\n\\nA is the area \\nb is the base (the length of one of the sides)\\nh is the height (the length from the base to the opposite vertex)\\n\\nSo the area depends on the specific dimensions of the triangle. There is no single \"square of a triangle\". The area can vary greatly depending on the base and height measurements.', additional_kwargs={}, example=False)\n",
    "```\n",
    "\n",
    "Sometimes you may want to condition which examples are shown based on the input. For this, you can replace the examples with an example_selector.\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "    {\"input\": \"2+4\", \"output\": \"6\"},\n",
    "    {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"},\n",
    "    {\n",
    "        \"input\": \"Write me a poem about the moon\",\n",
    "        \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "# The prompt template will load examples by passing the input do the `select_examples` method\n",
    "example_selector.select_examples({\"input\": \"horse\"})\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "\n",
    "chain = final_prompt | ChatAnthropic(temperature=0.0)\n",
    "\n",
    "chain.invoke({\"input\": \"What's 3+3?\"})\n",
    "# AIMessage(content=' 3 + 3 = 6', additional_kwargs={}, example=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Prompt Templates\n",
    "\n",
    "It can make sense to \"partial\" a prompt template - e.g. pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\n",
    "\n",
    "One common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, foo and baz. If you get the foo value early on in the chain, but the baz value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the foo value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{foo}{bar}\")\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(partial_prompt.format(bar=\"baz\"))\n",
    "```\n",
    "\n",
    "The other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is a bit annoying. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date.\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))\n",
    "```\n",
    "\n",
    "You can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    partial_variables={\"date\": _get_datetime},\n",
    ")\n",
    "print(prompt.format(adjective=\"funny\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Composition\n",
    "\n",
    "LangChain provides a user friendly interface for composing different parts of prompts together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse.\n",
    "\n",
    "When working with string prompts, each template is joined together. You can work with either prompts directly or strings:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    + \", make it funny\"\n",
    "    + \"\\n\\nand in {language}\"\n",
    ")\n",
    "prompt\n",
    "# PromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}')\n",
    "```\n",
    "\n",
    "LangChain includes an abstraction PipelinePromptTemplate, which can be useful when you want to reuse parts of prompts. \n",
    "\n",
    "```python\n",
    "from langchain_core.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")\n",
    "\n",
    "pipeline_prompt.input_variables # ['example_q', 'person', 'input', 'example_a']\n",
    "\n",
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        person=\"Elon Musk\",\n",
    "        example_q=\"What's your favorite car?\",\n",
    "        example_a=\"Tesla\",\n",
    "        input=\"What's your favorite social media site?\",\n",
    "    )\n",
    ")\n",
    "You are impersonating Elon Musk.\n",
    "\n",
    "Here's an example of an interaction:\n",
    "\n",
    "Q: What's your favorite car?\n",
    "A: Tesla\n",
    "\n",
    "Now, do this for real!\n",
    "\n",
    "Q: What's your favorite social media site?\n",
    "A:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model\n",
    "\n",
    "Chat Models are built on top of LLMs. The LLM objects take string as input and output string. The ChatModel objects take a list of messages as input and output a message. While chat models use language models under the hood, the interface they use is a bit different. Rather than using a \"text in, text out\" API, they use an interface where \"chat messages\" are the inputs and outputs.\n",
    "\n",
    "**A chat model is a language model that uses chat messages as inputs and returns chat messages as outputs (as opposed to using plain text).** LangChain has integrations with many model providers (OpenAI, Cohere, Hugging Face, etc.) and exposes a standard interface to interact with all of these models.\n",
    "\n",
    "The chat model interface is based around messages rather than raw text. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, FunctionMessage and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model and LCEL\n",
    "\n",
    "Chat models implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n",
    "\n",
    "Chat models accept List[BaseMessage] as inputs, or objects which can be coerced to messages, including str (converted to HumanMessage) and PromptValue.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "\n",
    "# using invoke\n",
    "chat.invoke(messages)\n",
    "# AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to fit the noise in the training data, leading to poor generalization on unseen data. Regularization techniques introduce additional constraints or penalties to the model's objective function, discouraging it from becoming overly complex and promoting simpler and more generalizable models. Regularization helps to strike a balance between fitting the training data well and avoiding overfitting, leading to better performance on new, unseen data.\")\n",
    "\n",
    "# using stream\n",
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "# The purpose of model regularization is to prevent overfitting and improve the generalization of a machine learning model. Overfitting occurs when a model is too complex and learns the noise or random variations in the training data, which leads to poor performance on new, unseen data. Regularization techniques introduce additional constraints or penalties to the model's learning process, discouraging it from fitting the noise and reducing the complexity of the model. This helps to improve the model's ability to generalize well and make accurate predictions on unseen data.    \n",
    "\n",
    "# using batch\n",
    "chat.batch([messages])\n",
    "# [AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to learn the noise or random fluctuations in the training data, rather than the underlying patterns or relationships. Regularization techniques add a penalty term to the model's objective function, which discourages the model from becoming too complex and helps it generalize better to new, unseen data. This improves the model's ability to make accurate predictions on new data by reducing the variance and increasing the model's overall performance.\")]\n",
    "\n",
    "# using ainvoke\n",
    "await chat.ainvoke(messages)\n",
    "# AIMessage(content='The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning general patterns and relationships. This leads to poor performance on new, unseen data.\\n\\nRegularization techniques introduce additional constraints or penalties to the model during training, discouraging it from becoming overly complex. This helps to strike a balance between fitting the training data well and generalizing to new data. Regularization techniques can include adding a penalty term to the loss function, such as L1 or L2 regularization, or using techniques like dropout or early stopping. By regularizing the model, it encourages it to learn the most relevant features and reduce the impact of noise or outliers in the data.')\n",
    "\n",
    "# using astream\n",
    "async for chunk in chat.astream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "# The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning the underlying patterns. Regularization techniques help in reducing the complexity of the model by adding a penalty to the loss function. This penalty encourages the model to have smaller weights or fewer features, making it more generalized and less prone to overfitting. The goal is to find the right balance between fitting the training data well and being able to generalize well to unseen data.\n",
    "\n",
    "# using astream log\n",
    "async for chunk in chat.astream_log(messages):\n",
    "    print(chunk) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Langsmith\n",
    "\n",
    "All ChatModels come with built-in LangSmith tracing. Just set the following environment variables:\n",
    "\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=<your-api-key>\n",
    "```\n",
    "\n",
    "and any ChatModel invocation (whether it's nested in a chain or not) will automatically be traced. A trace will include inputs, outputs, latency, token usage, invocation params, environment params, and more.\n",
    "\n",
    "In LangSmith you can then provide feedback for any trace, compile annotated datasets for evals, debug performance in the playground, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model Message types\n",
    "\n",
    "ChatModels take a list of messages as input and return a message. There are a few different types of messages. All messages have a role and a content property. The role describes WHO is saying the message. LangChain has different message classes for different roles. The content property describes the content of the message. This can be a few different things:\n",
    "- A string (most models deal this type of content)\n",
    "- A List of dictionaries (this is used for multi-modal input, where the dictionary contains information about that input type and that input location)\n",
    "\n",
    "In addition, messages have an additional_kwargs property. This is where additional information about messages can be passed. This is largely used for input parameters that are provider specific and not general. The best known example of this is function_call from OpenAI.\n",
    "\n",
    "Message Types:\n",
    "- HumanMessage\n",
    "    - This represents a message from the user. Generally consists only of content.\n",
    "- AIMessage\n",
    "    - This represents a message from the model. This may have additional_kwargs in it - for example tool_calls if using OpenAI tool calling.\n",
    "- SystemMessage\n",
    "    - This represents a system message, which tells the model how to behave. This generally only consists of content. Not every model supports this.\n",
    "- FunctionMessage\n",
    "    - This represents the result of a function call. In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.\n",
    "- ToolMessage\n",
    "    - This represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool message types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to the tool that was called to produce this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Streaming\n",
    "\n",
    "All ChatModels implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all ChatModels basic support for streaming.\n",
    "\n",
    "**Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming)** of a single value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-token streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator of tokens can work for any of our ChatModel integrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Tool calling\n",
    "\n",
    "Tool Calling in LangChain is a feature that allows a language model to generate structured output by simulating the \"calling\" of a predefined tool. This process involves the model creating arguments that match a user-defined schema for a tool, but the actual execution of the tool (or even whether the tool is executed) is left to the user.\n",
    "\n",
    "When we define a tool in LangChain, we specify:\n",
    "- What the tool does: Its purpose or functionality.\n",
    "- What inputs (arguments) it requires: A schema that defines the parameters the tool needs to execute.\n",
    "\n",
    "The model \"creates arguments\" by generating these input values based on the user's query or prompt. Consider a tool for basic arithmetic operations:\n",
    "\n",
    "```python\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"calculate\"\n",
    "    args_schema = {\n",
    "        \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]},\n",
    "        \"num1\": {\"type\": \"number\"},\n",
    "        \"num2\": {\"type\": \"number\"},\n",
    "    }\n",
    "```\n",
    "- Operation: Specifies the type of calculation (e.g., \"add\").\n",
    "- The first number for the operation (e.g., 5).\n",
    "- The second number for the operation (e.g., 7).\n",
    "\n",
    "When a user provides input like: \"Add 5 and 7.\" The model interprets the query and generates the following arguments:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"operation\": \"add\",\n",
    "    \"num1\": 5,\n",
    "    \"num2\": 7\n",
    "}\n",
    "```\n",
    "These arguments match the schema defined in the example CalculatorTool.\n",
    "\n",
    "In effect, despite the name, tool calling does not involve the model directly performing actions. Instead, the model provides parameters or arguments that conform to the schema, which can then be used by the user to trigger the tool or extract structured data.\n",
    "\n",
    "How Tool Calling Works:\n",
    "- Define the Tool:\n",
    "    - A \"tool\" in LangChain represents an action the model can \"call\" by generating arguments that match a predefined schema.\n",
    "    - The schema specifies the inputs the tool accepts and can include validations or constraints.\n",
    "- Prompt the Model:\n",
    "    - The user provides a prompt or query to the model.\n",
    "    - The model generates output that matches the schema of the tool, simulating a \"call\" to the tool.\n",
    "- User Handles the Execution:\n",
    "    - The user decides whether to actually execute the tool based on the generated arguments.\n",
    "    - Alternatively, the user may treat the structured output directly as the final result.\n",
    "\n",
    "Many LLM providers, including Anthropic, Cohere, Google, Mistral, OpenAI, and others, support variants of a tool calling feature. These features typically allow requests to the LLM to include available tools and their schemas, and for responses to include calls to these tools. For instance, given a search engine tool, an LLM might handle a query by first issuing a call to the search engine. The system calling the LLM can receive the tool call, execute it, and return the output to the LLM to inform its response. LangChain includes a suite of built-in tools and supports several methods for defining your own custom tools. Tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally.\n",
    "\n",
    "Providers adopt different conventions for formatting tool schemas and tool calls. For instance, Anthropic returns tool calls as parsed structures within a larger content block:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"<thinking>\\nI should use a tool.\\n</thinking>\",\n",
    "    \"type\": \"text\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"id_value\",\n",
    "    \"input\": {\"arg_name\": \"arg_value\"},\n",
    "    \"name\": \"tool_name\",\n",
    "    \"type\": \"tool_use\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "whereas OpenAI separates tool calls into a distinct parameter, with arguments as JSON strings:\n",
    "```json\n",
    "{\n",
    "  \"tool_calls\": [\n",
    "    {\n",
    "      \"id\": \"id_value\",\n",
    "      \"function\": {\n",
    "        \"arguments\": '{\"arg_name\": \"arg_value\"}',\n",
    "        \"name\": \"tool_name\"\n",
    "      },\n",
    "      \"type\": \"function\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "LangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\n",
    "\n",
    "**For a model to be able to invoke tools, you need to pass tool schemas to it when making a chat request. LangChain ChatModels supporting tool calling features implement a .bind_tools method, which receives a list of LangChain tool objects, Pydantic classes, or JSON Schemas and binds them to the chat model in the provider-specific expected format.** That's a mouthful. Let's dissect this:\n",
    "- A tool in LangChain represents a predefined action the model can \"invoke\" by generating structured arguments that match a schema.\n",
    "- Binding Tools to the Model\n",
    "    - Tools must be bound to the model so that the model knows which tools are available and can generate arguments for them.\n",
    "    - Binding tools involves passing the tool schemas to the model, which adapts them to the specific format expected by the provider’s API (e.g., OpenAI, Anthropic).\n",
    "- .bind_tools() Method\n",
    "    - The .bind_tools() method in LangChain allows you to attach tools to a ChatModel instance.\n",
    "    - Once tools are bound, every subsequent API call to the model will include the tool schemas, enabling the model to simulate invoking them.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"calculate\"\n",
    "    description = \"Performs arithmetic operations\"\n",
    "    args_schema = {\n",
    "        \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]},\n",
    "        \"num1\": {\"type\": \"number\"},\n",
    "        \"num2\": {\"type\": \"number\"},\n",
    "    }\n",
    "\n",
    "    def _run(self, operation, num1, num2):\n",
    "        if operation == \"add\":\n",
    "            return num1 + num2\n",
    "        elif operation == \"subtract\":\n",
    "            return num1 - num2\n",
    "        elif operation == \"multiply\":\n",
    "            return num1 * num2\n",
    "        elif operation == \"divide\":\n",
    "            return num1 / num2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid operation\")\n",
    "\n",
    "# Use .bind_tools() to attach the tools to the model.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Instantiate the model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Instantiate the tool\n",
    "calculator_tool = CalculatorTool()\n",
    "\n",
    "# Bind the tool to the model\n",
    "llm_with_tools = llm.bind_tools([calculator_tool])\n",
    "```\n",
    "\n",
    "How Binding Works:\n",
    "- Schema Passing:\n",
    "    - When you call .bind_tools(), LangChain converts the tool definitions into the specific JSON schema format expected by the provider (e.g., OpenAI or Anthropic).\n",
    "- Model Awareness:\n",
    "    - After binding, the model becomes \"aware\" of the tools and their schemas. Every subsequent chat request automatically includes the tool schemas as part of the API call.\n",
    "- Tool Invocation by the Model:\n",
    "    - During a conversation, the model can decide whether to \"call\" a tool by generating arguments that match the tool schema.\n",
    "\n",
    "**When you interact with the bound model, the model can now include tool invocations in its responses.**\n",
    "\n",
    "```python\n",
    "response = llm_with_tools.invoke({\n",
    "    \"input\": \"Can you add 7 and 3?\"\n",
    "})\n",
    "\n",
    "print(response)\n",
    "\n",
    "# The model generates an output in the format:\n",
    "{\n",
    "    \"tool\": \"calculate\",\n",
    "    \"arguments\": {\n",
    "        \"operation\": \"add\",\n",
    "        \"num1\": 7,\n",
    "        \"num2\": 3\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We can define the schema for custom tools using the @tool decorator on Python functions:\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "```\n",
    "\n",
    "We can equivalently define the schema using Pydantic. Pydantic is useful when your tool inputs are more complex:\n",
    "\n",
    "```python\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "# We can bind them to chat models as follows:\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# We can use the bind_tools() method to handle converting Multiply to a \"tool\" and binding it to the model (i.e., passing it in each time the model is invoked).\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "```\n",
    "\n",
    "**When you just use bind_tools(tools), the model can choose whether to return one tool call, multiple tool calls, or no tool calls at all. Some models support a tool_choice parameter that gives you some ability to force the model to call a tool. For models that support this, you can pass in the name of the tool you want the model to always call tool_choice=\"xyz_tool_name\". Or you can pass in tool_choice=\"any\" to force the model to call at least one tool, without specifying which tool specifically.**\n",
    "\n",
    "Currently tool_choice=\"any\" functionality is supported by OpenAI, MistralAI, FireworksAI, and Groq. Currently Anthropic does not support tool_choice at all.\n",
    "\n",
    "If we wanted our model to always call the multiply tool we could do:\n",
    "```python\n",
    "always_multiply_llm = llm.bind_tools([multiply], tool_choice=\"multiply\")\n",
    "```\n",
    "\n",
    "And if we wanted it to always call at least one of add or multiply, we could do:\n",
    "```python\n",
    "always_call_tool_llm = llm.bind_tools([add, multiply], tool_choice=\"any\")\n",
    "```\n",
    "\n",
    "**If tool calls are included in a LLM response, they are attached to the corresponding AIMessage or AIMessageChunk (when streaming) as a list of ToolCall objects in the .tool_calls attribute.** A ToolCall is a typed dict that includes a tool name, dict of argument values, and (optionally) an identifier. Messages with no tool calls default to an empty list for this attribute.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools.invoke(query).tool_calls\n",
    "# [{'name': 'multiply',\n",
    "#   'args': {'a': 3, 'b': 12},\n",
    "#   'id': 'call_UL7E2232GfDHIQGOM4gJfEDD'},\n",
    "#  {'name': 'add',\n",
    "#   'args': {'a': 11, 'b': 49},\n",
    "#   'id': 'call_VKw8t5tpAuzvbHgdAXe9mjUx'}]\n",
    "```\n",
    "\n",
    "The .tool_calls attribute should contain valid tool calls. Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON). When parsing fails in these cases, instances of InvalidToolCall are populated in the .invalid_tool_calls attribute. An InvalidToolCall can have a name, string arguments, identifier, and error message.\n",
    "\n",
    "If desired, output parsers can further process the output. For example, we can convert back to the original Pydantic class:\n",
    "```python\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=[multiply, add])\n",
    "chain.invoke(query)\n",
    "# [multiply(a=3, b=12), add(a=11, b=49)]\n",
    "```\n",
    "How cool is that! We get back a pydantic object of the response from the model.\n",
    "\n",
    "Streaming\n",
    "\n",
    "**When tools are called in a streaming context, message chunks will be populated with tool call chunk objects in a list via the .tool_call_chunks attribute.** A ToolCallChunk includes optional string fields for the tool name, args, and id, and includes an optional integer field index that can be used to join chunks together. Fields are optional because portions of a tool call may be streamed across different chunks (e.g., a chunk that includes a substring of the arguments may have null values for the tool name and id).\n",
    "\n",
    "```python\n",
    "async for chunk in llm_with_tools.astream(query):\n",
    "    print(chunk.tool_call_chunks)\n",
    "# []\n",
    "# [{'name': 'multiply', 'args': '', 'id': 'call_5Gdgx3R2z97qIycWKixgD2OU', 'index': 0}]\n",
    "# [{'name': None, 'args': '{\"a\"', 'id': None, 'index': 0}]\n",
    "# [{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]\n",
    "# [{'name': None, 'args': '\"b\": 1', 'id': None, 'index': 0}]\n",
    "# [{'name': None, 'args': '2}', 'id': None, 'index': 0}]\n",
    "# [{'name': 'add', 'args': '', 'id': 'call_DpeKaF8pUCmLP0tkinhdmBgD', 'index': 1}]\n",
    "# [{'name': None, 'args': '{\"a\"', 'id': None, 'index': 1}]\n",
    "# [{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]\n",
    "# [{'name': None, 'args': ' \"b\": ', 'id': None, 'index': 1}]\n",
    "# [{'name': None, 'args': '49}', 'id': None, 'index': 1}]\n",
    "# []\n",
    "```\n",
    "\n",
    "Note that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various tool output parsers support streaming. For example, below we accumulate tool call chunks:\n",
    "\n",
    "```python\n",
    "first = True\n",
    "async for chunk in llm_with_tools.astream(query):\n",
    "    if first:\n",
    "        gathered = chunk\n",
    "        first = False\n",
    "    else:\n",
    "        gathered = gathered + chunk\n",
    "\n",
    "    print(gathered.tool_call_chunks)\n",
    "# []\n",
    "# [{'name': 'multiply', 'args': '', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\"', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, ', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 1', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}, {'name': 'add', 'args': '', 'id': 'call_GERgANDUbRqdtmXRbIAS9JTS', 'index': 1}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}, {'name': 'add', 'args': '{\"a\"', 'id': 'call_GERgANDUbRqdtmXRbIAS9JTS', 'index': 1}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11,', 'id': 'call_GERgANDUbRqdtmXRbIAS9JTS', 'index': 1}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11, \"b\": ', 'id': 'call_GERgANDUbRqdtmXRbIAS9JTS', 'index': 1}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_GERgANDUbRqdtmXRbIAS9JTS', 'index': 1}]\n",
    "# [{'name': 'multiply', 'args': '{\"a\": 3, \"b\": 12}', 'id': 'call_hXqj6HxzACkpiPG4hFFuIKuP', 'index': 0}, {'name': 'add', 'args': '{\"a\": 11, \"b\": 49}', 'id': 'call_GERgANDUbRqdtmXRbIAS9JTS', 'index': 1}]\n",
    "```\n",
    "\n",
    "**If we're using the model-generated tool invocations to actually call tools and want to pass the tool results back to the model, we can do so using ToolMessages.** Let's explore this in-depth because it is so important:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "messages\n",
    "# AIMessage(content='3 * 12 = 36\\n11 + 49 = 60', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 209, 'total_tokens': 225}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-a55f8cb5-6d6d-4835-9c6b-7de36b2590c7-0')\n",
    "```\n",
    "- Tools (@tool): These are functions registered as tools that the model can \"call.\"\n",
    "- Each tool has a name, arguments, and a return type.\n",
    "    - Two tools, add and multiply, are defined with argument types (int) and return types (int).\n",
    "    - Example: add(a, b) adds two numbers.\n",
    "    - Example: multiply(a, b) multiplies two numbers.\n",
    "- The language model (LLM) is bound to the tools using bind_tools(tools).\n",
    "    - The tools are \"registered\" with the LLM so that the model knows about them and can invoke them when appropriate.\n",
    "    - This allows the model to \"invoke\" tools by generating tool calls.\n",
    "- Initialize Messages: messages = [HumanMessage(query)]. A conversation starts with a HumanMessage containing the user’s query.\n",
    "- The user sends a query (HumanMessage). The model responds (AIMessage) with tool invocations.\n",
    "    - The LLM processes the input messages and generates an AIMessage response.\n",
    "    - This response (ai_msg) contains tool calls in its tool_calls attribute.\n",
    "\n",
    "For example, if the query is: \"What is 3 multiplied by 12, and then add 11 to the result?\" The AIMessage might include tool calls:\n",
    "```json\n",
    "[\n",
    "    {\"name\": \"multiply\", \"args\": {\"a\": 3, \"b\": 12}, \"id\": \"tool_call_1\"},\n",
    "    {\"name\": \"add\", \"args\": {\"a\": 11, \"b\": 49}, \"id\": \"tool_call_2\"}\n",
    "]\n",
    "```\n",
    "\n",
    "The tools are executed, and their results (ToolMessages) are passed back to the model:\n",
    "```python\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "```\n",
    "- Iterate through the tool_calls in the AIMessage.\n",
    "- Find the corresponding tool (add or multiply) based on the name in the tool call.\n",
    "- Execute the tool with the provided args.\n",
    "- After executing the tool, the result is added as a ToolMessage to the conversation.\n",
    "- The ToolMessage includes:\n",
    "    - The tool output (e.g., 36 or 60).\n",
    "    - The tool_call_id linking the result to the specific tool invocation.\n",
    "\n",
    "After processing the tool calls, the messages list contains:\n",
    "- HumanMessage: The user’s query.\n",
    "- AIMessage: The LLM’s response with tool calls.\n",
    "- ToolMessages: The results of the tool executions.\n",
    "```python\n",
    "[\n",
    "    HumanMessage(content=\"What is 3 multiplied by 12, and then add 11 to the result?\"),\n",
    "    AIMessage(\n",
    "        content=\"3 * 12 = 36\\n11 + 49 = 60\",\n",
    "        tool_calls=[\n",
    "            {\"name\": \"multiply\", \"args\": {\"a\": 3, \"b\": 12}, \"id\": \"tool_call_1\"},\n",
    "            {\"name\": \"add\", \"args\": {\"a\": 11, \"b\": 49}, \"id\": \"tool_call_2\"}\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(content=36, tool_call_id=\"tool_call_1\"),\n",
    "    ToolMessage(content=60, tool_call_id=\"tool_call_2\")\n",
    "]\n",
    "```\n",
    "\n",
    "Why Use ToolMessages?\n",
    "- ToolMessages allow tool outputs to be integrated back into the conversation, enabling the model to continue reasoning with updated information.\n",
    "- By including the tool_call_id, each ToolMessage explicitly links the result to its corresponding tool invocation, maintaining traceability.\n",
    "- This setup enables dynamic and interactive workflows where tools and the model collaborate iteratively.\n",
    "- The model can issue multiple tool calls, receive results, and use them to generate more informed responses.\n",
    "\n",
    "It's very critical to understand the topic above!\n",
    "\n",
    "Tools and Few-Shot Examples\n",
    "\n",
    "For more complex tool use it's very useful to add few-shot examples to the prompt. We can do this by adding AIMessages with ToolCalls and corresponding ToolMessages to our prompt.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "examples = [\n",
    "    HumanMessage(\n",
    "        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[\n",
    "            {\"name\": \"multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(\"16505054784\", tool_call_id=\"1\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[{\"name\": \"add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],\n",
    "    ),\n",
    "    ToolMessage(\"16505054788\", tool_call_id=\"2\"),\n",
    "    AIMessage(\n",
    "        \"The product of 317253 and 128472 plus four is 16505054788\",\n",
    "        name=\"example_assistant\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "system = \"\"\"You are bad at math but are an expert at using a calculator. \n",
    "\n",
    "Use past tool usage as an example of how to correctly use the tools.\"\"\"\n",
    "few_shot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        *examples,\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools\n",
    "chain.invoke(\"Whats 119 times 8 minus 20\").tool_calls\n",
    "# [{'name': 'multiply',\n",
    "#   'args': {'a': 119, 'b': 8},\n",
    "#   'id': 'call_tWwpzWqqc8dQtN13CyKZCVMe'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Structured Output\n",
    "\n",
    "It is often crucial to have LLMs return structured output. This is because oftentimes the outputs of the LLMs are used in downstream applications, where specific arguments are required. Having the LLM return structured output reliably is necessary for that.\n",
    "\n",
    "There are a few different high level strategies that are used to do this:\n",
    "- Prompting: This is when you ask the LLM (very nicely) to return output in the desired format (JSON, XML). This is nice because it works with all LLMs. It is not nice because there is no guarantee that the LLM returns the output in the right format.\n",
    "- Function calling: This is when the LLM is fine-tuned to be able to not just generate a completion, but also generate a function call. The functions the LLM can call are generally passed as extra parameters to the model API. The function names and descriptions should be treated as part of the prompt (they usually count against token counts, and are used by the LLM to decide what to do).\n",
    "- Tool calling: A technique similar to function calling, but it allows the LLM to call multiple functions at the same time.\n",
    "- JSON mode: This is when the LLM is guaranteed to return JSON.\n",
    "\n",
    "Different models may support different variants of these, with slightly different parameters. In order to make it easy to get LLMs to return structured output, we have added a common interface to LangChain models: .with_structured_output.\n",
    "\n",
    "By invoking this method (and passing in a JSON schema or a Pydantic model) the model will add whatever model parameters + output parsers are necessary to get back the structured output. There may be more than one way to do this (e.g., function calling vs JSON mode) - you can configure which method to use by passing into that method.\n",
    "\n",
    "Example with OpenAI (supports Tool/function Calling):\n",
    "```python\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# By default, we will use function_calling:\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = model.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "# Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!')\n",
    "\n",
    "# JSON mode:\n",
    "structured_llm = model.with_structured_output(Joke, method=\"json_mode\")\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")\n",
    "# Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!')\n",
    "```\n",
    "\n",
    "Example with Mistral (only support function calling):\n",
    "\n",
    "```python\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "structured_llm = model.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "# Joke(setup=\"Why don't cats play poker in the jungle?\", punchline='Too many cheetahs!')\n",
    "```\n",
    "\n",
    "Example with Groq (supports Tool/function Calling):\n",
    "\n",
    "```python\n",
    "model = ChatGroq()\n",
    "structured_llm = model.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "# Joke(setup=\"Why don't cats play poker in the jungle?\", punchline='Too many cheetahs!')\n",
    "\n",
    "# using JSON mode:\n",
    "structured_llm = model.with_structured_output(Joke, method=\"json_mode\")\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")\n",
    "# Joke(setup=\"Why don't cats play poker in the jungle?\", punchline='Too many cheetahs!')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Caching\n",
    "\n",
    "LangChain provides an optional caching layer for chat models. It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. \n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a joke\")\n",
    "\n",
    "# The second time it is, so it goes faster\n",
    "llm.predict(\"Tell me a joke\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Chat Model\n",
    "\n",
    "Wrapping your LLM with the standard BaseChatModel interface allow you to use your LLM in existing LangChain programs with minimal code modifications. As an bonus, your LLM will automatically become a LangChain Runnable and will benefit from some optimizations out of the box (e.g., batch via a threadpool), async support, the astream_events API, etc.\n",
    "\n",
    "Chat models take messages as inputs and return a message as output. LangChain has a few built-in message types:\n",
    "- SystemMessage: \n",
    "    - Used for priming AI behavior, usually passed in as the first of a sequence of input messages.\n",
    "- HumanMessage: \n",
    "    - Represents a message from a person interacting with the chat model.\n",
    "- AIMessage: \n",
    "    - Represents a message from the chat model. This can be either text or a request to invoke a tool.\n",
    "- FunctionMessage / ToolMessage: \n",
    "    - Message for passing the results of tool invocation back to the model.\n",
    "    - ToolMessage and FunctionMessage closely follow OpenAIs function and tool roles.\n",
    "- AIMessageChunk / HumanMessageChunk: \n",
    "    - Chunk variant of each type of message.\n",
    "\n",
    "Import Example:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "# All the chat messages have a streaming variant that contains Chunk in the name.\n",
    "from langchain_core.messages import (\n",
    "    AIMessageChunk,\n",
    "    FunctionMessageChunk,\n",
    "    HumanMessageChunk,\n",
    "    SystemMessageChunk,\n",
    "    ToolMessageChunk,\n",
    ")\n",
    "```\n",
    "\n",
    "Note the streaming variant chunks are used when streaming output from chat models, and they all define an additive property:\n",
    "```python\n",
    "AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")\n",
    "```\n",
    "\n",
    "When we inherit from BaseChatModel, we need to implement the following:\n",
    "- _generate: \n",
    "    - Use to generate a chat result from a prompt\n",
    "    - Required\n",
    "- _llm_type (property): \n",
    "    - Used to uniquely identify the type of the model. Used for logging.\n",
    "    - Required\n",
    "- _identifying_params (property): \n",
    "    - Represent model parameterization for tracing purposes.\n",
    "    - Optional\n",
    "- _stream: \n",
    "    - Use to implement streaming.\n",
    "    - Optional\n",
    "- _agenerate: \n",
    "    - Use to implement a native async method.\t\n",
    "    - Optional\n",
    "- _astream: \n",
    "    - Use to implement async version of _stream.\t\n",
    "    - Optional\n",
    "    - The _astream implementation uses run_in_executor to launch the sync _stream in a separate thread if _stream is implemented, otherwise it fallsback to use _agenerate.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor\n",
    "\n",
    "class CustomChatModelAdvanced(BaseChatModel):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    \"\"\"The name of the model\"\"\"\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Override the _generate method to implement the chat model logic.\n",
    "\n",
    "        This can be a call to an API, a call to a local model, or any other\n",
    "        implementation that generates a response to the input prompt.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        # Replace this with actual logic to generate a response from a list\n",
    "        # of messages.\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[: self.n]\n",
    "        message = AIMessage(\n",
    "            content=tokens,\n",
    "            additional_kwargs={},  # Used to add additional payload (e.g., function calling request)\n",
    "            response_metadata={  # Use for response metadata\n",
    "                \"time_in_seconds\": 3,\n",
    "            },\n",
    "        )\n",
    "        ##\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"Stream the output of the model.\n",
    "\n",
    "        This method should be implemented if the model can generate output\n",
    "        in a streaming fashion. If the model does not support streaming,\n",
    "        do not implement it. In that case streaming requests will be automatically\n",
    "        handled by the _generate method.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[: self.n]\n",
    "\n",
    "        for token in tokens:\n",
    "            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))\n",
    "\n",
    "            if run_manager:\n",
    "                # This is optional in newer versions of LangChain\n",
    "                # The on_llm_new_token will be called automatically\n",
    "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "        # Let's add some other information (e.g., response metadata)\n",
    "        chunk = ChatGenerationChunk(\n",
    "            message=AIMessageChunk(content=\"\", response_metadata={\"time_in_sec\": 3})\n",
    "        )\n",
    "        if run_manager:\n",
    "            # This is optional in newer versions of LangChain\n",
    "            # The on_llm_new_token will be called automatically\n",
    "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "        yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "```\n",
    "\n",
    "The chat model will implement the standard Runnable interface of LangChain:\n",
    "```python\n",
    "model = CustomChatModelAdvanced(n=3, model_name=\"my_custom_model\")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"hello!\"),\n",
    "        AIMessage(content=\"Hi there human!\"),\n",
    "        HumanMessage(content=\"Meow!\"),\n",
    "    ]\n",
    ")\n",
    "# AIMessage(content='Meo', response_metadata={'time_in_seconds': 3}, id='run-ddb42bd6-4fdd-4bd2-8be5-e11b67d3ac29-0')\n",
    "\n",
    "model.invoke(\"hello\")\n",
    "# AIMessage(content='hel', response_metadata={'time_in_seconds': 3}, id='run-4d3cc912-44aa-454b-977b-ca02be06c12e-0')\n",
    "\n",
    "model.batch([\"hello\", \"goodbye\"])\n",
    "# [AIMessage(content='hel', response_metadata={'time_in_seconds': 3}, id='run-9620e228-1912-4582-8aa1-176813afec49-0'),\n",
    "#  AIMessage(content='goo', response_metadata={'time_in_seconds': 3}, id='run-1ce8cdf8-6f75-448e-82f7-1bb4a121df93-0')]\n",
    "\n",
    "for chunk in model.stream(\"cat\"):\n",
    "    print(chunk.content, end=\"|\")\n",
    "# c|a|t||\n",
    "\n",
    "async for chunk in model.astream(\"cat\"):\n",
    "    print(chunk.content, end=\"|\")\n",
    "# c|a|t||\n",
    "\n",
    "# Let's try to use the astream events API which will also help double check that all the callbacks were implemented!\n",
    "async for event in model.astream_events(\"cat\", version=\"v1\"):\n",
    "    print(event)\n",
    "# {'event': 'on_chat_model_start', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'name': 'CustomChatModelAdvanced', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}}\n",
    "# {'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='c', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}\n",
    "# {'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='a', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}\n",
    "# {'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='t', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}\n",
    "# {'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='', response_metadata={'time_in_sec': 3}, id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}\n",
    "# {'event': 'on_chat_model_end', 'name': 'CustomChatModelAdvanced', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', response_metadata={'time_in_sec': 3}, id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response metadata\n",
    "\n",
    "Many model providers include some metadata in their chat generation responses. This metadata can be accessed via the AIMessage.response_metadata: Dict attribute. Depending on the model provider and model configuration, this can contain information like token counts, logprobs, and more.\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "msg = llm.invoke([(\"human\", \"What's the oldest known example of cuneiform\")])\n",
    "msg.response_metadata\n",
    "# {'token_usage': {'completion_tokens': 164,\n",
    "#   'prompt_tokens': 17,\n",
    "#   'total_tokens': 181},\n",
    "#  'model_name': 'gpt-4-turbo',\n",
    "#  'system_fingerprint': 'fp_76f018034d',\n",
    "#  'finish_reason': 'stop',\n",
    "#  'logprobs': None}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.\n",
    "\n",
    "There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.\n",
    "\n",
    "LLMs implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls. LLMs accept strings as inputs, or objects which can be coerced to string prompts, including List[BaseMessage] and PromptValue.\n",
    "\n",
    "```python\n",
    "llm.invoke(\n",
    "    \"What are some theories about the relationship between unemployment and inflation?\"\n",
    ")\n",
    "# '\\n\\n1. The Phillips Curve Theory: This suggests that there is an inverse relationship between unemployment and inflation, meaning that when unemployment is low, inflation will be higher, and when unemployment is high, inflation will be lower.\\n\\n2. The Monetarist Theory: This theory suggests that the relationship between unemployment and inflation is weak, and that changes in the money supply are more important in determining inflation.\\n\\n3. The Resource Utilization Theory: This suggests that when unemployment is low, firms are able to raise wages and prices in order to take advantage of the increased demand for their products and services. This leads to higher inflation.'\n",
    "\n",
    "for chunk in llm.stream(\n",
    "    \"What are some theories about the relationship between unemployment and inflation?\"\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "llm.batch(\n",
    "    [\n",
    "        \"What are some theories about the relationship between unemployment and inflation?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "await llm.ainvoke(\n",
    "    \"What are some theories about the relationship between unemployment and inflation?\"\n",
    ")\n",
    "\n",
    "async for chunk in llm.astream(\n",
    "    \"What are some theories about the relationship between unemployment and inflation?\"\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "await llm.abatch(\n",
    "    [\n",
    "        \"What are some theories about the relationship between unemployment and inflation?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "async for chunk in llm.astream_log(\n",
    "    \"What are some theories about the relationship between unemployment and inflation?\"\n",
    "):\n",
    "    print(chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs and LangSmith\n",
    "\n",
    "All LLMs come with built-in LangSmith tracing. Just set the following environment variables:\n",
    "\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=<your-api-key>\n",
    "```\n",
    "\n",
    "and any LLM invocation (whether it's nested in a chain or not) will automatically be traced. A trace will include inputs, outputs, latency, token usage, invocation params, environment params, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LLM\n",
    "\n",
    "Wrapping your LLM with the standard LLM interface allow you to use your LLM in existing LangChain programs with minimal code modifications. As an bonus, your LLM will automatically become a LangChain Runnable and will benefit from some optimizations out of the box, async support, the astream_events API, etc.\n",
    "\n",
    "There are only two required things that a custom LLM needs to implement:\n",
    "- _call: Takes in a string and some optional stop words, and returns a string. Used by invoke.\n",
    "- _llm_type: A property that returns a string, used for logging purposes only.\n",
    "\n",
    "Optional implementations:\n",
    "- _identifying_params: Used to help with identifying the model and printing the LLM; should return a dictionary. This is a @property.\n",
    "- _acall: Provides an async native implementation of _call, used by ainvoke.\n",
    "- _stream: Method to stream the output token by token.\n",
    "- _astream: Provides an async native implementation of _stream; in newer LangChain versions, defaults to _stream.\n",
    "\n",
    "Let's implement a simple custom LLM that just returns the first n characters of the input.\n",
    "\n",
    "```python\n",
    "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Override this method to implement the LLM logic.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "                If stop tokens are not supported consider raising NotImplementedError.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"Stream the LLM on the given prompt.\n",
    "\n",
    "        This method should be overridden by subclasses that support streaming.\n",
    "\n",
    "        If not implemented, the default behavior of calls to stream will be to\n",
    "        fallback to the non-streaming version of the model and return\n",
    "        the output as a single chunk.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of these substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            An iterator of GenerationChunks.\n",
    "        \"\"\"\n",
    "        for char in prompt[: self.n]:\n",
    "            chunk = GenerationChunk(text=char)\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\"\n",
    "```\n",
    "\n",
    "This LLM will implement the standard Runnable interface of LangChain:\n",
    "\n",
    "```python\n",
    "llm = CustomLLM(n=5)\n",
    "print(llm)\n",
    "# Params: {'model_name': 'CustomChatModel'}\n",
    "\n",
    "llm.invoke(\"This is a foobar thing\")\n",
    "# 'This '\n",
    "\n",
    "await llm.ainvoke(\"world\")\n",
    "# 'world'\n",
    "\n",
    "llm.batch([\"woof woof woof\", \"meow meow meow\"])\n",
    "# ['woof ', 'meow ']\n",
    "\n",
    "await llm.abatch([\"woof woof woof\", \"meow meow meow\"])\n",
    "# ['woof ', 'meow ']\n",
    "\n",
    "async for token in llm.astream(\"hello\"):\n",
    "    print(token, end=\"|\", flush=True)\n",
    "# h|e|l|l|o|\n",
    "```\n",
    "\n",
    "Let's confirm that in integrates nicely with other LangChain APIs:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"you are a bot\"), (\"human\", \"{input}\")]\n",
    ")\n",
    "\n",
    "llm = CustomLLM(n=7)\n",
    "chain = prompt | llm\n",
    "\n",
    "idx = 0\n",
    "async for event in chain.astream_events({\"input\": \"hello there!\"}, version=\"v1\"):\n",
    "    print(event)\n",
    "    idx += 1\n",
    "    if idx > 7:\n",
    "        # Truncate\n",
    "        break\n",
    "# {'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n",
    "# {'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n",
    "# {'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}\n",
    "# {'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\\nHuman: hello there!']}}}\n",
    "# {'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}\n",
    "# {'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}\n",
    "# {'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}\n",
    "# {'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Caching\n",
    "\n",
    "LangChain provides an optional caching layer for LLMs. It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\n",
    "\n",
    "```python\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# To make the caching really obvious, lets use a slower model.\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)\n",
    "\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a joke\")\n",
    "\n",
    "# The second time it is, so it goes faster\n",
    "llm.predict(\"Tell me a joke\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Streaming\n",
    "\n",
    "All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all LLMs basic support for streaming.\n",
    "\n",
    "Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations.\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)\n",
    "for chunk in llm.stream(\"Write me a song about sparkling water.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data. Besides having a large collection of different types of output parsers, one distinguishing benefit of LangChain OutputParsers is that many of them support streaming.\n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in. Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
    "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "And then one optional one:\n",
    "- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n",
    "\n",
    "Below we go over the main type of output parser, the PydanticOutputParser.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
    "parser.invoke(output)\n",
    "# Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers and LCEL\n",
    "\n",
    "Output parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n",
    "\n",
    "Output parsers accept a string or BaseMessage as input and can return an arbitrary type.\n",
    "\n",
    "```python\n",
    "parser.invoke(output)\n",
    "```\n",
    "\n",
    "Instead of manually invoking the parser, we also could've just added it to our Runnable sequence:\n",
    "\n",
    "```python\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})\n",
    "# Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') \n",
    "```\n",
    "\n",
    "While all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\n",
    "\n",
    "The SimpleJsonOutputParser for example can stream through partial outputs:\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "json_chain = json_prompt | model | json_parser\n",
    "\n",
    "list(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\n",
    "# [{},\n",
    "#  {'answer': ''},\n",
    "#  {'answer': 'Ant'},\n",
    "#  {'answer': 'Anton'},\n",
    "#  {'answer': 'Antonie'},\n",
    "#  {'answer': 'Antonie van'},\n",
    "#  {'answer': 'Antonie van Lee'},\n",
    "#  {'answer': 'Antonie van Leeu'},\n",
    "#  {'answer': 'Antonie van Leeuwen'},\n",
    "#  {'answer': 'Antonie van Leeuwenho'},\n",
    "#  {'answer': 'Antonie van Leeuwenhoek'}]\n",
    "```\n",
    "\n",
    "While the PydanticOutputParser cannot:\n",
    "\n",
    "```python\n",
    "list(chain.stream({\"query\": \"Tell me a joke.\"}))\n",
    "# [Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Output Parsers\n",
    "\n",
    "In some situations you may want to implement a custom parser to structure the model output into a custom format.\n",
    "\n",
    "There are two ways to implement a custom parser:\n",
    "- Using RunnableLambda (non-streaming) or RunnableGenerator (streaming) in LCEL -- we strongly recommend this for most use cases\n",
    "- By inherting from one of the base classes for out parsing -- this is the hard way of doing things\n",
    "\n",
    "Example using Runnable Lambdas and Generators:\n",
    "\n",
    "Here, we will make a simple parser that inverts the case of the output from the model. For example, if the model outputs: \"Meow\", the parser will produce \"mEOW\".\n",
    "\n",
    "```python\n",
    "from typing import Iterable\n",
    "\n",
    "from langchain_anthropic.chat_models import ChatAnthropic\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk\n",
    "\n",
    "model = ChatAnthropic(model_name=\"claude-2.1\")\n",
    "\n",
    "\n",
    "def parse(ai_message: AIMessage) -> str:\n",
    "    \"\"\"Parse the AI message.\"\"\"\n",
    "    return ai_message.content.swapcase()\n",
    "\n",
    "\n",
    "chain = model | parse\n",
    "chain.invoke(\"hello\")\n",
    "# 'hELLO!'\n",
    "```\n",
    "\n",
    "LCEL automatically upgrades the function parse to RunnableLambda(parse) when composed using a | syntax. If you don't like that you can manually import RunnableLambda and then run: parse = RunnableLambda(parse).\n",
    "\n",
    "Streaming doesn't work in the original implementation because the parser (parse) processes the entire response at once, aggregating all chunks before applying the transformation. In a streaming scenario, the output is expected to be processed and returned incrementally as each chunk is received. However, with the original parse, the response from the model is first fully collected, and only after that is the case inversion applied. This behavior inherently defeats the purpose of streaming. Failure to work:\n",
    "\n",
    "```python\n",
    "for chunk in chain.stream(\"tell me about yourself in one sentence\"):\n",
    "    print(chunk, end=\"|\", flush=True)\n",
    "# i'M cLAUDE, AN ai ASSISTANT CREATED BY aNTHROPIC TO BE HELPFUL, HARMLESS, AND HONEST.|\n",
    "```\n",
    "\n",
    "In a streaming scenario, the output needs to be generated chunk by chunk as the input becomes available. When using RunnableLambda (automatically applied by LangChain when | is used), the parse function is wrapped in a non-streaming runnable. The wrapping assumes the function processes a complete input and produces a single output, which is incompatible with streaming.\n",
    "\n",
    "To enable streaming, the parser must:\n",
    "- Accept a stream of chunks (e.g., Iterable[AIMessageChunk]) instead of a complete AIMessage.\n",
    "- Yield processed chunks incrementally as they are received.\n",
    "\n",
    "The corrected implementation uses RunnableGenerator, which is designed for streaming tasks. This allows the parser to process and yield each chunk of the response as it arrives.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from typing import Iterable\n",
    "from langchain_core.runnables import RunnableGenerator\n",
    "from langchain_anthropic.chat_models import ChatAnthropic\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "# Define the model\n",
    "model = ChatAnthropic(model_name=\"claude-2.1\")\n",
    "\n",
    "# Define the streaming parser\n",
    "def streaming_parse(chunks: Iterable[AIMessageChunk]) -> Iterable[str]:\n",
    "    for chunk in chunks:\n",
    "        # Process each chunk and yield the result incrementally\n",
    "        yield chunk.content.swapcase()\n",
    "\n",
    "# Wrap the parser in a RunnableGenerator for streaming\n",
    "streaming_parse = RunnableGenerator(streaming_parse)\n",
    "\n",
    "# Combine the model and the streaming parser\n",
    "chain = model | streaming_parse\n",
    "\n",
    "# Use the chain in a streaming scenario\n",
    "for chunk in chain.stream(\"tell me about yourself in one sentence\"):\n",
    "    print(chunk, end=\"|\", flush=True)\n",
    "```\n",
    "\n",
    "Another approach to implement a parser is by inherting from BaseOutputParser, BaseGenerationOutputParser or another one of the base parsers depending on what you need to do. In general, we do not recommend this approach for most use cases as it results in more code to write without significant benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: CSV parser\n",
    "\n",
    "This output parser can be used when you want to return a list of comma-separated items.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"subject\": \"ice cream flavors\"})\n",
    "# ['Vanilla',\n",
    "#  'Chocolate',\n",
    "#  'Strawberry',\n",
    "#  'Mint Chocolate Chip',\n",
    "#  'Cookies and Cream']\n",
    "\n",
    "for s in chain.stream({\"subject\": \"ice cream flavors\"}):\n",
    "    print(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: Datetime parser\n",
    "\n",
    "This OutputParser can be used to parse LLM output into datetime format.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | OpenAI() | output_parser\n",
    "\n",
    "output = chain.invoke({\"question\": \"when was bitcoin founded?\"})\n",
    "\n",
    "print(output)\n",
    "# 2009-01-03 18:15:05\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: Enum parser\n",
    "\n",
    "This notebook shows how to use an Enum output parser.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Colors(Enum):\n",
    "    RED = \"red\"\n",
    "    GREEN = \"green\"\n",
    "    BLUE = \"blue\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Colors)\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"What color eyes does this person have?\n",
    "\n",
    "> Person: {person}\n",
    "\n",
    "Instructions: {instructions}\"\"\"\n",
    ").partial(instructions=parser.get_format_instructions())\n",
    "chain = prompt | ChatOpenAI() | parser\n",
    "\n",
    "chain.invoke({\"person\": \"Frank Sinatra\"})\n",
    "# <Colors.BLUE: 'blue'>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: JSON parser\n",
    "\n",
    "This output parser allows users to specify an arbitrary JSON schema and query LLMs for outputs that conform to that schema.\n",
    "\n",
    "Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON. In the OpenAI family, DaVinci can do reliably but Curie's ability already drops off dramatically.\n",
    "\n",
    "You can optionally use Pydantic to declare your data model.\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})\n",
    "# {'setup': \"Why don't scientists trust atoms?\",\n",
    "#  'punchline': 'Because they make up everything!'}\n",
    "```\n",
    "\n",
    "This output parser supports streaming.\n",
    "\n",
    "for s in chain.stream({\"query\": joke_query}):\n",
    "    print(s)\n",
    "<!-- {'setup': ''}\n",
    "{'setup': 'Why'}\n",
    "{'setup': 'Why don'}\n",
    "{'setup': \"Why don't\"}\n",
    "{'setup': \"Why don't scientists\"}\n",
    "{'setup': \"Why don't scientists trust\"}\n",
    "{'setup': \"Why don't scientists trust atoms\"}\n",
    "{'setup': \"Why don't scientists trust atoms?\", 'punchline': ''}\n",
    "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because'}\n",
    "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they'}\n",
    "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make'}\n",
    "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up'}\n",
    "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything'}\n",
    "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'} -->\n",
    "\n",
    "You can also use this without Pydantic. This will prompt it return JSON, but doesn't provide specific about what the schema should be.\n",
    "\n",
    "```python\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})\n",
    "# {'joke': \"Why don't scientists trust atoms? Because they make up everything!\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: OpenAI Functions\n",
    "\n",
    "These output parsers use OpenAI function calling to structure its outputs. This means they are only usable with models that support function calling. There are a few different variants:\n",
    "\n",
    "- JsonOutputFunctionsParser: Returns the arguments of the function call as JSON\n",
    "- PydanticOutputFunctionsParser: Returns the arguments of the function call as a Pydantic Model\n",
    "- JsonKeyOutputFunctionsParser: Returns the value of specific key in the function call as JSON\n",
    "- PydanticAttrOutputFunctionsParser: Returns the value of specific key in the function call as a Pydantic Model\n",
    "\n",
    "```python\n",
    "from langchain_community.utils.openai_functions import (\n",
    "    convert_pydantic_to_openai_function,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "openai_functions = [convert_pydantic_to_openai_function(Joke)]\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are helpful assistant\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "parser = JsonOutputFunctionsParser()\n",
    "\n",
    "chain = prompt | model.bind(functions=openai_functions) | parser\n",
    "\n",
    "chain.invoke({\"input\": \"tell me a joke\"})\n",
    "# {'setup': \"Why don't scientists trust atoms?\",\n",
    "#  'punchline': 'Because they make up everything!'}\n",
    "\n",
    "for s in chain.stream({\"input\": \"tell me a joke\"}):\n",
    "    print(s)\n",
    "# {}\n",
    "# {'setup': ''}\n",
    "# {'setup': 'Why'}\n",
    "# {'setup': 'Why don'}\n",
    "# {'setup': \"Why don't\"}\n",
    "# {'setup': \"Why don't scientists\"}\n",
    "# {'setup': \"Why don't scientists trust\"}\n",
    "# {'setup': \"Why don't scientists trust atoms\"}\n",
    "# {'setup': \"Why don't scientists trust atoms?\", 'punchline': ''}\n",
    "# {'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because'}\n",
    "# {'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they'}\n",
    "# {'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make'}\n",
    "# {'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up'}\n",
    "# {'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything'}\n",
    "# {'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}\n",
    "\n",
    "PydanticOutputFunctionsParser builds on top of JsonOutputFunctionsParser but passes the results to a Pydantic Model. This allows for further validation should you choose. \n",
    "\n",
    "```python\n",
    "from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "parser = PydanticOutputFunctionsParser(pydantic_schema=Joke)\n",
    "\n",
    "openai_functions = [convert_pydantic_to_openai_function(Joke)]\n",
    "chain = prompt | model.bind(functions=openai_functions) | parser\n",
    "\n",
    "chain.invoke({\"input\": \"tell me a joke\"})\n",
    "# Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')\n",
    "```\n",
    "\n",
    "Notice the return value of above is a pydantic object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: OpenAI Tools\n",
    "\n",
    "These output parsers extract tool calls from OpenAI's function calling API responses. This means they are only usable with models that support function calling, and specifically the latest tools and tool_choice parameters. \n",
    "\n",
    "There are a few different variants of output parsers:\n",
    "- JsonOutputToolsParser: Returns the arguments of the function call as JSON\n",
    "- JsonOutputKeyToolsParser: Returns the value of specific key in the function call as JSON\n",
    "- PydanticToolsParser: Returns the arguments of the function call as a Pydantic Model\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0).bind_tools([Joke])\n",
    "\n",
    "model.kwargs[\"tools\"]\n",
    "# [{'type': 'function',\n",
    "#   'function': {'name': 'Joke',\n",
    "#    'description': 'Joke to tell user.',\n",
    "#    'parameters': {'type': 'object',\n",
    "#     'properties': {'setup': {'description': 'question to set up a joke',\n",
    "#       'type': 'string'},\n",
    "#      'punchline': {'description': 'answer to resolve the joke',\n",
    "#       'type': 'string'}},\n",
    "#     'required': ['setup', 'punchline']}}}]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are helpful assistant\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "parser = JsonOutputToolsParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"input\": \"tell me a joke\"})\n",
    "# [{'type': 'Joke',\n",
    "#   'args': {'setup': \"Why don't scientists trust atoms?\",\n",
    "#    'punchline': 'Because they make up everything!'}}]\n",
    "```\n",
    "\n",
    "To include the tool call id we can specify return_id=True:\n",
    "\n",
    "```python\n",
    "parser = JsonOutputToolsParser(return_id=True)\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\"input\": \"tell me a joke\"})\n",
    "# [{'type': 'Joke',\n",
    "#   'args': {'setup': \"Why don't scientists trust atoms?\",\n",
    "#    'punchline': 'Because they make up everything!'},\n",
    "#   'id': 'call_Isuoh0RTeQzzOKGg5QlQ7UqI'}]\n",
    "```\n",
    "\n",
    "PydanticToolsParser builds on top of JsonOutputToolsParser but passes the results to a Pydantic Model. This allows for further validation should you choose.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "parser = PydanticToolsParser(tools=[Joke])\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0).bind_tools([Joke])\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"input\": \"tell me a joke\"})\n",
    "# [Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: Output-fixing parser\n",
    "\n",
    "This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors. But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it. For this example, we'll use the above Pydantic output parser. Here's what happens if we pass it a result that does not comply with the schema:\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
    "\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\n",
    "\n",
    "parser.parse(misformatted)\n",
    "# ---------------------------------------------------------------------------\n",
    "# ``````output\n",
    "# JSONDecodeError                           Traceback (most recent call last)\n",
    "# ``````output\n",
    "# File ~/workplace/langchain/libs/langchain/langchain/output_parsers/pydantic.py:29, in PydanticOutputParser.parse(self, text)\n",
    "#      28     json_str = match.group()\n",
    "# ---> 29 json_object = json.loads(json_str, strict=False)\n",
    "#      30 return self.pydantic_object.parse_obj(json_object)\n",
    "# ``````output\n",
    "# File ~/.pyenv/versions/3.10.1/lib/python3.10/json/__init__.py:359, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n",
    "#     358     kw['parse_constant'] = parse_constant\n",
    "# --> 359 return cls(**kw).decode(s)\n",
    "# ``````output\n",
    "# File ~/.pyenv/versions/3.10.1/lib/python3.10/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\n",
    "#     333 \"\"\"Return the Python representation of ``s`` (a ``str`` instance\n",
    "#     334 containing a JSON document).\n",
    "#     335 \n",
    "#     336 \"\"\"\n",
    "# --> 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
    "#     338 end = _w(s, end).end()\n",
    "# ``````output\n",
    "# File ~/.pyenv/versions/3.10.1/lib/python3.10/json/decoder.py:353, in JSONDecoder.raw_decode(self, s, idx)\n",
    "#     352 try:\n",
    "# --> 353     obj, end = self.scan_once(s, idx)\n",
    "#     354 except StopIteration as err:\n",
    "# ``````output\n",
    "# JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
    "# ``````output\n",
    "\n",
    "# During handling of the above exception, another exception occurred:\n",
    "# ``````output\n",
    "# OutputParserException                     Traceback (most recent call last)\n",
    "# ``````output\n",
    "# Cell In[4], line 1\n",
    "# ----> 1 parser.parse(misformatted)\n",
    "# ``````output\n",
    "# File ~/workplace/langchain/libs/langchain/langchain/output_parsers/pydantic.py:35, in PydanticOutputParser.parse(self, text)\n",
    "#      33 name = self.pydantic_object.__name__\n",
    "#      34 msg = f\"Failed to parse {name} from completion {text}. Got: {e}\"\n",
    "# ---> 35 raise OutputParserException(msg, llm_output=text)\n",
    "# ``````output\n",
    "# OutputParserException: Failed to parse Actor from completion {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
    "```\n",
    "\n",
    "Now we can construct and use a OutputFixingParser. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\n",
    "\n",
    "new_parser.parse(misformatted)\n",
    "# Actor(name='Tom Hanks', film_names=['Forrest Gump'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: Pandas DataFrame Parser\n",
    "\n",
    "The PandasDataFrameOutputParser in LangChain:\n",
    "- Allows you to query a DataFrame using an LLM.\n",
    "- Structures the LLM's output to match the format of a Pandas DataFrame operation.\n",
    "    - The parser acts as a bridge between the natural language output generated by the language model (LLM) and the structured, programmatic format required to work with a Pandas DataFrame.\n",
    "    - The parser provides the model with clear instructions on how to format its output.\n",
    "        - Example: A column retrieval query should produce a dictionary with column names as keys and values as data.\n",
    "        - Example: A row retrieval query should produce a dictionary with row indices as keys and column-value mappings as values.\n",
    "    - The parser validates Against the DataFrame Schema:\n",
    "        - The parser ensures that the response only references valid columns or operations defined in the DataFrame.\n",
    "        - Example: A query asking for a non-existent column will fail. \n",
    "    - The parser transforms the LLM's Output:\n",
    "         - Converts the natural language output into a structured, machine-readable format (like a dictionary or JSON) that aligns with the DataFrame's data and structure.\n",
    "\n",
    "In the example below, the parser provides format instructions for the LLM to interpret user queries. Validates and processes the response to ensure it matches the expected structure. Facilitates operations on a Pandas DataFrame, such as retrieving rows, columns, or performing calculations.\n",
    "\n",
    "```python\n",
    "import pprint\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define your desired Pandas DataFrame.\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"num_legs\": [2, 4, 8, 0],\n",
    "        \"num_wings\": [2, 0, 0, 0],\n",
    "        \"num_specimen_seen\": [10, 2, 1, 8],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)\n",
    "\n",
    "# Here's an example of a column operation being performed.\n",
    "df_query = \"Retrieve the num_wings column.\"\n",
    "\n",
    "# Set up the prompt.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "format_parser_output(parser_output)\n",
    "# {'num_wings': {0: 2,\n",
    "#                1: 0,\n",
    "#                2: 0,\n",
    "#                3: 0}}\n",
    "\n",
    "# Here's an example of a row operation being performed.\n",
    "df_query = \"Retrieve the first row.\"\n",
    "\n",
    "# Set up the prompt.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "format_parser_output(parser_output)\n",
    "# {'0': {'num_legs': 2,\n",
    "#        'num_specimen_seen': 10,\n",
    "#        'num_wings': 2}}\n",
    "\n",
    "# Here's an example of a random Pandas DataFrame operation limiting the number of rows\n",
    "df_query = \"Retrieve the average of the num_legs column from rows 1 to 3.\"\n",
    "\n",
    "# Set up the prompt.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "print(parser_output)\n",
    "# {'mean': 4.0}\n",
    "\n",
    "# Here's an example of a poorly formatted query\n",
    "df_query = \"Retrieve the mean of the num_fingers column.\"\n",
    "\n",
    "# Set up the prompt.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "# OutputParserException: Invalid column: num_fingers. Please check the format instructions.\n",
    "```\n",
    "\n",
    "Why the Last Query Fails:\n",
    "- Invalid Column:\n",
    "    - The DataFrame does not contain a column named num_fingers.\n",
    "    - The parser validates the query and finds that it references a non-existent column.\n",
    "- Validation Failure:\n",
    "    - The PandasDataFrameOutputParser checks the LLM's response against the DataFrame's structure.\n",
    "    - If the response contains an invalid column, the parser raises an OutputParserException.\n",
    "- Error Raised:\n",
    "    - OutputParserException: Invalid column: num_fingers. Please check the format instructions.\n",
    "\n",
    "The Pandas DataFrame Parser does not replace the Pandas DataFrame Agent! Pandas DataFrame Agent provides a more general-purpose, conversational interface for performing complex and multi-step operations. It can can handle multi-step reasoning, chain operations, and respond dynamically to arbitrary queries. It is best for complex, multi-step operations requiring reasoning and decision-making (e.g., combining multiple columns, filtering rows dynamically). It acts as the decision-maker, dynamically figuring out what DataFrame operations to perform and executing Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: Pydantic parser\n",
    "\n",
    "This output parser allows users to specify an arbitrary Pydantic Model and query LLMs for outputs that conform to that schema. Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON. In the OpenAI family, DaVinci can do reliably but Curie's ability already drops off dramatically.\n",
    "\n",
    "Use Pydantic to declare your data model. Pydantic's BaseModel is like a Python dataclass, but with actual type checking + coercion.\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})\n",
    "# Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')\n",
    "\n",
    "# Here's another example, but with a compound typed field (e.g. List[str]).\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
    "\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": actor_query})\n",
    "# Actor(name='Tom Hanks', film_names=['Forrest Gump', 'Cast Away', 'Saving Private Ryan', 'Toy Story', 'The Green Mile'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers: Retry Parser\n",
    "\n",
    "While in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it isn't. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import (\n",
    "    OutputFixingParser,\n",
    "    PydanticOutputParser,\n",
    ")\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "template = \"\"\"Based on the user question, provide an Action and Action Input for what step should be taken.\n",
    "{format_instructions}\n",
    "Question: {query}\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "class Action(BaseModel):\n",
    "    action: str = Field(description=\"action to take\")\n",
    "    action_input: str = Field(description=\"input to the action\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Action)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt_value = prompt.format_prompt(query=\"who is leo di caprios gf?\")\n",
    "\n",
    "bad_response = '{\"action\": \"search\"}'\n",
    "\n",
    "parser.parse(bad_response)\n",
    "# OutputParserException: Failed to parse Action from completion {\"action\": \"search\"}. Got: 1 validation error for Action\n",
    "# action_input\n",
    "#   field required (type=value_error.missing)\n",
    "```\n",
    "\n",
    "If we try to use the OutputFixingParser to fix this error, it will be confused - namely, it doesn't know what to actually put for action input.\n",
    "\n",
    "```python\n",
    "fix_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\n",
    "\n",
    "fix_parser.parse(bad_response)\n",
    "# Action(action='search', action_input='input')\n",
    "```\n",
    "\n",
    "Instead, we can use the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import RetryOutputParser\n",
    "\n",
    "retry_parser = RetryOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))\n",
    "\n",
    "retry_parser.parse_with_prompt(bad_response, prompt_value)\n",
    "# Action(action='search', action_input='leo di caprio girlfriend')\n",
    "```\n",
    "\n",
    "We can also add the RetryOutputParser easily with a custom chain which transform the raw LLM/ChatModel output into a more workable format.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "completion_chain = prompt | OpenAI(temperature=0)\n",
    "\n",
    "main_chain = RunnableParallel(\n",
    "    completion=completion_chain, prompt_value=prompt\n",
    ") | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "\n",
    "\n",
    "main_chain.invoke({\"query\": \"who is leo di caprios gf?\"})\n",
    "# Action(action='search', action_input='leo di caprio girlfriend')\n",
    "```\n",
    "- A user query: \"Who is Leo DiCaprio's girlfriend?\"\n",
    "- The prompt instructs the LLM to return an object with the fields:\n",
    "    - action: The action to take.\n",
    "    - action_input: The input for the action.\n",
    "- The initial LLM output: {\"action\": \"search\"}. This fails validation because the action_input field is missing. The action_input field is required by the schema defined in the Action class.\n",
    "- The RetryOutputParser:\n",
    "    - Takes the initial invalid output.\n",
    "    - Reuses the original prompt (prompt_value) to ask the LLM to generate a better response.\n",
    "    - Provides the invalid output as additional context to guide the LLM.\n",
    "- The LLM now generates a corrected response: {\"action\": \"search\", \"action_input\": \"leo di caprio girlfriend\"}\n",
    "\n",
    "It's important to note that the output of RunnableParallel is piped to RunnableLambda which does the retry work. The purpose of RunnableParallel here is to parallelize independent tasks. In this example: The retry itself is sequential and dependent on the raw output, so it cannot and should not run in parallel.\n",
    "\n",
    "The retry loop in the RetryOutputParser is encapsulated within its parse_with_prompt method. In the context of RunnableParallel, the retry loop doesn't run in parallel itself—it runs sequentially within the RetryOutputParser as part of its task.\n",
    "\n",
    "Inside RetryOutputParser.parse_with_prompt:\n",
    "- Initial Attempt: The parser tries to validate the raw output (bad_response) using parser.parse().\n",
    "- Error Handling: If validation fails (e.g., due to missing fields), it catches the error and retries with the LLM.\n",
    "- Re-Prompting Logic: The original prompt (prompt_value) is sent to the LLM again. The invalid output is included as additional context, helping the LLM generate a better response.\n",
    "- Retry Loop: This retry process continues until the output passes validation or a maximum retry count is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step.\n",
    "\n",
    "1) Document loaders\n",
    "\n",
    "Document loaders load documents from many different sources. LangChain provides over 100 different document loaders as well as integrations with other major providers in the space, like AirByte and Unstructured. LangChain provides integrations to load all types of documents (HTML, PDF, code) from all types of locations (private S3 buckets, public websites).\n",
    "\n",
    "2) Text Splitting\n",
    "\n",
    "A key part of retrieval is fetching only the relevant parts of documents. This involves several transformation steps to prepare the documents for retrieval. One of the primary ones here is splitting (or chunking) a large document into smaller chunks. LangChain provides several transformation algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\n",
    "\n",
    "3) Text embedding models\n",
    "\n",
    "Another key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.\n",
    "\n",
    "4) Vector stores\n",
    "\n",
    "With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.\n",
    "\n",
    "5) Retrievers\n",
    "\n",
    "Once the data is in the database, you still need to retrieve it. LangChain supports many different retrieval algorithms and is one of the places where we add the most value. LangChain supports basic methods that are easy to get started - namely simple semantic search. However, we have also added a collection of algorithms on top of this to increase performance. These include:\n",
    "- Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\n",
    "- Self Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query.\n",
    "- Ensemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\n",
    "\n",
    "6) Indexing\n",
    "\n",
    "The LangChain Indexing API syncs your data from any source into a vector store, helping you:\n",
    "- Avoid writing duplicated content into the vector store\n",
    "- Avoid re-writing unchanged content\n",
    "- Avoid re-computing embeddings over unchanged content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval: Document Loaders\n",
    "\n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval: Custom Document Loader\n",
    "\n",
    "Applications based on LLMs frequently entail extracting data from databases or files, like PDFs, and converting it into a format that LLMs can utilize. In LangChain, this usually involves creating Document objects, which encapsulate the extracted text (page_content) along with metadata—a dictionary containing details about the document, such as the author's name or the date of publication.\n",
    "\n",
    "Document objects are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the Document to generate a desired response (e.g., summarizing the document). Documents can be either used immediately or indexed into a vectorstore for future retrieval and use.\n",
    "\n",
    "The main abstractions for Document Loading are:\n",
    "- Document: Contains text and metadata\n",
    "- BaseLoader: Use to convert raw data into Documents\n",
    "- Blob: A representation of binary data that's located either in a file or in memory\n",
    "- BaseBlobParser: Logic to parse a Blob to yield Document objects\n",
    "\n",
    "This guide will demonstrate how to write custom document loading:\n",
    "- Create a standard document Loader by sub-classing from BaseLoader.\n",
    "- Create a parser using BaseBlobParser and use it in conjunction with Blob and BlobLoaders. This is useful primarily when working with files.\n",
    "\n",
    "A document loader can be implemented by sub-classing from a BaseLoader which provides a standard interface for loading documents:\n",
    "- lazy_load: Used to load documents one by one lazily. Use for production code.\n",
    "- alazy_load: Async variant of lazy_load\n",
    "- load: Used to load all the documents into memory eagerly. Use for prototyping or interactive work.\n",
    "- aload: Used to load all the documents into memory eagerly. Use for prototyping or interactive work. Added in 2024-04 to LangChain.\n",
    "\n",
    "All configuration is expected to be passed through the initializer (init). This was a design choice made by LangChain to make sure that once a document loader has been instantiated it has all the information needed to load documents.\n",
    "\n",
    "Let's create an example of a standard document loader that loads a file and creates a document from each line in the file:\n",
    "```python \n",
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class CustomDocumentLoader(BaseLoader):\n",
    "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path to the file to load.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "        When you're implementing lazy load methods, you should use a generator\n",
    "        to yield documents one by one.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "\n",
    "    # alazy_load is OPTIONAL.\n",
    "    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
    "    async def alazy_load(\n",
    "        self,\n",
    "    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
    "        # Requires aiofiles\n",
    "        # Install with `pip install aiofiles`\n",
    "        # https://github.com/Tinche/aiofiles\n",
    "        import aiofiles\n",
    "\n",
    "        async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            async for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "```\n",
    "\n",
    "To test out the document loader, we need a file with some quality content.\n",
    "\n",
    "```python\n",
    "with open(\"./meow.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    quality_content = \"meow meow🐱 \\n meow meow🐱 \\n meow😻😻\"\n",
    "    f.write(quality_content)\n",
    "\n",
    "loader = CustomDocumentLoader(\"./meow.txt\")\n",
    "\n",
    "## Test out the lazy load interface\n",
    "for doc in loader.lazy_load():\n",
    "    print()\n",
    "    print(type(doc))\n",
    "    print(doc)\n",
    "# <class 'langchain_core.documents.base.Document'>\n",
    "# page_content='meow meow🐱 \\n' metadata={'line_number': 0, 'source': './meow.txt'}\n",
    "\n",
    "# <class 'langchain_core.documents.base.Document'>\n",
    "# page_content=' meow meow🐱 \\n' metadata={'line_number': 1, 'source': './meow.txt'}\n",
    "\n",
    "# <class 'langchain_core.documents.base.Document'>\n",
    "# page_content=' meow😻😻' metadata={'line_number': 2, 'source': './meow.txt'}\n",
    "\n",
    "## Test out the async implementation\n",
    "async for doc in loader.alazy_load():\n",
    "    print()\n",
    "    print(type(doc))\n",
    "    print(doc)\n",
    "```\n",
    "\n",
    "load() can be helpful in an interactive environment such as a jupyter notebook. Avoid using it for production code since eager loading assumes that all the content can fit into memory, which is not always the case, especially for enterprise data.\n",
    "\n",
    "Many document loaders involve parsing files. The difference between such loaders usually stems from how the file is parsed rather than how the file is loaded. For example, you can use open to read the binary content of either a PDF or a markdown file, but you need different parsing logic to convert that binary data into text. As a result, it can be helpful to decouple the parsing logic from the loading logic, which makes it easier to re-use a given parser regardless of how the data was loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval: Custom Document Parser\n",
    "\n",
    "Do not conflate the Loader with the Parser. The BaseLoader is responsible for directly loading documents from a source (e.g., files, APIs, databases) and converting them into Document objects. It works at a higher level of abstraction.\n",
    "- Handles complete loading and parsing: Combines both fetching the data and parsing it into Document objects.\n",
    "- Designed for specific sources like text files, APIs, or databases.\n",
    "- Provides standard methods:\n",
    "    - lazy_load: Loads documents one at a time (lazy loading).\n",
    "    - alazy_load: Async variant of lazy loading.\n",
    "    - load: Loads all documents into memory at once (eager loading).\n",
    "    - aload: Async variant of eager loading.\n",
    "\n",
    "Use BaseLoader when the source (e.g., a file, database, or API) is known, and you want to quickly transform raw data into Document objects without extra modularity.\n",
    "\n",
    "BaseBlobParser\n",
    "\n",
    "**The BaseBlobParser focuses solely on parsing raw binary data (blobs) into Document objects. It is designed for scenarios where the data-loading logic is decoupled from the data-parsing logic**.\n",
    "- Works with Blob objects, which represent binary data from either files or memory.\n",
    "- Only parses the provided binary data into Document objects—it does not handle loading the data.\n",
    "- Provides methods like:\n",
    "    - lazy_parse: Parses blobs one by one (lazy parsing).\n",
    "\n",
    "Use BaseBlobParser when:\n",
    "- You need modularity: Data loading (e.g., fetching files) and data parsing are handled separately.\n",
    "- The same parsing logic can be reused across different types of data sources (e.g., files, in-memory data, APIs).\n",
    "- Suitable for files or binary data that require specialized parsing (e.g., PDFs, CSVs, JSON).\n",
    "\n",
    "A BaseBlobParser is an interface that accepts a blob and outputs a list of Document objects. A blob is a representation of data that lives either in memory or in a file. LangChain python has a Blob primitive which is inspired by the Blob WebAPI spec.\n",
    "\n",
    "```python\n",
    "from langchain_core.document_loaders import BaseBlobParser, Blob\n",
    "\n",
    "class MyParser(BaseBlobParser):\n",
    "    \"\"\"A simple parser that creates a document from each line.\"\"\"\n",
    "\n",
    "    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n",
    "        \"\"\"Parse a blob into a document line by line.\"\"\"\n",
    "        line_number = 0\n",
    "        with blob.as_bytes_io() as f:\n",
    "            for line in f:\n",
    "                line_number += 1\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": blob.source},\n",
    "                )\n",
    "\n",
    "blob = Blob.from_path(\"./meow.txt\")\n",
    "parser = MyParser()\n",
    "\n",
    "list(parser.lazy_parse(blob))\n",
    "# [Document(page_content='meow meow🐱 \\n', metadata={'line_number': 1, 'source': './meow.txt'}),\n",
    "#  Document(page_content=' meow meow🐱 \\n', metadata={'line_number': 2, 'source': './meow.txt'}),\n",
    "#  Document(page_content=' meow😻😻', metadata={'line_number': 3, 'source': './meow.txt'})]\n",
    "```\n",
    "\n",
    "**Using the blob API also allows one to load content direclty from memory without having to read it from a file!**\n",
    "\n",
    "```python\n",
    "blob = Blob(data=b\"some data from memory\\nmeow\")\n",
    "list(parser.lazy_parse(blob))\n",
    "# [Document(page_content='some data from memory\\n', metadata={'line_number': 1, 'source': None}),\n",
    "#  Document(page_content='meow', metadata={'line_number': 2, 'source': None})]\n",
    "```\n",
    "\n",
    "The following illustrates features of the Blob API:\n",
    "\n",
    "```python\n",
    "blob = Blob.from_path(\"./meow.txt\", metadata={\"foo\": \"bar\"})\n",
    "\n",
    "blob.encoding\n",
    "# 'utf-8'\n",
    "\n",
    "blob.as_bytes()\n",
    "# b'meow meow\\xf0\\x9f\\x90\\xb1 \\n meow meow\\xf0\\x9f\\x90\\xb1 \\n meow\\xf0\\x9f\\x98\\xbb\\xf0\\x9f\\x98\\xbb'\n",
    "\n",
    "blob.as_string()\n",
    "# 'meow meow🐱 \\n meow meow🐱 \\n meow😻😻'\n",
    "\n",
    "blob.as_bytes_io()\n",
    "# <contextlib._GeneratorContextManager at 0x743f34324450>\n",
    "\n",
    "blob.metadata\n",
    "# {'foo': 'bar'}\n",
    "\n",
    "blob.source\n",
    "# './meow.txt'\n",
    "```\n",
    "\n",
    "Blob Loaders\n",
    "\n",
    "Let's distinguish DocumentLoader from BlobLoader: A DocumentLoader is a higher-level abstraction that directly converts raw data (e.g., from files, APIs, or databases) into Document objects, which include:\n",
    "- page_content: The main text content of the document.\n",
    "- metadata: Additional information about the document (e.g., source, author, creation date).\n",
    "\n",
    "DocumentLoader abstracts both data retrieval and parsing logic into a single interface. It is ideal for straightforward use cases where parsing is tightly coupled to data loading.\n",
    "\n",
    "Examples of Use Cases:\n",
    "- Loading text files where each line corresponds to a document.\n",
    "- Loading a JSON file where each record represents a document.\n",
    "\n",
    "A BlobLoader focuses on fetching raw binary data (referred to as \"blobs\") from a storage location. Unlike a DocumentLoader, it is not responsible for parsing the raw data into documents; instead, it works in tandem with a BlobParser, which handles parsing logic.\n",
    "\n",
    "Responsibilities of BlobLoader:\n",
    "- Encapsulates the loading logic for retrieving blobs from a specific source, such as:\n",
    "    - Local files (e.g., via FileSystemBlobLoader).\n",
    "    - Cloud storage (future support).\n",
    "- Provides blobs as raw binary data or streams to be processed further by a parser.\n",
    "- While a blob parser encapsulates the logic needed to parse binary data into documents, blob loaders encapsulate the logic that's necessary to load blobs from a given storage location. A the moment, LangChain only supports FileSystemBlobLoader.\n",
    "\n",
    "You can use the FileSystemBlobLoader to load blobs and then use the parser to parse them.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders.blob_loaders import FileSystemBlobLoader\n",
    "\n",
    "blob_loader = FileSystemBlobLoader(path=\".\", glob=\"*.mdx\", show_progress=True)\n",
    "\n",
    "parser = MyParser()\n",
    "for blob in blob_loader.yield_blobs():\n",
    "    for doc in parser.lazy_parse(blob):\n",
    "        print(doc)\n",
    "        break\n",
    "# page_content='# Microsoft Office\\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}\n",
    "# page_content='# Markdown\\n' metadata={'line_number': 1, 'source': 'markdown.mdx'}\n",
    "# page_content='# JSON\\n' metadata={'line_number': 1, 'source': 'json.mdx'}\n",
    "# page_content='---\\n' metadata={'line_number': 1, 'source': 'pdf.mdx'}\n",
    "# page_content='---\\n' metadata={'line_number': 1, 'source': 'index.mdx'}\n",
    "# page_content='# File Directory\\n' metadata={'line_number': 1, 'source': 'file_directory.mdx'}\n",
    "# page_content='# CSV\\n' metadata={'line_number': 1, 'source': 'csv.mdx'}\n",
    "# page_content='# HTML\\n' metadata={'line_number': 1, 'source': 'html.mdx'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval: Generic Loader\n",
    "\n",
    "LangChain has a GenericLoader abstraction which composes a BlobLoader with a BaseBlobParser. GenericLoader is meant to provide standardized classmethods that make it easy to use existing BlobLoader implementations. At the moment, only the FileSystemBlobLoader is supported.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    path=\".\", glob=\"*.mdx\", show_progress=True, parser=MyParser()\n",
    ")\n",
    "\n",
    "for idx, doc in enumerate(loader.lazy_load()):\n",
    "    if idx < 5:\n",
    "        print(doc)\n",
    "\n",
    "print(\"... output truncated for demo purposes\")\n",
    "# page_content='# Microsoft Office\\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}\n",
    "# page_content='\\n' metadata={'line_number': 2, 'source': 'office_file.mdx'}\n",
    "# page_content='>[The Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\\n' metadata={'line_number': 3, 'source': 'office_file.mdx'}\n",
    "# page_content='\\n' metadata={'line_number': 4, 'source': 'office_file.mdx'}\n",
    "# page_content='This covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a document format that we can use downstream.\\n' metadata={'line_number': 5, 'source': 'office_file.mdx'}\n",
    "# ... output truncated for demo purposes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval: CSV Loader\n",
    "\n",
    "A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n",
    "\n",
    "Each Row in the CSV Becomes a Separate Document:\n",
    "- The loader treats each row in the CSV file as an individual Document object.\n",
    "- This means that:\n",
    "    - The content of the row (all columns combined) becomes the page_content of the Document.\n",
    "    - Metadata, such as column names and other details, can also be included in the metadata field of the Document.\n",
    "\n",
    "CSV File Example:\n",
    "```csv\n",
    "name,age,city\n",
    "Alice,30,New York\n",
    "Bob,25,Los Angeles\n",
    "Charlie,35,Chicago\n",
    "```\n",
    "\n",
    "If you load this CSV file using the CSV Document Loader, you’ll get three separate Document objects, one for each row:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"Alice,30,New York\",\n",
    "    metadata={\"source\": \"data.csv\", \"line_number\": 1}\n",
    ")\n",
    "\n",
    "Document(\n",
    "    page_content=\"Bob,25,Los Angeles\",\n",
    "    metadata={\"source\": \"data.csv\", \"line_number\": 2}\n",
    ")\n",
    "\n",
    "Document(\n",
    "    page_content=\"Charlie,35,Chicago\",\n",
    "    metadata={\"source\": \"data.csv\", \"line_number\": 3}\n",
    ")\n",
    "```\n",
    "- You can perform operations like querying, embedding, or indexing on each row separately, which is useful for tasks like retrieval-augmented generation (RAG). \n",
    "- Each document can carry metadata specific to its row, such as the row number, file name, or specific column names. \n",
    "- Processing each row as a separate document helps with handling large CSVs by working on chunks rather than loading the entire file into memory.\n",
    "\n",
    "How to include Column Headers for Context\n",
    "\n",
    "When using the CSV Document Loader, each row of the CSV file is converted into a Document object with:\n",
    "- page_content: The raw text of the row (comma-separated values).\n",
    "- metadata: This includes the column headers as keys and the corresponding row values as their values.\n",
    "\n",
    "CSV File Example:\n",
    "```csv\n",
    "name,age,city\n",
    "Alice,30,New York\n",
    "Bob,25,Los Angeles\n",
    "Charlie,35,Chicago\n",
    "```\n",
    "\n",
    "You can store the column headers and their respective values in the metadata:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"Alice,30,New York\",\n",
    "    metadata={\n",
    "        \"source\": \"data.csv\",\n",
    "        \"line_number\": 1,\n",
    "        \"name\": \"Alice\",\n",
    "        \"age\": \"30\",\n",
    "        \"city\": \"New York\"\n",
    "    }\n",
    ")\n",
    "\n",
    "Document(\n",
    "    page_content=\"Bob,25,Los Angeles\",\n",
    "    metadata={\n",
    "        \"source\": \"data.csv\",\n",
    "        \"line_number\": 2,\n",
    "        \"name\": \"Bob\",\n",
    "        \"age\": \"25\",\n",
    "        \"city\": \"Los Angeles\"\n",
    "    }\n",
    ")\n",
    "\n",
    "Document(\n",
    "    page_content=\"Charlie,35,Chicago\",\n",
    "    metadata={\n",
    "        \"source\": \"data.csv\",\n",
    "        \"line_number\": 3,\n",
    "        \"name\": \"Charlie\",\n",
    "        \"age\": \"35\",\n",
    "        \"city\": \"Chicago\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "Associating column names to their respective values in a Retrieval-Augmented Generation (RAG) flow requires careful design, as vector databases typically work with unstructured or semi-structured text rather than tabular data directly. To ensure that both column names (headers) and their corresponding values are meaningfully represented in the vector embeddings, you can flatten the structured metadata into text during vectorization while maintaining the context.\n",
    "- Flatten the structured data (metadata) into a natural language or key-value text representation.\n",
    "- This provides the necessary context to the vectorizer, associating column names with their values.\n",
    "\n",
    "For a CSV row like:\n",
    "\n",
    "```csv\n",
    "name,age,city\n",
    "Alice,30,New York\n",
    "```\n",
    "\n",
    "- Flatten the metadata into text: \"name: Alice, age: 30, city: New York\"\n",
    "- Natural Language Format: \"The person's name is Alice, their age is 30, and they live in New York.\"\n",
    "- This combined text becomes the input for vectorization.\n",
    "\n",
    "\n",
    "The Source Column\n",
    "\n",
    "The source_column argument in the LangChain CSV Loader allows you to specify a particular column in the CSV file that should be used as the source identifier for each document created from a row. By default, the CSV Loader assigns the same file_path as the source metadata for all documents created from the CSV file. This means all rows (documents) share the same source, which may not be ideal for certain use cases.\n",
    "\n",
    "If you load a CSV file without specifying source_column:\n",
    "\n",
    "```python\n",
    "loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv')\n",
    "data = loader.load()\n",
    "```\n",
    "\n",
    "Metadata for each Document:\n",
    "```python\n",
    "{'source': './example_data/mlb_teams_2012.csv', 'row': 0}\n",
    "{'source': './example_data/mlb_teams_2012.csv', 'row': 1}\n",
    "```\n",
    "\n",
    "If you load the same CSV file but specify source_column=\"Team\":\n",
    "\n",
    "```python\n",
    "loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv', source_column=\"Team\")\n",
    "data = loader.load()\n",
    "```\n",
    "\n",
    "Metadata for each Document:\n",
    "```python\n",
    "{'source': 'Nationals', 'row': 0}\n",
    "{'source': 'Reds', 'row': 1}\n",
    "{'source': 'Yankees', 'row': 2}\n",
    "```\n",
    "\n",
    "Now, the source metadata reflects the value in the \"Team\" column, which uniquely identifies each row (document).\n",
    "\n",
    "In retrieval-augmented generation (RAG) workflows, this feature is especially useful when:\n",
    "- You retrieve relevant rows (documents) from a vector database.\n",
    "- You want to include the source of the information in the model's output.\n",
    "\n",
    "Example\n",
    "\n",
    "Query: \"Which MLB team had the highest payroll in 2012?\"\n",
    "\n",
    "If source_column=\"Team\" is used, the retrieved document for \"Yankees\" will have the source set to \"Yankees\". This allows the model to output:\n",
    "\n",
    "```text\n",
    "The Yankees had the highest payroll in 2012 with $197.96 million.\n",
    "Source: Yankees\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval: File Directory Loader\n",
    "\n",
    "We can use the glob parameter to control which files to load. Note that here it doesn't load the .rst file or the .html files.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('../', glob=\"**/*.md\")\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "# 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE:\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/file_directory/\n",
    "https://python.langchain.com/v0.1/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL and Runnable Interface"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
