{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Router\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "User input does not need to be passed directly into the LLM. The prompt template provides additional context on the specific task at hand.\n",
    "\n",
    "Prompt Templates are designed for single-turn interactions. It typically accepts a single template string with placeholders that can be dynamically filled with inputs.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Translate the following English text to French: {text}\"\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "filled_prompt = prompt.format(text=\"Hello, how are you?\")\n",
    "print(filled_prompt)\n",
    "# Output: Translate the following English text to French: Hello, how are you?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Template\n",
    "\n",
    "ChatPromptTemplate is used for multi-turn interactions or prompts designed for chat-based language models. It is composed of messages like SystemMessage, HumanMessage, and AIMessage. These represent the context, user input, and AI responses, respectively.\n",
    "\n",
    "ChatPromptTemplate can take messages in various formats, including predefined message classes (e.g., SystemMessage, HumanMessage) as well as tuple formats. LangChain provides message classes such as SystemMessage, HumanMessage, AIMessage, and ChatMessage. These are structured representations for different roles in a chat.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessage, HumanMessage\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Can you help me with my math homework?\")\n",
    "])\n",
    "formatted_messages = chat_prompt.format_messages()\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "```\n",
    "\n",
    "Instead of using the predefined message classes, you can represent messages as tuples, where the first element is the role (e.g., \"system\", \"human\", \"assistant\"), and the second element is the content of the message.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Can you help me solve a quadratic equation?\"),\n",
    "    (\"assistant\", \"Of course! What's the equation?\")\n",
    "])\n",
    "formatted_messages = chat_prompt.format_messages()\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Template and Message Placeholder\n",
    "\n",
    "The MessagesPlaceholder in LangChain is a utility used within ChatPromptTemplate to insert dynamic chat histories or message sequences into a prompt. This is particularly useful when you want to incorporate conversation history dynamically without hardcoding all previous messages into the prompt.\n",
    "\n",
    "MessagesPlaceholder is commonly used when:\n",
    "- Maintaining Context: You want to include past messages in a chat.\n",
    "- Dynamic History: The history may change or be trimmed based on application logic.\n",
    "- Trimming: You can preprocess the chat_history (e.g., trim irrelevant messages) before passing it to the prompt.\n",
    "\n",
    "Here’s how to use MessagesPlaceholder in a ChatPromptTemplate:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"What do you think about {topic}?\")\n",
    "])\n",
    "```\n",
    "\n",
    "At runtime, you pass the actual chat history to replace the MessagesPlaceholder:\n",
    "\n",
    "```python\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Can you summarize quantum physics?\"),\n",
    "    AIMessage(content=\"Sure! Quantum physics deals with subatomic particles and wave-particle duality.\"),\n",
    "]\n",
    "\n",
    "formatted_messages = chat_prompt.format_messages(\n",
    "    chat_history=chat_history,  # Replace placeholder\n",
    "    topic=\"black holes\"\n",
    ")\n",
    "\n",
    "for message in formatted_messages:\n",
    "    print(f\"{message.type}: {message.content}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Template and LCEL\n",
    "\n",
    "PromptTemplate and ChatPromptTemplate implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n",
    "\n",
    "PromptTemplate accepts a dictionary (of the prompt variables) and returns a StringPromptValue. A ChatPromptTemplate accepts a dictionary and returns a ChatPromptValue.\n",
    "\n",
    "```python\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "prompt_val # StringPromptValue(text='Tell me a funny joke about chickens.')\n",
    "prompt_val.to_string() # 'Tell me a funny joke about chickens.'\n",
    "prompt_val.to_messages() # [HumanMessage(content='Tell me a funny joke about chickens.')]\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "chat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})\n",
    "chat_val.to_messages()\n",
    "# [SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompt Templates and Example Selectors\n",
    "\n",
    "The purpose of Example Selectors in LangChain is to dynamically choose the most relevant examples to include in a prompt. This is especially important when you have a large dataset of examples and cannot include all of them due to constraints like token limits or relevance to the current input. Example selectors ensure that the chosen examples are tailored to the specific input, improving the quality of responses from the language model.\n",
    "\n",
    "Large language models like GPT often perform better with few-shot learning, where the prompt includes input-output examples to demonstrate the task. Including irrelevant or too many examples can lead to suboptimal responses or exceed token limits.\n",
    "\n",
    "Example Selectors dynamically choose the most relevant subset of examples based on the current input, ensuring optimal context is provided to the model.\n",
    "\n",
    "LangChain provides several prebuilt example selectors, and you can also implement custom ones. Common types include:\n",
    "- Similarity Selector\n",
    "    - huih\n",
    "- Max Marginal Relevance (MMR) Selector\n",
    "    - Balances relevance and diversity by selecting examples that are similar to the input but not too similar to each other.\n",
    "- Length-Based Selector\n",
    "    - Chooses examples that fit within a token limit or are of a similar length to the input.\n",
    "- N-gram Overlap Selector\n",
    "    - Selects examples based on shared n-grams between the input and examples.\n",
    "\n",
    "Select by length example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\n",
    "\n",
    "```python\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    # The examples it has available to choose from.\n",
    "    examples=examples,\n",
    "    # The PromptTemplate being used to format the examples.\n",
    "    example_prompt=example_prompt,\n",
    "    # The maximum length that the formatted examples should be.\n",
    "    # Length is measured by the get_text_length function below.\n",
    "    max_length=25,\n",
    "    # The function used to get the length of a string, which is used\n",
    "    # to determine which examples to include. It is commented out because\n",
    "    # it is provided as a default value if none is specified.\n",
    "    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "```\n",
    "\n",
    "The MaxMarginalRelevanceExampleSelector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import (\n",
    "    MaxMarginalRelevanceExampleSelector,\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# Input is a feeling, so should select the happy/sad example as the first one\n",
    "print(mmr_prompt.format(adjective=\"worried\"))\n",
    "Give the antonym of every input\n",
    "\n",
    "Input: happy\n",
    "Output: sad\n",
    "\n",
    "Input: windy\n",
    "Output: calm\n",
    "\n",
    "Input: worried\n",
    "Output:\n",
    "\n",
    "\n",
    "# Let's compare this to what we would just get if we went solely off of similarity,\n",
    "# by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=2,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "print(similar_prompt.format(adjective=\"worried\"))\n",
    "Give the antonym of every input\n",
    "\n",
    "Input: happy\n",
    "Output: sad\n",
    "\n",
    "Input: sunny\n",
    "Output: gloomy\n",
    "\n",
    "Input: worried\n",
    "Output:\n",
    "```\n",
    "\n",
    "The NGramOverlapExampleSelector selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\n",
    "\n",
    "The selector allows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input.\n",
    "\n",
    "```python\n",
    "from langchain_community.example_selector.ngram_overlap import (\n",
    "    NGramOverlapExampleSelector,\n",
    ")\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a fictional translation task.\n",
    "examples = [\n",
    "    {\"input\": \"See Spot run.\", \"output\": \"Ver correr a Spot.\"},\n",
    "    {\"input\": \"My dog barks.\", \"output\": \"Mi perro ladra.\"},\n",
    "    {\"input\": \"Spot can run.\", \"output\": \"Spot puede correr.\"},\n",
    "]\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector(\n",
    "    # The examples it has available to choose from.\n",
    "    examples=examples,\n",
    "    # The PromptTemplate being used to format the examples.\n",
    "    example_prompt=example_prompt,\n",
    "    # The threshold, at which selector stops.\n",
    "    # It is set to -1.0 by default.\n",
    "    threshold=-1.0,\n",
    "    # For negative threshold:\n",
    "    # Selector sorts examples by ngram overlap score, and excludes none.\n",
    "    # For threshold greater than 1.0:\n",
    "    # Selector excludes all examples, and returns an empty list.\n",
    "    # For threshold equal to 0.0:\n",
    "    # Selector sorts examples by ngram overlap score,\n",
    "    # and excludes those with no ngram overlap with input.\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the Spanish translation of every input\",\n",
    "    suffix=\"Input: {sentence}\\nOutput:\",\n",
    "    input_variables=[\"sentence\"],\n",
    ")\n",
    "\n",
    "# An example input with large ngram overlap with \"Spot can run.\"\n",
    "# and no overlap with \"My dog barks.\"\n",
    "print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\n",
    "Give the Spanish translation of every input\n",
    "\n",
    "Input: Spot can run.\n",
    "Output: Spot puede correr.\n",
    "\n",
    "Input: See Spot run.\n",
    "Output: Ver correr a Spot.\n",
    "\n",
    "Input: My dog barks.\n",
    "Output: Mi perro ladra.\n",
    "\n",
    "Input: Spot can run fast.\n",
    "Output:\n",
    "\n",
    "Select by similarity selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # The number of examples to produce.\n",
    "    k=1,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# Input is a feeling, so should select the happy/sad example\n",
    "print(similar_prompt.format(adjective=\"worried\"))\n",
    "Give the antonym of every input\n",
    "\n",
    "Input: happy\n",
    "Output: sad\n",
    "\n",
    "Input: worried\n",
    "Output:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Chat Message Prompt Template\n",
    "\n",
    "FewShotPromptTemplate is a general-purpose template for few-shot prompting. Examples are stored as plain text templates or dictionaries. Examples are formatted using a simple PromptTemplate. Includes a prefix (instructions or context) and a suffix (the current input or question). Dynamically selects the most relevant examples using an ExampleSelector (e.g., SemanticSimilarityExampleSelector).\n",
    "\n",
    "FewShotChatMessagePromptTemplate is specifically designed for chat-based models (e.g., ChatGPT, Anthropic, or any model that uses chat messages). Formats examples into chat-style messages, distinguishing between system, human, and ai roles. Each example is converted into a sequence of chat messages (e.g., Human: Input\\nAI: Output). Uses ChatPromptTemplate to define message roles. Can dynamically insert examples in a conversational format using ExampleSelector. Suitable for conversational tasks where examples need to be formatted as chat exchanges.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"3+3\", \"output\": \"6\"},\n",
    "]\n",
    "\n",
    "few_shot_chat_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_chat_prompt.format())\n",
    "Human: 2+2\n",
    "AI: 4\n",
    "Human: 3+3\n",
    "AI: 6\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_chat_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "\n",
    "chain = final_prompt | ChatAnthropic(temperature=0.0)\n",
    "\n",
    "chain.invoke({\"input\": \"What's the square of a triangle?\"})\n",
    "AIMessage(content=' Triangles do not have a \"square\". A square refers to a shape with 4 equal sides and 4 right angles. Triangles have 3 sides and 3 angles.\\n\\nThe area of a triangle can be calculated using the formula:\\n\\nA = 1/2 * b * h\\n\\nWhere:\\n\\nA is the area \\nb is the base (the length of one of the sides)\\nh is the height (the length from the base to the opposite vertex)\\n\\nSo the area depends on the specific dimensions of the triangle. There is no single \"square of a triangle\". The area can vary greatly depending on the base and height measurements.', additional_kwargs={}, example=False)\n",
    "```\n",
    "\n",
    "Sometimes you may want to condition which examples are shown based on the input. For this, you can replace the examples with an example_selector.\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "    {\"input\": \"2+4\", \"output\": \"6\"},\n",
    "    {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"},\n",
    "    {\n",
    "        \"input\": \"Write me a poem about the moon\",\n",
    "        \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "# The prompt template will load examples by passing the input do the `select_examples` method\n",
    "example_selector.select_examples({\"input\": \"horse\"})\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "\n",
    "chain = final_prompt | ChatAnthropic(temperature=0.0)\n",
    "\n",
    "chain.invoke({\"input\": \"What's 3+3?\"})\n",
    "# AIMessage(content=' 3 + 3 = 6', additional_kwargs={}, example=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Prompt Templates\n",
    "\n",
    "It can make sense to \"partial\" a prompt template - e.g. pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\n",
    "\n",
    "One common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, foo and baz. If you get the foo value early on in the chain, but the baz value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the foo value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{foo}{bar}\")\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(partial_prompt.format(bar=\"baz\"))\n",
    "```\n",
    "\n",
    "The other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is a bit annoying. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date.\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))\n",
    "```\n",
    "\n",
    "You can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    partial_variables={\"date\": _get_datetime},\n",
    ")\n",
    "print(prompt.format(adjective=\"funny\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Composition\n",
    "\n",
    "LangChain provides a user friendly interface for composing different parts of prompts together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse.\n",
    "\n",
    "When working with string prompts, each template is joined together. You can work with either prompts directly or strings:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    + \", make it funny\"\n",
    "    + \"\\n\\nand in {language}\"\n",
    ")\n",
    "prompt\n",
    "# PromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}')\n",
    "```\n",
    "\n",
    "LangChain includes an abstraction PipelinePromptTemplate, which can be useful when you want to reuse parts of prompts. \n",
    "\n",
    "```python\n",
    "from langchain_core.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")\n",
    "\n",
    "pipeline_prompt.input_variables # ['example_q', 'person', 'input', 'example_a']\n",
    "\n",
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        person=\"Elon Musk\",\n",
    "        example_q=\"What's your favorite car?\",\n",
    "        example_a=\"Tesla\",\n",
    "        input=\"What's your favorite social media site?\",\n",
    "    )\n",
    ")\n",
    "You are impersonating Elon Musk.\n",
    "\n",
    "Here's an example of an interaction:\n",
    "\n",
    "Q: What's your favorite car?\n",
    "A: Tesla\n",
    "\n",
    "Now, do this for real!\n",
    "\n",
    "Q: What's your favorite social media site?\n",
    "A:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "The LLM is a pure text completion model. The LLM takes a string prompt as input and output a string completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model\n",
    "\n",
    "Chat Models are built on top of LLMs. The LLM objects take string as input and output string. The ChatModel objects take a list of messages as input and output a message. While chat models use language models under the hood, the interface they use is a bit different. Rather than using a \"text in, text out\" API, they use an interface where \"chat messages\" are the inputs and outputs.\n",
    "\n",
    "A chat model is a language model that uses chat messages as inputs and returns chat messages as outputs (as opposed to using plain text). LangChain has integrations with many model providers (OpenAI, Cohere, Hugging Face, etc.) and exposes a standard interface to interact with all of these models.\n",
    "\n",
    "The chat model interface is based around messages rather than raw text. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, FunctionMessage and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model and LCEL\n",
    "\n",
    "Chat models implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n",
    "\n",
    "Chat models accept List[BaseMessage] as inputs, or objects which can be coerced to messages, including str (converted to HumanMessage) and PromptValue.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "\n",
    "# using invoke\n",
    "chat.invoke(messages)\n",
    "# AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to fit the noise in the training data, leading to poor generalization on unseen data. Regularization techniques introduce additional constraints or penalties to the model's objective function, discouraging it from becoming overly complex and promoting simpler and more generalizable models. Regularization helps to strike a balance between fitting the training data well and avoiding overfitting, leading to better performance on new, unseen data.\")\n",
    "\n",
    "# using stream\n",
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "# The purpose of model regularization is to prevent overfitting and improve the generalization of a machine learning model. Overfitting occurs when a model is too complex and learns the noise or random variations in the training data, which leads to poor performance on new, unseen data. Regularization techniques introduce additional constraints or penalties to the model's learning process, discouraging it from fitting the noise and reducing the complexity of the model. This helps to improve the model's ability to generalize well and make accurate predictions on unseen data.    \n",
    "\n",
    "# using batch\n",
    "chat.batch([messages])\n",
    "# [AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to learn the noise or random fluctuations in the training data, rather than the underlying patterns or relationships. Regularization techniques add a penalty term to the model's objective function, which discourages the model from becoming too complex and helps it generalize better to new, unseen data. This improves the model's ability to make accurate predictions on new data by reducing the variance and increasing the model's overall performance.\")]\n",
    "\n",
    "# using ainvoke\n",
    "await chat.ainvoke(messages)\n",
    "# AIMessage(content='The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning general patterns and relationships. This leads to poor performance on new, unseen data.\\n\\nRegularization techniques introduce additional constraints or penalties to the model during training, discouraging it from becoming overly complex. This helps to strike a balance between fitting the training data well and generalizing to new data. Regularization techniques can include adding a penalty term to the loss function, such as L1 or L2 regularization, or using techniques like dropout or early stopping. By regularizing the model, it encourages it to learn the most relevant features and reduce the impact of noise or outliers in the data.')\n",
    "\n",
    "# using astream\n",
    "async for chunk in chat.astream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "# The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning the underlying patterns. Regularization techniques help in reducing the complexity of the model by adding a penalty to the loss function. This penalty encourages the model to have smaller weights or fewer features, making it more generalized and less prone to overfitting. The goal is to find the right balance between fitting the training data well and being able to generalize well to unseen data.\n",
    "\n",
    "# using astream log\n",
    "async for chunk in chat.astream_log(messages):\n",
    "    print(chunk) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Langsmith\n",
    "\n",
    "All ChatModels come with built-in LangSmith tracing. Just set the following environment variables:\n",
    "\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=<your-api-key>\n",
    "```\n",
    "\n",
    "and any ChatModel invocation (whether it's nested in a chain or not) will automatically be traced. A trace will include inputs, outputs, latency, token usage, invocation params, environment params, and more.\n",
    "\n",
    "In LangSmith you can then provide feedback for any trace, compile annotated datasets for evals, debug performance in the playground, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model Message types\n",
    "\n",
    "ChatModels take a list of messages as input and return a message. There are a few different types of messages. All messages have a role and a content property. The role describes WHO is saying the message. LangChain has different message classes for different roles. The content property describes the content of the message. This can be a few different things:\n",
    "- A string (most models deal this type of content)\n",
    "- A List of dictionaries (this is used for multi-modal input, where the dictionary contains information about that input type and that input location)\n",
    "\n",
    "In addition, messages have an additional_kwargs property. This is where additional information about messages can be passed. This is largely used for input parameters that are provider specific and not general. The best known example of this is function_call from OpenAI.\n",
    "\n",
    "Message Types:\n",
    "- HumanMessage\n",
    "    - This represents a message from the user. Generally consists only of content.\n",
    "- AIMessage\n",
    "    - This represents a message from the model. This may have additional_kwargs in it - for example tool_calls if using OpenAI tool calling.\n",
    "- SystemMessage\n",
    "    - This represents a system message, which tells the model how to behave. This generally only consists of content. Not every model supports this.\n",
    "- FunctionMessage\n",
    "    - This represents the result of a function call. In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.\n",
    "- ToolMessage\n",
    "    - This represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool message types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to the tool that was called to produce this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Streaming\n",
    "\n",
    "All ChatModels implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all ChatModels basic support for streaming.\n",
    "\n",
    "Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-token streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator of tokens can work for any of our ChatModel integrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models and Tool calling\n",
    "\n",
    "Tool Calling in LangChain is a feature that allows a language model to generate structured output by simulating the \"calling\" of a predefined tool. This process involves the model creating arguments that match a user-defined schema for a tool, but the actual execution of the tool (or even whether the tool is executed) is left to the user.\n",
    "\n",
    "When we define a tool in LangChain, we specify:\n",
    "- What the tool does: Its purpose or functionality.\n",
    "- What inputs (arguments) it requires: A schema that defines the parameters the tool needs to execute.\n",
    "\n",
    "The model \"creates arguments\" by generating these input values based on the user's query or prompt. Consider a tool for basic arithmetic operations:\n",
    "\n",
    "```python\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"calculate\"\n",
    "    args_schema = {\n",
    "        \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]},\n",
    "        \"num1\": {\"type\": \"number\"},\n",
    "        \"num2\": {\"type\": \"number\"},\n",
    "    }\n",
    "```\n",
    "- Operation: Specifies the type of calculation (e.g., \"add\").\n",
    "- The first number for the operation (e.g., 5).\n",
    "- The second number for the operation (e.g., 7).\n",
    "\n",
    "When a user provides input like: \"Add 5 and 7.\" The model interprets the query and generates the following arguments:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"operation\": \"add\",\n",
    "    \"num1\": 5,\n",
    "    \"num2\": 7\n",
    "}\n",
    "```\n",
    "These arguments match the schema defined in the example CalculatorTool.\n",
    "\n",
    "In effect, despite the name, tool calling does not involve the model directly performing actions. Instead, the model provides parameters or arguments that conform to the schema, which can then be used by the user to trigger the tool or extract structured data.\n",
    "\n",
    "How Tool Calling Works:\n",
    "- Define the Tool:\n",
    "    - A \"tool\" in LangChain represents an action the model can \"call\" by generating arguments that match a predefined schema.\n",
    "    - The schema specifies the inputs the tool accepts and can include validations or constraints.\n",
    "- Prompt the Model:\n",
    "    - The user provides a prompt or query to the model.\n",
    "    - The model generates output that matches the schema of the tool, simulating a \"call\" to the tool.\n",
    "- User Handles the Execution:\n",
    "    - The user decides whether to actually execute the tool based on the generated arguments.\n",
    "    - Alternatively, the user may treat the structured output directly as the final result.\n",
    "\n",
    "Many LLM providers, including Anthropic, Cohere, Google, Mistral, OpenAI, and others, support variants of a tool calling feature. These features typically allow requests to the LLM to include available tools and their schemas, and for responses to include calls to these tools. For instance, given a search engine tool, an LLM might handle a query by first issuing a call to the search engine. The system calling the LLM can receive the tool call, execute it, and return the output to the LLM to inform its response. LangChain includes a suite of built-in tools and supports several methods for defining your own custom tools. Tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally.\n",
    "\n",
    "Providers adopt different conventions for formatting tool schemas and tool calls. For instance, Anthropic returns tool calls as parsed structures within a larger content block:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"<thinking>\\nI should use a tool.\\n</thinking>\",\n",
    "    \"type\": \"text\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"id_value\",\n",
    "    \"input\": {\"arg_name\": \"arg_value\"},\n",
    "    \"name\": \"tool_name\",\n",
    "    \"type\": \"tool_use\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "whereas OpenAI separates tool calls into a distinct parameter, with arguments as JSON strings:\n",
    "```json\n",
    "{\n",
    "  \"tool_calls\": [\n",
    "    {\n",
    "      \"id\": \"id_value\",\n",
    "      \"function\": {\n",
    "        \"arguments\": '{\"arg_name\": \"arg_value\"}',\n",
    "        \"name\": \"tool_name\"\n",
    "      },\n",
    "      \"type\": \"function\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "LangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\n",
    "\n",
    "For a model to be able to invoke tools, you need to pass tool schemas to it when making a chat request. LangChain ChatModels supporting tool calling features implement a .bind_tools method, which receives a list of LangChain tool objects, Pydantic classes, or JSON Schemas and binds them to the chat model in the provider-specific expected format. That's a mouthful. Let's dissect this:\n",
    "- A tool in LangChain represents a predefined action the model can \"invoke\" by generating structured arguments that match a schema.\n",
    "- Binding Tools to the Model\n",
    "    - Tools must be bound to the model so that the model knows which tools are available and can generate arguments for them.\n",
    "    - Binding tools involves passing the tool schemas to the model, which adapts them to the specific format expected by the provider’s API (e.g., OpenAI, Anthropic).\n",
    "- .bind_tools() Method\n",
    "    - The .bind_tools() method in LangChain allows you to attach tools to a ChatModel instance.\n",
    "    - Once tools are bound, every subsequent API call to the model will include the tool schemas, enabling the model to simulate invoking them.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"calculate\"\n",
    "    description = \"Performs arithmetic operations\"\n",
    "    args_schema = {\n",
    "        \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]},\n",
    "        \"num1\": {\"type\": \"number\"},\n",
    "        \"num2\": {\"type\": \"number\"},\n",
    "    }\n",
    "\n",
    "    def _run(self, operation, num1, num2):\n",
    "        if operation == \"add\":\n",
    "            return num1 + num2\n",
    "        elif operation == \"subtract\":\n",
    "            return num1 - num2\n",
    "        elif operation == \"multiply\":\n",
    "            return num1 * num2\n",
    "        elif operation == \"divide\":\n",
    "            return num1 / num2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid operation\")\n",
    "\n",
    "# Use .bind_tools() to attach the tools to the model.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Instantiate the model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Instantiate the tool\n",
    "calculator_tool = CalculatorTool()\n",
    "\n",
    "# Bind the tool to the model\n",
    "llm_with_tools = llm.bind_tools([calculator_tool])\n",
    "```\n",
    "\n",
    "How Binding Works:\n",
    "- Schema Passing:\n",
    "    - When you call .bind_tools(), LangChain converts the tool definitions into the specific JSON schema format expected by the provider (e.g., OpenAI or Anthropic).\n",
    "- Model Awareness:\n",
    "    - After binding, the model becomes \"aware\" of the tools and their schemas. Every subsequent chat request automatically includes the tool schemas as part of the API call.\n",
    "- Tool Invocation by the Model:\n",
    "    - During a conversation, the model can decide whether to \"call\" a tool by generating arguments that match the tool schema.\n",
    "\n",
    "When you interact with the bound model, the model can now include tool invocations in its responses.\n",
    "\n",
    "```python\n",
    "response = llm_with_tools.invoke({\n",
    "    \"input\": \"Can you add 7 and 3?\"\n",
    "})\n",
    "\n",
    "print(response)\n",
    "\n",
    "# The model generates an output in the format:\n",
    "{\n",
    "    \"tool\": \"calculate\",\n",
    "    \"arguments\": {\n",
    "        \"operation\": \"add\",\n",
    "        \"num1\": 7,\n",
    "        \"num2\": 3\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We can define the schema for custom tools using the @tool decorator on Python functions:\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "```\n",
    "\n",
    "We can equivalently define the schema using Pydantic. Pydantic is useful when your tool inputs are more complex:\n",
    "\n",
    "```python\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "# We can bind them to chat models as follows:\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# We can use the bind_tools() method to handle converting Multiply to a \"tool\" and binding it to the model (i.e., passing it in each time the model is invoked).\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "```\n",
    "\n",
    "When you just use bind_tools(tools), the model can choose whether to return one tool call, multiple tool calls, or no tool calls at all. Some models support a tool_choice parameter that gives you some ability to force the model to call a tool. For models that support this, you can pass in the name of the tool you want the model to always call tool_choice=\"xyz_tool_name\". Or you can pass in tool_choice=\"any\" to force the model to call at least one tool, without specifying which tool specifically.\n",
    "\n",
    "Currently tool_choice=\"any\" functionality is supported by OpenAI, MistralAI, FireworksAI, and Groq. Currently Anthropic does not support tool_choice at all.\n",
    "\n",
    "If we wanted our model to always call the multiply tool we could do:\n",
    "```python\n",
    "always_multiply_llm = llm.bind_tools([multiply], tool_choice=\"multiply\")\n",
    "```\n",
    "\n",
    "And if we wanted it to always call at least one of add or multiply, we could do:\n",
    "```python\n",
    "always_call_tool_llm = llm.bind_tools([add, multiply], tool_choice=\"any\")\n",
    "```\n",
    "\n",
    "If tool calls are included in a LLM response, they are attached to the corresponding AIMessage or AIMessageChunk (when streaming) as a list of ToolCall objects in the .tool_calls attribute. A ToolCall is a typed dict that includes a tool name, dict of argument values, and (optionally) an identifier. Messages with no tool calls default to an empty list for this attribute.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools.invoke(query).tool_calls\n",
    "# [{'name': 'multiply',\n",
    "#   'args': {'a': 3, 'b': 12},\n",
    "#   'id': 'call_UL7E2232GfDHIQGOM4gJfEDD'},\n",
    "#  {'name': 'add',\n",
    "#   'args': {'a': 11, 'b': 49},\n",
    "#   'id': 'call_VKw8t5tpAuzvbHgdAXe9mjUx'}]\n",
    "```\n",
    "\n",
    "The .tool_calls attribute should contain valid tool calls. Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON). When parsing fails in these cases, instances of InvalidToolCall are populated in the .invalid_tool_calls attribute. An InvalidToolCall can have a name, string arguments, identifier, and error message.\n",
    "\n",
    "If desired, output parsers can further process the output. For example, we can convert back to the original Pydantic class:\n",
    "```python\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=[multiply, add])\n",
    "chain.invoke(query)\n",
    "# [multiply(a=3, b=12), add(a=11, b=49)]\n",
    "```\n",
    "\n",
    "When tools are called in a streaming context, message chunks will be populated with tool call chunk objects in a list via the .tool_call_chunks attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "OutputParsers convert the raw output of a language model into a format that can be used downstream. There are a few main types of OutputParsers, including:\n",
    "- Convert text from LLM into structured information (e.g. JSON)\n",
    "- Convert a ChatMessage into just a string\n",
    "- Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL and Runnable Interface"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
