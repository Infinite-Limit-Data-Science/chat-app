### Basic Formula for calculating max-batch-total-tokens

Given you are running a g5.48xlarge instance type, which has 192GiB of NVidia Cuda VRAM, and you want to run meta-llama/Llama-3.2-90B-Vision-Instruct, which is a 90B parameter model that requires around 188GiB of VRAM, that leaves 4GiB of VRAM remaining. For the HF TGI, the default max-concurrent-request parameter is 128. This is a reasonable number for many use cases. How many applications receive more than 100 concurrent requests?

Note MAX_BATCH_SIZE is not used for NVidia targets since they support unpadded inference. In TGI, memory usage is primarily driven by the number of tokens being processed in a batch. This includes both the input tokens (the text provided by users) and the output tokens (the text generated by the model). For example, if multiple requests are grouped into a batch and the combined number of tokens in that batch increases, the VRAM usage will also increase accordingly. The server allocates memory to handle these tokens effectively. **TGI automatically adjusts how it uses VRAM based on the available GPU memory and the total number of tokens in a batch. This means that the server will maximize the number of tokens it can handle while staying within the constraints of the GPU's VRAM capacity.**

In effect, the crucial arguments to factor are max-batch-total-tokens and max-batch-prefill-tokens. In order to calculate the memory for these two parameters, on the surface it may seem you only need to factor the number of layers in the model (‚Ñì), the dimension of the activations (d), max sequence length (k), and the number of bytes, given 4GiB of VRAM available:

$$
M = \frac{4 \, \text{GB}}{\ell \cdot d \cdot k \cdot 2 \, \text{bytes}}
$$

- M: The calculated memory that each token sequence can use, which will help determine the maximum max-batch-total-tokens that can fit.
- 4GiB: The remaining GPU memory available after loading the model. In this example, the user has about 4GiB of free memory.
- ‚Ñì (Layers): The number of layers in the model (transformer blocks). Each layer processes the tokens, so the memory usage scales with the number of layers. 
- d (Dimension of Activations): The size of the activation dimension, which is essentially the width of each layer in terms of neuron count. 
- ùëò (Max Sequence Length): The maximum length of the input sequence, representing the number of tokens in the sequence.
- 2 bytes: Represents the memory required to store each value in bfloat16 (2 bytes per activation, as each floating-point number in bfloat16 requires 2 bytes).

Calculate Memory per Token Sequence:

- For each token sequence that passes through the model, the required memory can be estimated as:
$$
\ell \cdot d \cdot k \cdot 2 \, \text{bytes}
$$
- This accounts for the number of layers, each of size d (dimension of activations), processing a sequence of length k with each activation requiring 2 bytes.

- Since there‚Äôs 4GiB of GPU memory available, the formula divides Available Memory by **this per-sequence requirement**:
$$
\frac{4 \, \text{GB}}{\ell \cdot d \cdot k \cdot 2 \, \text{bytes}}
$$
- The result, M, represents how much of this 4GiB can be allocated per sequence (roughly).
$$
M = \frac{4 \, \text{GB}}{\ell \cdot d \cdot k \cdot 2 \, \text{bytes}}
$$
- A sequence is a series of tokens that represent the input text processed by the model. For example, in a sentence like "The quick brown fox jumps over the lazy dog," each word (or part of a word) is tokenized into smaller units called tokens, which collectively form the input sequence. The length of the sequence (denoted as ùëò in the formula) is the number of tokens in the input.
- The sequence length ùëò corresponds closely to the --max-input-tokens parameter in HF TGI. Sequence Length (ùëò): In the context of the formula and transformer models, the sequence length refers to the number of tokens in a single input sequence that the model processes. --max-input-tokens Parameter: This parameter in HF TGI specifies the maximum number of input tokens that any individual request can have. The --max-input-tokens parameter defines the upper bound for the input sequence length that the model will accept for processing in any request.
- With M calculated, you can estimate the max-batch-total-tokens by multiplying M by the maximum sequence length, k:
$$
max-batch-total-tokens ‚âà ‚åäM √ó ùëò‚åã
$$

On the HuggingFace Hub, every Model Card has the model's files and versions. This will include a specific file called config.json. The config.json file serves as the configuration file that defines the architecture and operational settings of a model. The config.json file provides essential details about the model‚Äôs architecture, such as the number of hidden layers, dimensions of activation, maximum sequence length, precision (torch_dtype), and attention heads. These hyperparameters can be very crucial in determining the memory required for your input sequences:

- num_hidden_layers: Number of transformer layers, e.g. self-attention and feed-forward layers (Do not conflate with supervised learning hidden layer in forward prop Neural Network like word2vec)
- hidden_size: Dimensions of activation
- max_position_embeddings: Maximum sequence length
- torch_dtype: precision or memory requirements per element

**meta-llama/Llama-3.2-90B-Vision-Instruct contains 100 hidden layers. The size of the dimensions of activation is 1280. The maximum sequence length is a stunning 131,072 tokens. Remember the sequence length refers to the input sequence sent in a request to the TGI. The embeddings model BAAI/bge-large-en-v1.5 has max_position_embeddings of 512 (it can process 512 input tokens in a single forward pass). meta-llama/Llama-3.2-90B-Vision-Instruct can take 131,072 input tokens in a single forward pass. A single forward pass refers to the process where an input sequence is passed through the model once to produce an output or intermediate results. Forward Pass: This is the phase where data flows through the layers of the neural network starting from the input layer, through the hidden layers, and up to the output layer. Each layer processes the input data and passes the result to the next layer.**

But in fact we won't be using 131,072. We will set our own max-input-token value to something much smaller, since we only have 4GiB of VRAM remaining. Let's try 35,453.

The torch_dtype for is meta-llama/Llama-3.2-90B-Vision-Instruct is bfloat16. So the Precision in Bytes should be set to 2.

To complete the calculation, we need to convert the 4GiB into bytes, since the denominator of the equation will return a number of bytes:
$$
M = \frac{4,294,967,296 \, \text{bytes}}{100 \cdot 1280 \cdot 35453 \cdot 2 \, \text{bytes}}
$$
$$
M = \frac{4,294,967,296 \, \text{bytes}}{9,075,968,000 \, \text{bytes}}
$$
$$
M = 0.4732241559247454
$$
This formula gives an estimate of how many full sequences (with all tokens) can fit into the available memory. M in the Formula: Represents the number of sequences you can handle in 4GiB of memory, based on the current configuration. That is, the value of M indicates how many sequences you can fit into the available memory, not the amount of memory used per sequence. In this context, M represents the number of sequences that can be processed given the memory constraints.

If ùëÄ < 1, it means that less than one sequence of the specified size can fit in the available memory, indicating that the configuration exceeds the available memory capacity. The result of 0.473 means that only about 47.3% of a single sequence of that size can fit into the available 4 GiB of memory. CLARIFICATION: the result of ùëÄ=0.473 implies that only about 47.3% of a single input sequence of length 35,453 tokens can fit into the available 4 GiB of memory. Of course, if we dropped the sequence length from 35,453 to a smaller number, then yes of course we can fit full sequences into memory!!!!

Now let's calculate the max batch total tokens for a max sequence length of 35,453. Max batch total tokens represents the total number of tokens that can be processed in a batch, encompassing both the prefill operation and the decoding phase (generation of new tokens):

- Prefill operation tokens: These are the input tokens that are initially processed to set up the context for the model before it starts generating new tokens.
- Decoding tokens: These are the tokens that the model generates during the response phase.

In essence, max batch total tokens defines the upper limit for the sum of input tokens and generated tokens across all requests in a batch. This helps ensure that the combined memory usage for the prefill and decoding phases does not exceed the available GPU memory. The formula is as follows, given our max sequence length is 35453:

$$
max-batch-total-tokens = ‚åäM ‚ãÖ k‚åã

16,777.216 = 0.4732241559247454 * 35453
$$

The calculation suggests that we can fit approximately 16,777 tokens in memory, given the available memory and the parameters we used. Here's a breakdown of your logic:

- Your calculated ùëÄ value of approximately 0.473 indicates how many sequences of the given length (one sequence of tokens) can fit into the available memory.
- Multiplying ùëÄ by the sequence length ùëò (which is 35,453) gives you the total number of tokens that can fit in memory.

So, you can fit approximately 16,777 tokens in memory given your configuration, assuming this is the maximum batch of tokens that can be processed without exceeding the available GPU memory.

Let's embrace the two calculations:

- Single Input Sequence: When you calculate how many full input sequences you can fit, you're considering the memory required for a single sequence with all its tokens processed end-to-end through all layers (in our case 35,453 for the sequence length). If you find that ùëÄ is less than 1, it means you can't fit one complete input sequence of the given length (35,453) in memory for a forward pass.
- Total Number of Tokens: When you calculate how many individual tokens you can fit in memory across multiple sequences or batches, you might get a larger number because each token contributes proportionally less memory usage compared to the whole sequence processed through all layers. So we can fit 16,777 tokens, but that is only half of our sequence length of 35,453!!!!!!!! So with a sequence length of 35,453, our TGI will crash!!!!

Now if we want to figure out what the sequence length must be to fit 1 full sequence into 4GiB of memory, we can find the value of k in a formulat:

$$
\text{To find the value of } k \text{ (sequence length) such that:}
$$
$$
\frac{4,294,967,296}{100 \cdot 1280 \cdot k \cdot 2} = 1
$$
$$
\text{Rearrange the equation to solve for } k:
$$
$$
4,294,967,296 = 100 \cdot 1280 \cdot k \cdot 2
$$
$$
k = \frac{4,294,967,296}{100 \cdot 1280 \cdot 2}
$$
$$
\text{Substitute the values:}
$$
$$
k = \frac{4,294,967,296}{256,000}
$$
$$
k = 16,777.216
$$

Yet when trying to run the model, it errors out:

```shell
model=meta-llama/Llama-3.2-90B-Vision-Instruct
token=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
volume=$PWD/data
docker container run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:2.4.0 --model-id $model --max-batch-prefill-tokens 12582 --max-batch-total-tokens 16777 --max-input-tokens 12582 --max-total-tokens 16777 --num-shard 8

RuntimeError: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
2024-11-03T15:21:38.601420Z ERROR warmup{max_input_length=12582 max_prefill_tokens=12582 max_total_tokens=16777 max_batch_size=None}:warmup: text_generation_router_v3::client: backends/v3/src/client/mod.rs:45: Server error: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
2024-11-03T15:21:38.601508Z ERROR warmup{max_input_length=12582 max_prefill_tokens=12582 max_total_tokens=16777 max_batch_size=None}:warmup: text_generation_router_v3::client: backends/v3/src/client/mod.rs:45: Server error: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
2024-11-03T15:21:38.603326Z ERROR warmup{max_input_length=12582 max_prefill_tokens=12582 max_total_tokens=16777 max_batch_size=None}:warmup: text_generation_router_v3::client: backends/v3/src/client/mod.rs:45: Server error: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
2024-11-03T15:21:38.610091Z ERROR warmup{max_input_length=12582 max_prefill_tokens=12582 max_total_tokens=16777 max_batch_size=None}:warmup: text_generation_router_v3::client: backends/v3/src/client/mod.rs:45: Server error: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
2024-11-03T15:21:38.615759Z ERROR warmup{max_input_length=12582 max_prefill_tokens=12582 max_total_tokens=16777 max_batch_size=None}:warmup: text_generation_router_v3::client: backends/v3/src/client/mod.rs:45: Server error: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
2024-11-03T15:21:38.616204Z ERROR warmup{max_input_length=12582 max_prefill_tokens=12582 max_total_tokens=16777 max_batch_size=None}:warmup: text_generation_router_v3::client: backends/v3/src/client/mod.rs:45: Server error: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
2024-11-03T15:21:38.619094Z ERROR warmup{max_input_length=12582 max_prefill_tokens=12582 max_total_tokens=16777 max_batch_size=None}:warmup: text_generation_router_v3::client: backends/v3/src/client/mod.rs:45: Server error: Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
Error: Backend(Warmup(Generation("Not enough memory to handle 12582 prefill tokens. You need to decrease `--max-batch-prefill-tokens`")))
```

### Complex Formula for calculating max-batch-total-tokens

Clearly, the formula above is missing something. Let's investigate this further.

In large language models like Meta Llama 3.1 70B, the term Number of Layers specifically refers to the number of transformer blocks (or transformer layers) stacked in sequence, rather than traditional neural network layers like "input," "hidden," and "output" layers in a fully connected neural network.

START HERE




model=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
token=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
volume=$PWD/data
docker container run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:2.3.1 --model-id $model --max-batch-prefill-tokens 4800 --max-input-length 4768 --max-total-tokens 10960 --num-shard 4

For a single request:
--max-input-tokens : 35,453
--max-total-tokens : 43641
--max-batch-prefill-tokens: 43,690
--max-concurrent-requests

--max-concurrent-requests 128
--max-batch-prefill-tokens: 50k default

--max-batch-total-tokens (not set)

4 GPU: 80GiB : 320GiB

context length 120k (context length)
43k 
